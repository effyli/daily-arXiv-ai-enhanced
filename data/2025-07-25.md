<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 49]
- [cs.DB](#cs.DB) [Total: 1]
- [cs.AI](#cs.AI) [Total: 21]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [VeriMinder: Mitigating Analytical Vulnerabilities in NL2SQL](https://arxiv.org/abs/2507.17896)
*Shubham Mohole,Sainyam Galhotra*

Main category: cs.CL

TL;DR: VeriMinder is an interactive system designed to detect and mitigate cognitive biases in natural language database queries, improving the quality of data analysis.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of bias-free analytical question formulation in natural language interface systems, beyond just improving text-to-SQL accuracy.

Method: Introducing a semantic mapping framework, an analytical guidance system, and an LLM-powered prompt generation process.

Result: User testing shows improved analysis quality and higher scores compared to alternatives; system is open-source.

Conclusion: VeriMinder helps users avoid incorrect questions and enhances analytical robustness in NLIDBs.

Abstract: Application systems using natural language interfaces to databases (NLIDBs)
have democratized data analysis. This positive development has also brought
forth an urgent challenge to help users who might use these systems without a
background in statistical analysis to formulate bias-free analytical questions.
Although significant research has focused on text-to-SQL generation accuracy,
addressing cognitive biases in analytical questions remains underexplored. We
present VeriMinder, https://veriminder.ai, an interactive system for detecting
and mitigating such analytical vulnerabilities. Our approach introduces three
key innovations: (1) a contextual semantic mapping framework for biases
relevant to specific analysis contexts (2) an analytical framework that
operationalizes the Hard-to-Vary principle and guides users in systematic data
analysis (3) an optimized LLM-powered system that generates high-quality,
task-specific prompts using a structured process involving multiple candidates,
critic feedback, and self-reflection.
  User testing confirms the merits of our approach. In direct user experience
evaluation, 82.5% participants reported positively impacting the quality of the
analysis. In comparative evaluation, VeriMinder scored significantly higher
than alternative approaches, at least 20% better when considered for metrics of
the analysis's concreteness, comprehensiveness, and accuracy. Our system,
implemented as a web application, is set to help users avoid "wrong question"
vulnerability during data analysis. VeriMinder code base with prompts,
https://reproducibility.link/veriminder, is available as an MIT-licensed
open-source software to facilitate further research and adoption within the
community.

</details>


### [2] [Factual Inconsistencies in Multilingual Wikipedia Tables](https://arxiv.org/abs/2507.18406)
*Silvia Cappa,Lingxiao Kong,Pille-Riin Peet,Fanfu Wei,Yuchen Zhou,Jan-Christoph Kalo*

Main category: cs.CL

TL;DR: This study analyzes cross-lingual inconsistencies in Wikipedia's structured tabular data to improve factual accuracy and AI reliability.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address factual inconsistencies across different language versions of Wikipedia that can affect content neutrality, reliability, and AI system training.

Method: The paper develops a methodology to collect, align, and analyze tables from multilingual Wikipedia articles, applying quantitative and qualitative metrics.

Result: The analysis reveals patterns of inconsistency in multilingual tables, providing insights into factual verification and multilingual knowledge integration.

Conclusion: Understanding these inconsistencies can enhance the design of more reliable AI systems that leverage Wikipedia content.

Abstract: Wikipedia serves as a globally accessible knowledge source with content in
over 300 languages. Despite covering the same topics, the different versions of
Wikipedia are written and updated independently. This leads to factual
inconsistencies that can impact the neutrality and reliability of the
encyclopedia and AI systems, which often rely on Wikipedia as a main training
source. This study investigates cross-lingual inconsistencies in Wikipedia's
structured content, with a focus on tabular data. We developed a methodology to
collect, align, and analyze tables from Wikipedia multilingual articles,
defining categories of inconsistency. We apply various quantitative and
qualitative metrics to assess multilingual alignment using a sample dataset.
These insights have implications for factual verification, multilingual
knowledge interaction, and design for reliable AI systems leveraging Wikipedia
content.

</details>


### [3] [Shop-R1: Rewarding LLMs to Simulate Human Behavior in Online Shopping via Reinforcement Learning](https://arxiv.org/abs/2507.17842)
*Yimeng Zhang,Tian Wang,Jiri Gesi,Ziyi Wang,Yuxuan Lu,Jiacheng Lin,Sinong Zhan,Vianne Gao,Ruochen Jiao,Junze Liu,Kun Qian,Yuxin Tang,Ran Xue,Houyu Zhang,Qingjun Cui,Yufan Guo,Dakuo Wang*

Main category: cs.CL

TL;DR: Shop-R1 is a reinforcement learning framework that enhances the reasoning abilities of large language models for simulating human behavior in online shopping environments, leading to significant performance improvements.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of existing LLMs in reasoning for human behavior simulation, which are bounded by the models' inherent reasoning capabilities.

Method: Decomposing the task into rationale generation and action prediction, with distinct reward signals, including self-supervised rationale guidance and a hierarchical, difficulty-aware reward structure for actions.

Result: Achieved over 65% relative improvement compared to the baseline, demonstrating enhanced reasoning and action prediction accuracy in simulated online shopping behavior.

Conclusion: Shop-R1 effectively improves the reasoning capabilities of LLMs for human behavior simulation through a multi-stage, reward-guided RL framework, surpassing previous approaches.

Abstract: Large Language Models (LLMs) have recently demonstrated strong potential in
generating 'believable human-like' behavior in web environments. Prior work has
explored augmenting training data with LLM-synthesized rationales and applying
supervised fine-tuning (SFT) to enhance reasoning ability, which in turn can
improve downstream action prediction. However, the performance of such
approaches remains inherently bounded by the reasoning capabilities of the
model used to generate the rationales. In this paper, we introduce Shop-R1, a
novel reinforcement learning (RL) framework aimed at enhancing the reasoning
ability of LLMs for simulation of real human behavior in online shopping
environments Specifically, Shop-R1 decomposes the human behavior simulation
task into two stages: rationale generation and action prediction, each guided
by distinct reward signals. For rationale generation, we leverage internal
model signals (e.g., logit distributions) to guide the reasoning process in a
self-supervised manner. For action prediction, we propose a hierarchical reward
structure with difficulty-aware scaling to prevent reward hacking and enable
fine-grained reward assignment. This design evaluates both high-level action
types and the correctness of fine-grained sub-action details (attributes and
values), rewarding outputs proportionally to their difficulty. Experimental
results show that our method achieves a relative improvement of over 65%
compared to the baseline.

</details>


### [4] [Dynamic and Generalizable Process Reward Modeling](https://arxiv.org/abs/2507.17849)
*Zhangyue Yin,Qiushi Sun,Zhiyuan Zeng,Qinyuan Cheng,Xipeng Qiu,Xuanjing Huang*

Main category: cs.CL

TL;DR: DG-PRM introduces a dynamic, fine-grained reward framework for LLMs, outperforming heuristic methods and enhancing out-of-distribution robustness.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of existing heuristic and static reward models, particularly in cross-domain generalization and complex process supervision.

Method: DG-PRM employs a reward tree for multi-dimensional, fine-grained reward criteria and Pareto dominance for discriminative pair identification, enabling step-wise dynamic reward selection.

Result: Significant performance improvements on benchmarks, with strong generalization to out-of-distribution scenarios.

Conclusion: DG-PRM's dynamic, multi-faceted reward approach enhances LLM guidance in complex, diverse tasks, marking a step forward in reward modeling.

Abstract: Process Reward Models (PRMs) are crucial for guiding Large Language Models
(LLMs) in complex scenarios by providing dense reward signals. However,
existing PRMs primarily rely on heuristic approaches, which struggle with
cross-domain generalization. While LLM-as-judge has been proposed to provide
generalized rewards, current research has focused mainly on feedback results,
overlooking the meaningful guidance embedded within the text. Additionally,
static and coarse-grained evaluation criteria struggle to adapt to complex
process supervision. To tackle these challenges, we propose Dynamic and
Generalizable Process Reward Modeling (DG-PRM), which features a reward tree to
capture and store fine-grained, multi-dimensional reward criteria. DG-PRM
dynamically selects reward signals for step-wise reward scoring. To handle
multifaceted reward signals, we pioneeringly adopt Pareto dominance estimation
to identify discriminative positive and negative pairs. Experimental results
show that DG-PRM achieves stunning performance on prevailing benchmarks,
significantly boosting model performance across tasks with dense rewards.
Further analysis reveals that DG-PRM adapts well to out-of-distribution
scenarios, demonstrating exceptional generalizability.

</details>


### [5] [One Whisper to Grade Them All](https://arxiv.org/abs/2507.17918)
*Nhan Phan,Anusha Porwal,Yaroslav Getman,Ekaterina Voskoboinik,Tamás Grósz,Mikko Kurimo*

Main category: cs.CL

TL;DR: The paper proposes an efficient end-to-end system for holistic Automatic Speaking Assessment using a single encoder, outperforming text-based baselines.


<details>
  <summary>Details</summary>
Motivation: To develop a practical, scalable system for automatic assessment of multi-part second-language spoken responses.

Method: Using a single Whisper-small encoder to process all responses, combined with a lightweight aggregator, avoiding transcription and per-part models.

Result: Achieved an RMSE of 0.384, better than baseline, with high data efficiency through a novel sampling strategy.

Conclusion: The proposed architecture is effective, efficient, and scalable for large-scale language learning assessments.

Abstract: We present an efficient end-to-end approach for holistic Automatic Speaking
Assessment (ASA) of multi-part second-language tests, developed for the 2025
Speak & Improve Challenge. Our system's main novelty is the ability to process
all four spoken responses with a single Whisper-small encoder, combine all
information via a lightweight aggregator, and predict the final score. This
architecture removes the need for transcription and per-part models, cuts
inference time, and makes ASA practical for large-scale Computer-Assisted
Language Learning systems.
  Our system achieved a Root Mean Squared Error (RMSE) of 0.384, outperforming
the text-based baseline (0.44) while using at most 168M parameters (about 70%
of Whisper-small). Furthermore, we propose a data sampling strategy, allowing
the model to train on only 44.8% of the speakers in the corpus and still reach
0.383 RMSE, demonstrating improved performance on imbalanced classes and strong
data efficiency.

</details>


### [6] [Evaluating the Performance of AI Text Detectors, Few-Shot and Chain-of-Thought Prompting Using DeepSeek Generated Text](https://arxiv.org/abs/2507.17944)
*Hulayyil Alshammari,Praveen Rao*

Main category: cs.CL

TL;DR: This paper evaluates the effectiveness of AI detection tools against DeepSeek-generated text and adversarial attacks, finding that some detectors perform well with certain prompts while others are vulnerable.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the gap in understanding how well current AI detection tools identify DeepSeek-generated text, especially under adversarial manipulation.

Method: The researchers generated AI and human-written texts, applied adversarial attacks, and tested six detection tools along with prompting techniques like few-shot and chain-of-thought reasoning.

Result: Detectors like QuillBot and Copyleaks maintained high accuracy, while others showed inconsistency, especially under humanization attacks; prompting methods significantly improved detection accuracy.

Conclusion: While some AI detection tools are robust against DeepSeek texts, adversarial attacks pose significant challenges, but advanced prompting can enhance detection performance.

Abstract: Large language models (LLMs) have rapidly transformed the creation of written
materials. LLMs have led to questions about writing integrity, thereby driving
the creation of artificial intelligence (AI) detection technologies.
Adversarial attacks, such as standard and humanized paraphrasing, inhibit
detectors' ability to detect machine-generated text. Previous studies have
mainly focused on ChatGPT and other well-known LLMs and have shown varying
accuracy across detectors. However, there is a clear gap in the literature
about DeepSeek, a recently published LLM. Therefore, in this work, we
investigate whether six generally accessible AI detection tools -- AI Text
Classifier, Content Detector AI, Copyleaks, QuillBot, GPT-2, and GPTZero -- can
consistently recognize text generated by DeepSeek. The detectors were exposed
to the aforementioned adversarial attacks. We also considered DeepSeek as a
detector by performing few-shot prompting and chain-of-thought reasoning (CoT)
for classifying AI and human-written text. We collected 49 human-authored
question-answer pairs from before the LLM era and generated matching responses
using DeepSeek-v3, producing 49 AI-generated samples. Then, we applied
adversarial techniques such as paraphrasing and humanizing to add 196 more
samples. These were used to challenge detector robustness and assess accuracy
impact. While QuillBot and Copyleaks showed near-perfect performance on
original and paraphrased DeepSeek text, others -- particularly AI Text
Classifier and GPT-2 -- showed inconsistent results. The most effective attack
was humanization, reducing accuracy to 71% for Copyleaks, 58% for QuillBot, and
52% for GPTZero. Few-shot and CoT prompting showed high accuracy, with the best
five-shot result misclassifying only one of 49 samples (AI recall 96%, human
recall 100%).

</details>


### [7] [Are LLM Belief Updates Consistent with Bayes' Theorem?](https://arxiv.org/abs/2507.17951)
*Sohaib Imran,Ihor Kendiukhov,Matthew Broerman,Aditya Thomas,Riccardo Campanella,Rob Lamb,Peter M. Atkinson*

Main category: cs.CL

TL;DR: Larger language models more closely follow Bayesian updating when evaluating evidence.


<details>
  <summary>Details</summary>
Motivation: To investigate whether larger and more capable language models learn to update beliefs in a way consistent with Bayes' theorem.

Method: Development of the Bayesian Coherence Coefficient (BCC) metric and measurement across various models; comparison with model size, data, and benchmarks.

Result: Findings suggest larger models have beliefs more aligned with Bayesian coherence.

Conclusion: Larger models' improved coherence with Bayesian principles has important implications for understanding and regulating LLMs.

Abstract: Do larger and more capable language models learn to update their "beliefs"
about propositions more consistently with Bayes' theorem when presented with
evidence in-context? To test this, we formulate a Bayesian Coherence
Coefficient (BCC) metric and generate a dataset with which to measure the BCC.
We measure BCC for multiple pre-trained-only language models across five model
families, comparing against the number of model parameters, the amount of
training data, and model scores on common benchmarks. Our results provide
evidence for our hypothesis that larger and more capable pre-trained language
models assign credences that are more coherent with Bayes' theorem. These
results have important implications for our understanding and governance of
LLMs.

</details>


### [8] [Natural Language Processing for Tigrinya: Current State and Future Directions](https://arxiv.org/abs/2507.17974)
*Fitsum Gaim,Jong C. Park*

Main category: cs.CL

TL;DR: A comprehensive survey of Tigrinya NLP research from 2011 to 2025 highlights progress, challenges, and future directions.


<details>
  <summary>Details</summary>
Motivation: To address the underrepresentation of Tigrinya in NLP research despite its speaker population.

Method: Systematic review of over 40 studies across various NLP tasks, analyzing resources, models, and applications.

Result: Identified a transition from rule-based to neural architectures, resources-driven progress, and key challenges like morphological complexity and resource scarcity.

Conclusion: Provides a research roadmap emphasizing morphology-aware modeling, cross-lingual transfer, and community resource development.

Abstract: Despite being spoken by millions of people, Tigrinya remains severely
underrepresented in Natural Language Processing (NLP) research. This work
presents a comprehensive survey of NLP research for Tigrinya, analyzing over 40
studies spanning more than a decade of work from 2011 to 2025. We
systematically review the current state of computational resources, models, and
applications across ten distinct downstream tasks, including morphological
processing, machine translation, speech recognition, and question-answering.
Our analysis reveals a clear trajectory from foundational, rule-based systems
to modern neural architectures, with progress consistently unlocked by resource
creation milestones. We identify key challenges rooted in Tigrinya's
morphological complexity and resource scarcity, while highlighting promising
research directions, including morphology-aware modeling, cross-lingual
transfer, and community-centered resource development. This work serves as both
a comprehensive reference for researchers and a roadmap for advancing Tigrinya
NLP. A curated metadata of the surveyed studies and resources is made publicly
available.\footnote{Tigrinya NLP Anthology:
https://github.com/fgaim/tigrinya-nlp-anthology.

</details>


### [9] [Technical Report of TeleChat2, TeleChat2.5 and T1](https://arxiv.org/abs/2507.18013)
*Zihan Wang,Xinzhang Liu,Yitong Yao,Chao Wang,Yu Zhao,Zhihao Yang,Wenmin Deng,Kaipeng Jia,Jiaxin Peng,Yuyao Huang,Sishi Xiong,Zhuo Jiang,Kaidong Yu,Xiaohui Hu,Fubei Yao,Ruiyu Fang,Zhuoru Jiang,Ruiting Song,Qiyi Xie,Rui Xue,Xuewei He,Yanlei Xue,Zhu Yuan,Zhaoxi Zhang,Zilu Huang,Shiquan Wang,Xin Wang,Hanming Wu,Mingyuan Wang,Xufeng Zhan,Yuhan Sun,Zhaohu Xing,Yuhao Jiang,Bingkai Yang,Shuangyong Song,Yongxiang Li,Zhongjiang He,Xuelong Li*

Main category: cs.CL

TL;DR: Introduction of TeleChat series models with performance enhancements through improved training strategies, focusing on reasoning, code generation, and speed.


<details>
  <summary>Details</summary>
Motivation: To advance language models by improving training methods and architecture to enhance reasoning, code generation, and inference speed.

Method: Enhanced training strategies involving large-scale pretraining, supervised fine-tuning, preference optimization, domain-specific continual pretraining, and reinforcement learning.

Result: Significant performance gains over predecessors, with 115B models outperforming proprietary counterparts, supporting complex reasoning, and offering both high performance and speed.

Conclusion: The new TeleChat series models set a new standard in large language models, demonstrating substantial improvements and public availability for diverse applications.

Abstract: We introduce the latest series of TeleChat models: \textbf{TeleChat2},
\textbf{TeleChat2.5}, and \textbf{T1}, offering a significant upgrade over
their predecessor, TeleChat. Despite minimal changes to the model architecture,
the new series achieves substantial performance gains through enhanced training
strategies in both pre-training and post-training stages. The series begins
with \textbf{TeleChat2}, which undergoes pretraining on 10 trillion
high-quality and diverse tokens. This is followed by Supervised Fine-Tuning
(SFT) and Direct Preference Optimization (DPO) to further enhance its
capabilities. \textbf{TeleChat2.5} and \textbf{T1} expand the pipeline by
incorporating a continual pretraining phase with domain-specific datasets,
combined with reinforcement learning (RL) to improve performance in code
generation and mathematical reasoning tasks. The \textbf{T1} variant is
designed for complex reasoning, supporting long Chain-of-Thought (CoT)
reasoning and demonstrating substantial improvements in mathematics and coding.
In contrast, \textbf{TeleChat2.5} prioritizes speed, delivering rapid
inference. Both flagship models of \textbf{T1} and \textbf{TeleChat2.5} are
dense Transformer-based architectures with 115B parameters, showcasing
significant advancements in reasoning and general task performance compared to
the original TeleChat. Notably, \textbf{T1-115B} outperform proprietary models
such as OpenAI's o1-mini and GPT-4o. We publicly release \textbf{TeleChat2},
\textbf{TeleChat2.5} and \textbf{T1}, including post-trained versions with 35B
and 115B parameters, to empower developers and researchers with
state-of-the-art language models tailored for diverse applications.

</details>


### [10] [NeuralDB: Scaling Knowledge Editing in LLMs to 100,000 Facts with Neural KV Database](https://arxiv.org/abs/2507.18028)
*Weizhi Fei,Hao Shi,Jing Xu,Jingchen Peng,Jiazheng Li,Jingzhao Zhang,Bo Bai,Wei Han,Zhenyuan Chen,Xueyan Niu*

Main category: cs.CL

TL;DR: NeuralDB is a new knowledge editing framework for LLMs that effectively updates facts while preserving general ability, scalable to 100,000 facts.


<details>
  <summary>Details</summary>
Motivation: Existing linear methods for editing large language models can compromise their general abilities and may forget facts when scaled up.

Method: NeuralDB models facts as a neural Key-Value database with a gated retrieval module that only activates during relevant inference, thus protecting overall performance.

Result: NeuralDB outperforms previous methods in editing quality, generalization, and task performance, even when scaling to 100,000 facts across multiple datasets and models.

Conclusion: NeuralDB provides an efficient, scalable, and effective approach to knowledge editing in LLMs, maintaining their broad capabilities.

Abstract: Efficiently editing knowledge stored in large language models (LLMs) enables
model updates without large-scale training. One possible solution is
Locate-and-Edit (L\&E), allowing simultaneous modifications of a massive number
of facts. However, such editing may compromise the general abilities of LLMs
and even result in forgetting edited facts when scaling up to thousands of
edits. In this paper, we model existing linear L\&E methods as querying a
Key-Value (KV) database. From this perspective, we then propose NeuralDB, an
editing framework that explicitly represents the edited facts as a neural KV
database equipped with a non-linear gated retrieval module, % In particular,
our gated module only operates when inference involves the edited facts,
effectively preserving the general abilities of LLMs. Comprehensive experiments
involving the editing of 10,000 facts were conducted on the ZsRE and
CounterFacts datasets, using GPT2-XL, GPT-J (6B) and Llama-3 (8B). The results
demonstrate that NeuralDB not only excels in editing efficacy, generalization,
specificity, fluency, and consistency, but also preserves overall performance
across six representative text understanding and generation tasks. Further
experiments indicate that NeuralDB maintains its effectiveness even when scaled
to 100,000 facts (\textbf{50x} more than in prior work).

</details>


### [11] [GrAInS: Gradient-based Attribution for Inference-Time Steering of LLMs and VLMs](https://arxiv.org/abs/2507.18043)
*Duy Nguyen,Archiki Prasad,Elias Stengel-Eskin,Mohit Bansal*

Main category: cs.CL

TL;DR: GrAInS improves model behavior at inference time by using attribution-based token influence to steer outputs, outperforming fine-tuning and other methods without retraining.


<details>
  <summary>Details</summary>
Motivation: To develop a lightweight, interpretable method for guiding large models' behavior without retraining.

Method: Uses Integrated Gradients for token attribution, constructs semantic steering vectors, and adjusts activations during inference.

Result: Achieves significant improvements in accuracy and reduced hallucinations across tasks, outperforming baseline methods.

Conclusion: GrAInS offers a modular, effective approach for inference-time model steering, enhancing task-specific performance without retraining.

Abstract: Inference-time steering methods offer a lightweight alternative to
fine-tuning large language models (LLMs) and vision-language models (VLMs) by
modifying internal activations at test time without updating model weights.
However, most existing approaches rely on fixed, global intervention vectors,
overlook the causal influence of individual input tokens, and fail to leverage
informative gradients from the model's logits, particularly in multimodal
settings where visual and textual inputs contribute unevenly. To address these
limitations, we introduce GrAInS, an inference-time steering approach that
operates across both language-only and vision-language models and tasks. GrAInS
uses contrastive, gradient-based attribution via Integrated Gradients to
identify the top-k most influential tokens, both positively and negatively
attributed based on their contribution to preferred versus dispreferred
outputs. These tokens are then used to construct directional steering vectors
that capture semantic shifts from undesirable to desirable behavior. During
inference, GrAInS adjusts hidden activations at transformer layers guided by
token-level attribution signals, and normalizes activations to preserve
representational scale. This enables fine-grained, interpretable, and modular
control over model behavior, without retraining or auxiliary supervision.
Empirically, GrAInS consistently outperforms both fine-tuning and existing
steering baselines: it achieves a 13.22% accuracy gain on TruthfulQA using
Llama-3.1-8B, reduces hallucination rates on MMHal-Bench from 0.624 to 0.514
with LLaVA-1.6-7B, and improves alignment win rates on SPA-VL by 8.11%, all
while preserving the model's fluency and general capabilities.

</details>


### [12] [Synthetic Data Generation for Phrase Break Prediction with Large Language Model](https://arxiv.org/abs/2507.18044)
*Hoyeon Lee,Sejung Son,Ye-Eun Kang,Jong-Hwan Kim*

Main category: cs.CL

TL;DR: Using LLMs to generate synthetic annotations for phrase break prediction reduces manual effort and improves data quality.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of manual data annotation and variability in speech data for phrase break prediction.

Method: Utilize large language models to generate synthetic phrase break annotations and compare with traditional annotations across multiple languages.

Result: LLM-generated synthetic data effectively mitigates data challenges in phrase break prediction, showing promise for speech applications.

Conclusion: LLMs are a viable solution for generating high-quality synthetic data, improving the robustness and efficiency of phrase break prediction in speech systems.

Abstract: Current approaches to phrase break prediction address crucial prosodic
aspects of text-to-speech systems but heavily rely on vast human annotations
from audio or text, incurring significant manual effort and cost. Inherent
variability in the speech domain, driven by phonetic factors, further
complicates acquiring consistent, high-quality data. Recently, large language
models (LLMs) have shown success in addressing data challenges in NLP by
generating tailored synthetic data while reducing manual annotation needs.
Motivated by this, we explore leveraging LLM to generate synthetic phrase break
annotations, addressing the challenges of both manual annotation and
speech-related tasks by comparing with traditional annotations and assessing
effectiveness across multiple languages. Our findings suggest that LLM-based
synthetic data generation effectively mitigates data challenges in phrase break
prediction and highlights the potential of LLMs as a viable solution for the
speech domain.

</details>


### [13] [Privacy-Preserving Synthetic Review Generation with Diverse Writing Styles Using LLMs](https://arxiv.org/abs/2507.18055)
*Tevin Atwal,Chan Nam Tieu,Yefeng Yuan,Zhan Shi,Yuhong Liu,Liang Cheng*

Main category: cs.CL

TL;DR: The paper assesses the diversity and privacy risks of synthetic text data generated by Large Language Models and proposes a prompt-based method to improve diversity while maintaining privacy.


<details>
  <summary>Details</summary>
Motivation: To address the underexplored issues of diversity and privacy risks in synthetic data generated by LLMs, which are crucial for safe and effective data-driven applications.

Method: Development of metrics to evaluate diversity and privacy aspects, experimental analysis of current LLM-generated data, and a prompt-based approach to enhance diversity and protect privacy.

Result: Current LLMs have limitations in generating diverse and privacy-preserving synthetic data; the proposed prompt-based method improves diversity without compromising privacy.

Conclusion: The study highlights the need for better evaluation metrics and methods to enhance the quality of synthetic data, ensuring both diversity and privacy are maintained.

Abstract: The increasing use of synthetic data generated by Large Language Models
(LLMs) presents both opportunities and challenges in data-driven applications.
While synthetic data provides a cost-effective, scalable alternative to
real-world data to facilitate model training, its diversity and privacy risks
remain underexplored. Focusing on text-based synthetic data, we propose a
comprehensive set of metrics to quantitatively assess the diversity (i.e.,
linguistic expression, sentiment, and user perspective), and privacy (i.e.,
re-identification risk and stylistic outliers) of synthetic datasets generated
by several state-of-the-art LLMs. Experiment results reveal significant
limitations in LLMs' capabilities in generating diverse and privacy-preserving
synthetic data. Guided by the evaluation results, a prompt-based approach is
proposed to enhance the diversity of synthetic reviews while preserving
reviewer privacy.

</details>


### [14] [TELEVAL: A Dynamic Benchmark Designed for Spoken Language Models in Chinese Interactive Scenarios](https://arxiv.org/abs/2507.18061)
*Zehan Li,Hongjie Chen,Yuxin Zhang,Jing Zhou,Xuening Wang,Hang Lv,Mengjie Du,Yaodong Song,Jie Lian,Jian Kang,Jie Li,Yongxiang Li,Zhongjiang He,Xuelong Li*

Main category: cs.CL

TL;DR: TELEVAL is a new benchmark for evaluating Chinese spoken language models as conversational agents, focusing on real-world interaction aspects.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks mainly assess complex task performance, neglecting natural conversational interactions.

Method: Developing a dynamic, multi-dimensional dialogue format that evaluates explicit semantics, paralinguistic cues, and system abilities, with separate text and audio assessments.

Result: Current SLMs show significant room for improvement in natural conversational settings.

Conclusion: TELEVAL aims to be a user-centered evaluation tool to foster more capable dialogue-oriented SLMs.

Abstract: Spoken language models (SLMs) have seen rapid progress in recent years, along
with the development of numerous benchmarks for evaluating their performance.
However, most existing benchmarks primarily focus on evaluating whether SLMs
can perform complex tasks comparable to those tackled by large language models
(LLMs), often failing to align with how users naturally interact in real-world
conversational scenarios. In this paper, we propose TELEVAL, a dynamic
benchmark specifically designed to evaluate SLMs' effectiveness as
conversational agents in realistic Chinese interactive settings. TELEVAL
defines three evaluation dimensions: Explicit Semantics, Paralinguistic and
Implicit Semantics, and System Abilities. It adopts a dialogue format
consistent with real-world usage and evaluates text and audio outputs
separately. TELEVAL particularly focuses on the model's ability to extract
implicit cues from user speech and respond appropriately without additional
instructions. Our experiments demonstrate that despite recent progress,
existing SLMs still have considerable room for improvement in natural
conversational tasks. We hope that TELEVAL can serve as a user-centered
evaluation framework that directly reflects the user experience and contributes
to the development of more capable dialogue-oriented SLMs.

</details>


### [15] [Hybrid and Unitary Fine-Tuning of Large Language Models: Methods and Benchmarking under Resource Constraints](https://arxiv.org/abs/2507.18076)
*Haomin Qi,Zihan Dai,Chengbo Huang*

Main category: cs.CL

TL;DR: The paper introduces a hybrid PEFT method combining BOFT, LoRA-GA, and uRNN principles, achieving efficient fine-tuning of large LLMs with better convergence and resource savings.


<details>
  <summary>Details</summary>
Motivation: To address the high computational and memory requirements of fine-tuning large language models.

Method: Developing a hybrid PEFT strategy with per-layer adaptive updates, integrating orthogonal stability and gradient alignment, also adapting uRNN principles to transformers.

Result: The hybrid method outperforms individual techniques across multiple benchmarks, nearing full fine-tuning accuracy while significantly reducing training time and memory usage.

Conclusion: The proposed hybrid PEFT approach is practical, scalable, and effective for resource-constrained LLM fine-tuning.

Abstract: Fine-tuning large language models (LLMs) remains a computational bottleneck
due to their scale and memory demands. This paper presents a comprehensive
evaluation of parameter-efficient fine-tuning (PEFT) techniques, including
LoRA, BOFT, LoRA-GA, and uRNN, and introduces a novel hybrid strategy that
dynamically integrates BOFT's orthogonal stability with LoRA-GA's
gradient-aligned rapid convergence. By computing per-layer adaptive updates
guided by gradient norms, the hybrid method achieves superior convergence
efficiency and generalization across diverse tasks. We also explore, for the
first time, the adaptation of unitary RNN (uRNN) principles to
transformer-based LLMs, enhancing gradient stability through structured unitary
constraints. Empirical evaluations on four benchmarks -- GLUE, GSM8K, MT-Bench,
and HumanEval -- using models ranging from 7B to 405B parameters demonstrate
that our hybrid method consistently outperforms individual PEFT baselines,
approaching full fine-tuning accuracy while reducing resource consumption by up
to 2.1 times in training time and 50 percent in memory usage. These findings
establish the hybrid approach as a practical and scalable fine-tuning solution
for real-world deployment of LLMs under resource constraints.

</details>


### [16] [A New Pair of GloVes](https://arxiv.org/abs/2507.18103)
*Riley Carlson,John Bauer,Christopher D. Manning*

Main category: cs.CL

TL;DR: The paper presents updated 2024 GloVe models that include new words and improved performance on recent datasets, addressing previous documentation issues.


<details>
  <summary>Details</summary>
Motivation: To update and better document GloVe models for contemporary language use and evaluation.

Method: Training new embeddings on Wikipedia, Gigaword, and Dolma data, followed by various evaluation methods including vocabulary analysis, direct tasks, and NER tasks.

Result: The models incorporate culturally relevant words, perform well on analogy and similarity tasks, and outperform previous models on recent NER datasets.

Conclusion: The 2024 GloVe models are an improved, well-documented resource that reflects contemporary language use and enhances NLP performance.

Abstract: This report documents, describes, and evaluates new 2024 English GloVe
(Global Vectors for Word Representation) models. While the original GloVe
models built in 2014 have been widely used and found useful, languages and the
world continue to evolve and we thought that current usage could benefit from
updated models. Moreover, the 2014 models were not carefully documented as to
the exact data versions and preprocessing that were used, and we rectify this
by documenting these new models. We trained two sets of word embeddings using
Wikipedia, Gigaword, and a subset of Dolma. Evaluation through vocabulary
comparison, direct testing, and NER tasks shows that the 2024 vectors
incorporate new culturally and linguistically relevant words, perform
comparably on structural tasks like analogy and similarity, and demonstrate
improved performance on recent, temporally dependent NER datasets such as
non-Western newswire data.

</details>


### [17] [GOAT-SLM: A Spoken Language Model with Paralinguistic and Speaker Characteristic Awareness](https://arxiv.org/abs/2507.18119)
*Hongjie Chen,Zehan Li,Yaodong Song,Wenming Deng,Yitong Yao,Yuxin Zhang,Hang Lv,Xuechao Zhu,Jian Kang,Jie Lian,Jie Li,Chao Wang,Shuangyong Song,Yongxiang Li,Zhongjiang He*

Main category: cs.CL

TL;DR: GOAT-SLM is a novel spoken language model that captures paralinguistic and speaker characteristics, improving naturalness and social awareness in speech interactions.


<details>
  <summary>Details</summary>
Motivation: Current models overlook paralinguistic cues in speech, limiting naturalness.

Method: A dual-modality architecture and staged training with large-scale speech-text data.

Result: Outperforms existing models in semantic and non-semantic tasks, especially in emotion, dialect, and age sensitivity.

Conclusion: Modeling beyond linguistic content enhances natural, adaptive, and socially aware spoken language systems.

Abstract: Recent advances in end-to-end spoken language models (SLMs) have
significantly improved the ability of AI systems to engage in natural spoken
interactions. However, most existing models treat speech merely as a vehicle
for linguistic content, often overlooking the rich paralinguistic and speaker
characteristic cues embedded in human speech, such as dialect, age, emotion,
and non-speech vocalizations. In this work, we introduce GOAT-SLM, a novel
spoken language model with paralinguistic and speaker characteristic awareness,
designed to extend spoken language modeling beyond text semantics. GOAT-SLM
adopts a dual-modality head architecture that decouples linguistic modeling
from acoustic realization, enabling robust language understanding while
supporting expressive and adaptive speech generation. To enhance model
efficiency and versatility, we propose a modular, staged training strategy that
progressively aligns linguistic, paralinguistic, and speaker characteristic
information using large-scale speech-text corpora. Experimental results on
TELEVAL, a multi-dimensional evaluation benchmark, demonstrate that GOAT-SLM
achieves well-balanced performance across both semantic and non-semantic tasks,
and outperforms existing open-source models in handling emotion, dialectal
variation, and age-sensitive interactions. This work highlights the importance
of modeling beyond linguistic content and advances the development of more
natural, adaptive, and socially aware spoken language systems.

</details>


### [18] [MathOPEval: A Fine-grained Evaluation Benchmark for Visual Operations of MLLMs in Mathematical Reasoning](https://arxiv.org/abs/2507.18140)
*Xiaoyuan Li,Moxin Li,Wenjie Wang,Rui Men,Yichang Zhang,Fuli Feng,Dayiheng Liu,Junyang Lin*

Main category: cs.CL

TL;DR: This paper evaluates multi-modal large language models' (MLLMs) ability to perform code-based multi-modal mathematical reasoning, focusing on visual operations like generation and editing, revealing significant gaps compared to human performance.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the gap in existing evaluations which mainly focus on text-only reasoning, ignoring the capability of MLLMs in precise visual operations via code.

Method: The framework assesses two key aspects: Multi-modal Code Generation for understanding and creating visualizations, and Multi-modal Code Editing for fine-grained visual modifications, using a dataset of five common mathematical figure types across nine mainstream MLLMs.

Result: Existing models perform significantly worse than humans in fine-grained visual operations, highlighting a substantial gap in current capabilities.

Conclusion: Current MLLMs have limited capacity for detailed multi-modal visual reasoning through code, indicating the need for further advancements.

Abstract: Recent progress in Multi-modal Large Language Models (MLLMs) has enabled
step-by-step multi-modal mathematical reasoning by performing visual operations
based on the textual instructions. A promising approach uses code as an
intermediate representation to precisely express and manipulate the images in
the reasoning steps. However, existing evaluations focus mainly on text-only
reasoning outputs, leaving the MLLM's ability to perform accurate visual
operations via code largely unexplored. This work takes a first step toward
addressing that gap by evaluating MLLM's code-based capabilities in multi-modal
mathematical reasoning.Specifically, our framework focuses on two key
evaluation aspects: (1) Multi-modal Code Generation (MCG) evaluates the model's
ability to accurately understand and construct visualizations from scratch. (2)
Multi-modal Code Editing (MCE) assesses the model's capacity for fine-grained
operations, which include three types: Deletion, Modification and Annotation.
To evaluate the above tasks, we incorporate a dataset that covers the five most
popular types of mathematical figures, including geometric diagrams, function
plots, and three types of statistical charts, to provide a comprehensive and
effective measurement of existing MLLMs. Our experimental evaluation involves
nine mainstream MLLMs, and the results reveal that existing models still lag
significantly behind human performance in performing fine-grained visual
operations.

</details>


### [19] [HIVMedQA: Benchmarking large language models for HIV medical decision support](https://arxiv.org/abs/2507.18143)
*Gonzalo Cardenal Antolin,Jacques Fellay,Bashkim Jaha,Roger Kouyos,Niko Beerenwinkel,Diane Duroux*

Main category: cs.CL

TL;DR: This study evaluates the capabilities of large language models (LLMs) in HIV management, using a new benchmark to assess their strengths and limitations in clinical question answering.


<details>
  <summary>Details</summary>
Motivation: To explore the potential and challenges of integrating LLMs into HIV clinical care, given the complexity of the disease and current lack of comprehensive evaluation frameworks.

Method: Developed HIVMedQA, a benchmark dataset of clinically relevant questions, and evaluated multiple general-purpose and medical LLMs using prompt engineering, with assessment across various performance dimensions.

Result: Gemini 2.5 Pro outperformed others; performance decreased with question complexity; medical fine-tuning did not always help; larger models were not consistently better; reasoning and comprehension were challenging.

Conclusion: Targeted development and evaluation are necessary to ensure safe, effective integration of LLMs into clinical HIV care.

Abstract: Large language models (LLMs) are emerging as valuable tools to support
clinicians in routine decision-making. HIV management is a compelling use case
due to its complexity, including diverse treatment options, comorbidities, and
adherence challenges. However, integrating LLMs into clinical practice raises
concerns about accuracy, potential harm, and clinician acceptance. Despite
their promise, AI applications in HIV care remain underexplored, and LLM
benchmarking studies are scarce. This study evaluates the current capabilities
of LLMs in HIV management, highlighting their strengths and limitations. We
introduce HIVMedQA, a benchmark designed to assess open-ended medical question
answering in HIV care. The dataset consists of curated, clinically relevant
questions developed with input from an infectious disease physician. We
evaluated seven general-purpose and three medically specialized LLMs, applying
prompt engineering to enhance performance. Our evaluation framework
incorporates both lexical similarity and an LLM-as-a-judge approach, extended
to better reflect clinical relevance. We assessed performance across key
dimensions: question comprehension, reasoning, knowledge recall, bias,
potential harm, and factual accuracy. Results show that Gemini 2.5 Pro
consistently outperformed other models across most dimensions. Notably, two of
the top three models were proprietary. Performance declined as question
complexity increased. Medically fine-tuned models did not always outperform
general-purpose ones, and larger model size was not a reliable predictor of
performance. Reasoning and comprehension were more challenging than factual
recall, and cognitive biases such as recency and status quo were observed.
These findings underscore the need for targeted development and evaluation to
ensure safe, effective LLM integration in clinical care.

</details>


### [20] [Sticking to the Mean: Detecting Sticky Tokens in Text Embedding Models](https://arxiv.org/abs/2507.18171)
*Kexin Chen,Dongxia Wang,Yi Liu,Haonan Zhang,Wenhai Wang*

Main category: cs.CL

TL;DR: Sticky tokens in Transformer models can skew sentence similarity and impair NLP performance, necessitating detection and better tokenization strategies.


<details>
  <summary>Details</summary>
Motivation: to address the reliability issues caused by anomalous 'sticky tokens' in Transformer-based text embeddings used in NLP.

Method: developing the Sticky Token Detector (STD) method to identify sticky tokens across various models, analyzing their origins, and evaluating their impact on downstream tasks.

Result: 868 sticky tokens identified, mostly from special entries and subwords; they significantly affect clustering and retrieval tasks, with models showing attention allocation biases.

Conclusion: Improved tokenization and model design are needed to reduce sticky token effects and enhance embedding robustness.

Abstract: Despite the widespread use of Transformer-based text embedding models in NLP
tasks, surprising 'sticky tokens' can undermine the reliability of embeddings.
These tokens, when repeatedly inserted into sentences, pull sentence similarity
toward a certain value, disrupting the normal distribution of embedding
distances and degrading downstream performance. In this paper, we
systematically investigate such anomalous tokens, formally defining them and
introducing an efficient detection method, Sticky Token Detector (STD), based
on sentence and token filtering. Applying STD to 40 checkpoints across 14 model
families, we discover a total of 868 sticky tokens. Our analysis reveals that
these tokens often originate from special or unused entries in the vocabulary,
as well as fragmented subwords from multilingual corpora. Notably, their
presence does not strictly correlate with model size or vocabulary size. We
further evaluate how sticky tokens affect downstream tasks like clustering and
retrieval, observing significant performance drops of up to 50%. Through
attention-layer analysis, we show that sticky tokens disproportionately
dominate the model's internal representations, raising concerns about
tokenization robustness. Our findings show the need for better tokenization
strategies and model design to mitigate the impact of sticky tokens in future
text embedding applications.

</details>


### [21] [SCOPE: Stochastic and Counterbiased Option Placement for Evaluating Large Language Models](https://arxiv.org/abs/2507.18182)
*Wonjun Jeong,Dongseok Kim,Taegkeun Whangbo*

Main category: cs.CL

TL;DR: SCOPE is a new evaluation framework that detects and mitigates position bias in Large Language Models, leading to more reliable assessments.


<details>
  <summary>Details</summary>
Motivation: To address the issue of inflated scores in LLMs caused by biases rather than genuine understanding.

Method: SCOPE estimates position bias via null prompts, redistributes answer options to counter bias, and prevents placement of similar distractors.

Result: SCOPE outperforms existing methods, providing more stable performance and clearer confidence metrics.

Conclusion: SCOPE improves fairness and reliability in LLM evaluations by mitigating selection bias.

Abstract: Large Language Models (LLMs) can achieve inflated scores on multiple-choice
tasks by exploiting inherent biases in option positions or labels, rather than
demonstrating genuine understanding. This study introduces SCOPE, an evaluation
framework designed to measure and mitigate such selection bias in a
dataset-independent manner. By repeatedly invoking a null prompt that lacks
semantic content, SCOPE estimates each model's unique position-bias
distribution. It then redistributes the answer slot according to the
inverse-bias distribution, thereby equalizing the lucky-rate, the probability
of selecting the correct answer by chance. Furthermore, it prevents
semantically similar distractors from being placed adjacent to the answer,
thereby blocking near-miss guesses based on superficial proximity cues. Across
multiple benchmark experiments, SCOPE consistently outperformed existing
debiasing methods in terms of stable performance improvements and showed
clearer confidence distributions over correct options. This framework thus
offers a new standard for enhancing the fairness and reliability of LLM
evaluations.

</details>


### [22] [TN-AutoRCA: Benchmark Construction and Agentic Framework for Self-Improving Alarm-Based Root Cause Analysis in Telecommunication Networks](https://arxiv.org/abs/2507.18190)
*Keyu Wu,Qianjin Yu,Manlin Mei,Ruiting Liu,Jun Wang,Kailai Zhang,Yelun Bao*

Main category: cs.CL

TL;DR: RCA in telecom networks is complex for AI because of graph reasoning and limited benchmarks.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of applying AI to RCA in telecommunication networks.

Method: The study likely explores AI techniques tailored for graph-based reasoning in RCA tasks.

Result: Potential improvements or insights into applying AI in telecom RCA.

Conclusion: Advancing AI methods could enhance RCA effectiveness in complex telecom networks.

Abstract: Root Cause Analysis (RCA) in telecommunication networks is a critical task,
yet it presents a formidable challenge for Artificial Intelligence (AI) due to
its complex, graph-based reasoning requirements and the scarcity of realistic
benchmarks.

</details>


### [23] [Integrating an ISO30401-compliant Knowledge management system with existing business processes of an organization](https://arxiv.org/abs/2507.18197)
*Aline Belloni,Patrick Prieur*

Main category: cs.CL

TL;DR: The paper discusses integrating ISO30401 Knowledge Management System with existing business process models, especially within ISO9001 frameworks, using SECI model and PDCA cycles.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of explaining how knowledge management activities align with operational processes in ISO9001 compliant organizations.

Method: The paper recaps process modeling principles and explores integration with ISO30401 using SECI model and PDCA cycles.

Result: It demonstrates how an ISO30401-compliant KMS can be woven into existing processes and managed effectively.

Conclusion: Integrating KMS via SECI and PDCA promotes alignment with operational processes and enhances management systems.

Abstract: Business process modeling is used by most organizations as an essential
framework for ensuring efficiency and effectiveness of the work and workflow
performed by its employees and for ensuring the alignment of such work with its
strategic goals. For organizations that are compliant or near-compliant with
ISO 9001, this approach involves the detailed mapping of processes,
sub-processes, activities, and tasks. ISO30401 is a Management System Standard,
introduced in 2018, establishing universal requirements for the set up of a
Knowledge Management System in an organization. As ``ISO30401 implementers'' we
regularly face the challenge of explaining our clients how the knowledge
development, transformation and conveyances activities depicted in ISO30401 do
integrate with existing operational processes. This article recaps process
modelling principles in the context of ISO9001 and explores, based on our
experience, how an ISO30401-compliant Knowledge Management System (KMS)
entwines with all other processes of an Integrated Management System and in
particular how it can be implemented by deploying the mechanisms of the SECI
model through the steps of PDCA cycles.

</details>


### [24] [Safeguarding RAG Pipelines with GMTP: A Gradient-based Masked Token Probability Method for Poisoned Document Detection](https://arxiv.org/abs/2507.18202)
*San Kim,Jonghwi Kim,Yejin Jeon,Gary Geunbae Lee*

Main category: cs.CL

TL;DR: GMTP effectively detects and filters poisoned documents in RAG systems by analyzing token gradients and masked probabilities, removing over 90% of malicious content.


<details>
  <summary>Details</summary>
Motivation: To enhance the security of Retrieval-Augmented Generation systems against adversarial document injection.

Method: Using gradient analysis of retriever's similarity function to identify high-impact tokens, masking them, and then evaluating masked token probabilities with an MLM.

Result: GMTP successfully filters out over 90% of poisoned content, preserving retrieval and generation performance.

Conclusion: GMTP provides a high-precision defense mechanism against adversarial attacks in RAG, improving robustness without sacrificing accuracy.

Abstract: Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs) by
providing external knowledge for accurate and up-to-date responses. However,
this reliance on external sources exposes a security risk, attackers can inject
poisoned documents into the knowledge base to steer the generation process
toward harmful or misleading outputs. In this paper, we propose Gradient-based
Masked Token Probability (GMTP), a novel defense method to detect and filter
out adversarially crafted documents. Specifically, GMTP identifies high-impact
tokens by examining gradients of the retriever's similarity function. These key
tokens are then masked, and their probabilities are checked via a Masked
Language Model (MLM). Since injected tokens typically exhibit markedly low
masked-token probabilities, this enables GMTP to easily detect malicious
documents and achieve high-precision filtering. Experiments demonstrate that
GMTP is able to eliminate over 90% of poisoned content while retaining relevant
documents, thus maintaining robust retrieval and generation performance across
diverse datasets and adversarial settings.

</details>


### [25] [Exploring the Impact of Instruction-Tuning on LLM's Susceptibility to Misinformation](https://arxiv.org/abs/2507.18203)
*Kyubeen Han,Junseo Jang,Hongjin Kim,Geunyeong Jeong,Harksoo Kim*

Main category: cs.CL

TL;DR: Instruction-tuning increases LLMs' reliance on user-provided misinformation, raising concerns about reliability.


<details>
  <summary>Details</summary>
Motivation: To investigate how instruction-tuning affects LLMs' susceptibility to misinformation.

Method: Analyzed the impact of instruction-tuning on LLMs compared to base models, examining factors like prompt structure and misinformation length.

Result: Instruction-tuned LLMs are more likely to accept misinformation, especially when presented by the user, with susceptibility influenced by prompt structure and warnings.

Conclusion: There is a need for systematic mitigation strategies to address the increased reliance on user input and improve LLM reliability.

Abstract: Instruction-tuning enhances the ability of large language models (LLMs) to
follow user instructions more accurately, improving usability while reducing
harmful outputs. However, this process may increase the model's dependence on
user input, potentially leading to the unfiltered acceptance of misinformation
and the generation of hallucinations. Existing studies primarily highlight that
LLMs are receptive to external information that contradict their parametric
knowledge, but little research has been conducted on the direct impact of
instruction-tuning on this phenomenon. In our study, we investigate the impact
of instruction-tuning on LLM's susceptibility to misinformation. Our analysis
reveals that instruction-tuned LLMs are significantly more likely to accept
misinformation when it is presented by the user. A comparison with base models
shows that instruction-tuning increases reliance on user-provided information,
shifting susceptibility from the assistant role to the user role. Furthermore,
we explore additional factors influencing misinformation susceptibility, such
as the role of the user in prompt structure, misinformation length, and the
presence of warnings in the system prompt. Our findings underscore the need for
systematic approaches to mitigate unintended consequences of instruction-tuning
and enhance the reliability of LLMs in real-world applications.

</details>


### [26] [Prune&Comp: Free Lunch for Layer-Pruned LLMs via Iterative Pruning with Magnitude Compensation](https://arxiv.org/abs/2507.18212)
*Xinrui Chen,Hongxing Zhang,Fanyi Zeng,Yongxian Wei,Yizhi Wang,Xitong Ling,Guanghao Li,Chun Yuan*

Main category: cs.CL

TL;DR: Prune&Comp is a training-free layer pruning method that mitigates performance loss by rescaling weights to compensate for magnitude gaps caused by layer removal, significantly improving pruning effectiveness.


<details>
  <summary>Details</summary>
Motivation: To address the performance degradation caused by layer pruning in LLMs due to magnitude gaps in hidden states.

Method: Estimate the magnitude gap from layer removal and rescale remaining weights offline to compensate, combined with an iterative pruning and compensation strategy.

Result: Prune&Comp nearly halves perplexity and retains 93.19% of QA performance after significant pruning, outperforming baseline methods.

Conclusion: Prune&Comp effectively enhances layer pruning by magnitude compensation, leading to better model compression with minimal performance loss.

Abstract: Layer pruning has emerged as a promising technique for compressing large
language models (LLMs) while achieving acceleration proportional to the pruning
ratio. In this work, we identify that removing any layer induces a significant
magnitude gap in hidden states, resulting in substantial performance
degradation. To address this issue, we propose Prune&Comp, a novel
plug-and-play layer pruning scheme that leverages magnitude compensation to
mitigate such gaps in a training-free manner. Specifically, we first estimate
the magnitude gap caused by layer removal and then eliminate this gap by
rescaling the remaining weights offline, with zero runtime overhead incurred.
We further demonstrate the advantages of Prune&Comp through an iterative
pruning strategy. When integrated with an iterative prune-and-compensate loop,
Prune&Comp consistently enhances existing layer pruning metrics. For instance,
when 5 layers of LLaMA-3-8B are pruned using the prevalent block influence
metric, Prune&Comp nearly halves the perplexity and retains 93.19\% of the
original model's question-answering performance, outperforming the baseline by
4.01%.

</details>


### [27] [Locate-and-Focus: Enhancing Terminology Translation in Speech Language Models](https://arxiv.org/abs/2507.18263)
*Suhang Wu,Jialong Tang,Chengyi Yang,Pei Zhang,Baosong Yang,Junhui Li,Junfeng Yao,Min Zhang,Jinsong Su*

Main category: cs.CL

TL;DR: The paper proposes a Locate-and-Focus method to improve terminology translation in speech translation by accurately locating terminology-containing speech clips and associating them with translation knowledge from multiple modalities.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of accurately translating terminology in direct speech translation amidst noise and ineffective knowledge utilization.

Method: Locates speech clips containing terminologies and links them with translation knowledge from audio and textual data to help the model focus better during translation.

Result: Improved localization of terminologies and higher success rates in terminology translation without compromising overall translation quality.

Conclusion: The proposed method effectively enhances terminology translation in speech translation systems by selectively focusing on relevant segments and leveraging multi-modal knowledge.

Abstract: Direct speech translation (ST) has garnered increasing attention nowadays,
yet the accurate translation of terminology within utterances remains a great
challenge. In this regard, current studies mainly concentrate on leveraging
various translation knowledge into ST models. However, these methods often
struggle with interference from irrelevant noise and can not fully utilize the
translation knowledge. To address these issues, in this paper, we propose a
novel Locate-and-Focus method for terminology translation. It first effectively
locates the speech clips containing terminologies within the utterance to
construct translation knowledge, minimizing irrelevant information for the ST
model. Subsequently, it associates the translation knowledge with the utterance
and hypothesis from both audio and textual modalities, allowing the ST model to
better focus on translation knowledge during translation. Experimental results
across various datasets demonstrate that our method effectively locates
terminologies within utterances and enhances the success rate of terminology
translation, while maintaining robust general translation performance.

</details>


### [28] [Zero-shot OCR Accuracy of Low-Resourced Languages: A Comparative Analysis on Sinhala and Tamil](https://arxiv.org/abs/2507.18264)
*Nevidu Jayatilleke,Nisansa de Silva*

Main category: cs.CL

TL;DR: This study evaluates the zero-shot OCR performance on Sinhala and Tamil using six different OCR engines, finding Surya best for Sinhala and Document AI best for Tamil, and introduces a new Tamil OCR benchmark dataset.


<details>
  <summary>Details</summary>
Motivation: Address the gap in OCR solutions for low-resourced languages with unique scripts, beyond well-resourced languages like English.

Method: Comparative analysis of six OCR engines on Sinhala and Tamil using five accuracy metrics, plus the creation of a synthetic Tamil OCR dataset.

Result: Surya outperformed others in Sinhala; Document AI was best for Tamil; a new Tamil dataset was introduced.

Conclusion: Different OCR systems show strengths for specific low-resourced languages; the study provides insights and new resources for future OCR development.

Abstract: Solving the problem of Optical Character Recognition (OCR) on printed text
for Latin and its derivative scripts can now be considered settled due to the
volumes of research done on English and other High-Resourced Languages (HRL).
However, for Low-Resourced Languages (LRL) that use unique scripts, it remains
an open problem. This study presents a comparative analysis of the zero-shot
performance of six distinct OCR engines on two LRLs: Sinhala and Tamil. The
selected engines include both commercial and open-source systems, aiming to
evaluate the strengths of each category. The Cloud Vision API, Surya, Document
AI, and Tesseract were evaluated for both Sinhala and Tamil, while Subasa OCR
and EasyOCR were examined for only one language due to their limitations. The
performance of these systems was rigorously analysed using five measurement
techniques to assess accuracy at both the character and word levels. According
to the findings, Surya delivered the best performance for Sinhala across all
metrics, with a WER of 2.61%. Conversely, Document AI excelled across all
metrics for Tamil, highlighted by a very low CER of 0.78%. In addition to the
above analysis, we also introduce a novel synthetic Tamil OCR benchmarking
dataset.

</details>


### [29] [StyleAdaptedLM: Enhancing Instruction Following Models with Efficient Stylistic Transfer](https://arxiv.org/abs/2507.18294)
*Pritika Ramu,Apoorv Saxena,Meghanath M Y,Varsha Sankar,Debraj Basu*

Main category: cs.CL

TL;DR: StyleAdaptedLM effectively personalizes large language models' style using Low-Rank Adaptation, maintaining instruction adherence while enhancing stylistic consistency.


<details>
  <summary>Details</summary>
Motivation: To customize LLMs with specific stylistic traits like brand voice or authorial tone for enterprise communication.

Method: Training LoRA adapters on unstructured stylistic corpora and merging them with instruction-following models.

Result: Improved stylistic consistency without compromising instruction adherence, validated through multiple datasets, models, and human evaluations.

Conclusion: StyleAdaptedLM provides an efficient method for stylistic personalization in LLMs, balancing style transfer and task performance.

Abstract: Adapting LLMs to specific stylistic characteristics, like brand voice or
authorial tones, is crucial for enterprise communication but challenging to
achieve from corpora which lacks instruction-response formatting without
compromising instruction adherence. We introduce StyleAdaptedLM, a framework
that efficiently transfers stylistic traits to instruction-following models
using Low-Rank Adaptation (LoRA). LoRA adapters are first trained on a base
model with diverse unstructured stylistic corpora, then merged with a separate
instruction-following model. This enables robust stylistic customization
without paired data or sacrificing task performance. Experiments across
multiple datasets and models demonstrate improved stylistic consistency while
preserving instruction adherence, with human evaluations confirming
brand-specific convention uptake. StyleAdaptedLM offers an efficient path for
stylistic personalization in LLMs.

</details>


### [30] [BadReasoner: Planting Tunable Overthinking Backdoors into Large Reasoning Models for Fun or Profit](https://arxiv.org/abs/2507.18305)
*Biao Yi,Zekun Fei,Jianing Geng,Tong Li,Lihai Nie,Zheli Liu,Yiming Li*

Main category: cs.CL

TL;DR: Introduction of 'overthinking backdoors' in large reasoning models (LRMs), allowing attackers to control reasoning verbosity via data poisoning, increasing resource consumption without affecting answer correctness.


<details>
  <summary>Details</summary>
Motivation: To identify and exploit vulnerabilities in LRMs related to reasoning processes, especially focusing on controlling the complexity and verbosity of reasoning.

Method: Developing a data poisoning technique that pairs a tunable trigger with verbose chain-of-thought responses, generated by instructing a teacher LLM to include redundant reasoning steps.

Result: The proposed attack reliably increases reasoning process length significantly while maintaining answer correctness, demonstrating its effectiveness and stealth.

Conclusion: The paper reveals a novel resource-consumption attack vector against LRMs, emphasizing the need for improved defenses against such tunable backdoors.

Abstract: Large reasoning models (LRMs) have emerged as a significant advancement in
artificial intelligence, representing a specialized class of large language
models (LLMs) designed to tackle complex reasoning tasks. The defining
characteristic of LRMs lies in their extensive chain-of-thought (CoT) reasoning
capabilities. In this paper, we identify a previously unexplored attack vector
against LRMs, which we term "overthinking backdoors". We advance this concept
by proposing a novel tunable backdoor, which moves beyond simple on/off attacks
to one where an attacker can precisely control the extent of the model's
reasoning verbosity. Our attack is implemented through a novel data poisoning
methodology. It pairs a tunable trigger-where the number of repetitions signals
the desired intensity-with a correspondingly verbose CoT response. These
responses are programmatically generated by instructing a teacher LLM to inject
a controlled number of redundant refinement steps into a correct reasoning
process. The approach preserves output correctness, which ensures stealth and
establishes the attack as a pure resource-consumption vector. Extensive
empirical results on various LRMs demonstrate that our method can reliably
trigger a controllable, multi-fold increase in the length of the reasoning
process, without degrading the final answer's correctness. Our source code is
available at https://github.com/FZaKK/BadReasoner.

</details>


### [31] [Uncertainty Quantification for Evaluating Machine Translation Bias](https://arxiv.org/abs/2507.18338)
*Ieva Raminta Staliūnaitė,Julius Cheng,Andreas Vlachos*

Main category: cs.CL

TL;DR: The paper investigates gender bias and uncertainty in machine translation, showing that models often lack appropriate uncertainty in ambiguous cases and that debiasing affects these cases differently.


<details>
  <summary>Details</summary>
Motivation: To improve understanding of how MT models handle gender ambiguity and bias, and to evaluate the models' uncertainty in such contexts.

Method: Analyzing model behavior using metrics of semantic uncertainty and assessing the impact of debiasing methods on ambiguous versus unambiguous instances.

Result: Models with high accuracy do not necessarily express expected uncertainty in ambiguous cases, and debiasing impacts ambiguous and unambiguous translations differently.

Conclusion: A need for better models that properly handle ambiguity and uncertainty in gender translation, and a nuanced approach to debiasing.

Abstract: In machine translation (MT), when the source sentence includes a lexeme whose
gender is not overtly marked, but whose target-language equivalent requires
gender specification, the model must infer the appropriate gender from the
context and/or external knowledge. Studies have shown that MT models exhibit
biased behaviour, relying on stereotypes even when they clash with contextual
information. We posit that apart from confidently translating using the correct
gender when it is evident from the input, models should also maintain
uncertainty about the gender when it is ambiguous. Using recently proposed
metrics of semantic uncertainty, we find that models with high translation and
gender accuracy on unambiguous instances do not necessarily exhibit the
expected level of uncertainty in ambiguous ones. Similarly, debiasing has
independent effects on ambiguous and unambiguous translation instances.

</details>


### [32] [TDR: Task-Decoupled Retrieval with Fine-Grained LLM Feedback for In-Context Learning](https://arxiv.org/abs/2507.18340)
*Yifu Chen,Bingchen Huang,Zhiling Wang,Yuanchao Du,Junfeng Luo,Lei Shen,Zhineng chen*

Main category: cs.CL

TL;DR: TDR framework improves in-context learning by better retrieving task-specific examples and utilizing fine-grained LLM feedback, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Enhance the quality of in-context learning examples by addressing the challenges in cross-task data distinction and fine-grained feedback integration.

Method: Decouples task examples and models fine-grained LLM feedback to guide retrieval training.

Result: Consistently outperforms existing methods across 30 NLP tasks, achieving state-of-the-art results.

Conclusion: TDR is an effective, plug-and-play framework that enhances example retrieval for ICL.

Abstract: In-context learning (ICL) has become a classic approach for enabling LLMs to
handle various tasks based on a few input-output examples. The effectiveness of
ICL heavily relies on the quality of these examples, and previous works which
focused on enhancing example retrieval capabilities have achieved impressive
performances. However, two challenges remain in retrieving high-quality
examples: (1) Difficulty in distinguishing cross-task data distributions, (2)
Difficulty in making the fine-grained connection between retriever output and
feedback from LLMs. In this paper, we propose a novel framework called TDR. TDR
decouples the ICL examples from different tasks, which enables the retrieval
module to retrieve examples specific to the target task within a multi-task
dataset. Furthermore, TDR models fine-grained feedback from LLMs to supervise
and guide the training of the retrieval module, which helps to retrieve
high-quality examples. We conducted extensive experiments on a suite of 30 NLP
tasks, the results demonstrate that TDR consistently improved results across
all datasets and achieves state-of-the-art performance. Meanwhile, our approach
is a plug-and-play method, which can be easily combined with various LLMs to
improve example retrieval abilities for ICL. The code is available at
https://github.com/Nnn-s/TDR.

</details>


### [33] [Hybrid Annotation for Propaganda Detection: Integrating LLM Pre-Annotations with Human Intelligence](https://arxiv.org/abs/2507.18343)
*Ariana Sahitaj,Premtim Sahitaj,Veronika Solopova,Jiaao Li,Sebastian Möller,Vera Schmitt*

Main category: cs.CL

TL;DR: The paper presents a hybrid approach combining human expertise and Large Language Models to improve propaganda detection on social media, emphasizing hierarchical taxonomy, LLM-assisted annotation, and knowledge distillation for scalable, transparent systems.


<details>
  <summary>Details</summary>
Motivation: Address challenges in propaganda detection due to task complexity and limited high-quality labeled data.

Method: Develop hierarchical taxonomy, utilize LLM-assisted pre-annotation, and apply knowledge distillation to train smaller models.

Result: Enhanced annotation consistency and efficiency, with promising results in scalable detection systems.

Conclusion: Hybrid human-AI framework improves propaganda detection, supporting transparent media ecosystems.

Abstract: Propaganda detection on social media remains challenging due to task
complexity and limited high-quality labeled data. This paper introduces a novel
framework that combines human expertise with Large Language Model (LLM)
assistance to improve both annotation consistency and scalability. We propose a
hierarchical taxonomy that organizes 14 fine-grained propaganda techniques into
three broader categories, conduct a human annotation study on the HQP dataset
that reveals low inter-annotator agreement for fine-grained labels, and
implement an LLM-assisted pre-annotation pipeline that extracts propagandistic
spans, generates concise explanations, and assigns local labels as well as a
global label. A secondary human verification study shows significant
improvements in both agreement and time-efficiency. Building on this, we
fine-tune smaller language models (SLMs) to perform structured annotation.
Instead of fine-tuning on human annotations, we train on high-quality
LLM-generated data, allowing a large model to produce these annotations and a
smaller model to learn to generate them via knowledge distillation. Our work
contributes towards the development of scalable and robust propaganda detection
systems, supporting the idea of transparent and accountable media ecosystems in
line with SDG 16. The code is publicly available at our GitHub repository.

</details>


### [34] [CLEAR: Error Analysis via LLM-as-a-Judge Made Easy](https://arxiv.org/abs/2507.18392)
*Asaf Yehudai,Lilach Eden,Yotam Perlitz,Roy Bar-Haim,Michal Shmueli-Scheuer*

Main category: cs.CL

TL;DR: CLEAR is an open-source tool for detailed error analysis of Large Language Models, providing insights beyond simple scores by outlining specific issues and enabling interactive, instance-level investigation.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of current LLM evaluation methods that only produce overall scores, obscuring actionable insights.

Method: Introducing CLEAR, which generates textual feedback and error issues, quantifies their prevalence, and offers an interactive dashboard for comprehensive analysis.

Result: CLEAR enables detailed understanding of LLM errors, as demonstrated on RAG and Math benchmarks, enhancing interpretability and diagnostic capabilities.

Conclusion: CLEAR advances LLM evaluation by providing an actionable, detailed, and interactive error analysis framework.

Abstract: The evaluation of Large Language Models (LLMs) increasingly relies on other
LLMs acting as judges. However, current evaluation paradigms typically yield a
single score or ranking, answering which model is better but not why. While
essential for benchmarking, these top-level scores obscure the specific,
actionable reasons behind a model's performance. To bridge this gap, we
introduce CLEAR, an interactive, open-source package for LLM-based error
analysis. CLEAR first generates per-instance textual feedback, then it creates
a set of system-level error issues, and quantifies the prevalence of each
identified issue. Our package also provides users with an interactive dashboard
that allows for a comprehensive error analysis through aggregate
visualizations, applies interactive filters to isolate specific issues or score
ranges, and drills down to the individual instances that exemplify a particular
behavioral pattern. We demonstrate CLEAR analysis for RAG and Math benchmarks,
and showcase its utility through a user case study.

</details>


### [35] [FinDPO: Financial Sentiment Analysis for Algorithmic Trading through Preference Optimization of LLMs](https://arxiv.org/abs/2507.18417)
*Giorgos Iacovides,Wuyang Zhou,Danilo Mandic*

Main category: cs.CL

TL;DR: FinDPO, a new finance-specific LLM framework, enhances sentiment analysis by aligning human preferences, outperforming existing models and enabling profitable trading strategies.


<details>
  <summary>Details</summary>
Motivation: Address the limitations of supervised fine-tuned LLMs in financial sentiment analysis, specifically their poor generalization to unseen data and domain-specific language.

Method: Introduce FinDPO, a framework based on post-training human preference alignment via DPO, and develop a 'logit-to-score' conversion for sentiment prediction.

Result: FinDPO outperforms existing models by 11%, achieves state-of-the-art sentiment classification, and demonstrates profitable trading with 67% annual returns and a Sharpe ratio of 2.0.

Conclusion: FinDPO advances financial sentiment analysis and trading strategies by effectively aligning LLM outputs with human preferences and providing robust, profitable predictions.

Abstract: Opinions expressed in online finance-related textual data are having an
increasingly profound impact on trading decisions and market movements. This
trend highlights the vital role of sentiment analysis as a tool for quantifying
the nature and strength of such opinions. With the rapid development of
Generative AI (GenAI), supervised fine-tuned (SFT) large language models (LLMs)
have become the de facto standard for financial sentiment analysis. However,
the SFT paradigm can lead to memorization of the training data and often fails
to generalize to unseen samples. This is a critical limitation in financial
domains, where models must adapt to previously unobserved events and the
nuanced, domain-specific language of finance. To this end, we introduce FinDPO,
the first finance-specific LLM framework based on post-training human
preference alignment via Direct Preference Optimization (DPO). The proposed
FinDPO achieves state-of-the-art performance on standard sentiment
classification benchmarks, outperforming existing supervised fine-tuned models
by 11% on the average. Uniquely, the FinDPO framework enables the integration
of a fine-tuned causal LLM into realistic portfolio strategies through a novel
'logit-to-score' conversion, which transforms discrete sentiment predictions
into continuous, rankable sentiment scores (probabilities). In this way,
simulations demonstrate that FinDPO is the first sentiment-based approach to
maintain substantial positive returns of 67% annually and strong risk-adjusted
performance, as indicated by a Sharpe ratio of 2.0, even under realistic
transaction costs of 5 basis points (bps).

</details>


### [36] [AraTable: Benchmarking LLMs' Reasoning and Understanding of Arabic Tabular Data](https://arxiv.org/abs/2507.18442)
*Rana Alshaikh,Israa Alghanmi,Shelan Jeawak*

Main category: cs.CL

TL;DR: AraTable benchmark evaluates Arabic language models on tabular reasoning, revealing strengths on simple tasks but challenges in complex reasoning.


<details>
  <summary>Details</summary>
Motivation: Address the lack of Arabic-specific benchmarks and resources for LLMs on tabular data.

Method: Hybrid pipeline with LLM generation, human filtering, and automated evaluation with self-deliberation.

Result: LLMs perform well on simple tasks but poorly on complex reasoning; a new automated evaluation framework is effective.

Conclusion: AraTable is a valuable resource to enhance Arabic structured data processing and can guide future improvements.

Abstract: The cognitive and reasoning abilities of large language models (LLMs) have
enabled remarkable progress in natural language processing. However, their
performance in interpreting structured data, especially in tabular formats,
remains limited. Although benchmarks for English tabular data are widely
available, Arabic is still underrepresented because of the limited availability
of public resources and its unique language features. To address this gap, we
present AraTable, a novel and comprehensive benchmark designed to evaluate the
reasoning and understanding capabilities of LLMs when applied to Arabic tabular
data. AraTable consists of various evaluation tasks, such as direct question
answering, fact verification, and complex reasoning, involving a wide range of
Arabic tabular sources. Our methodology follows a hybrid pipeline, where
initial content is generated by LLMs and subsequently filtered and verified by
human experts to ensure high dataset quality. Initial analyses using AraTable
show that, while LLMs perform adequately on simpler tabular tasks such as
direct question answering, they continue to face significant cognitive
challenges when tasks require deeper reasoning and fact verification. This
indicates that there are substantial opportunities for future work to improve
performance on complex tabular reasoning tasks. We also propose a fully
automated evaluation framework that uses a self-deliberation mechanism and
achieves performance nearly identical to that of human judges. This research
provides a valuable, publicly available resource and evaluation framework that
can help accelerate the development of foundational models for processing and
analysing Arabic structured data.

</details>


### [37] [Restoring Rhythm: Punctuation Restoration Using Transformer Models for Bangla, a Low-Resource Language](https://arxiv.org/abs/2507.18448)
*Md Obyedullahil Mamun,Md Adyelullahil Mamun,Arif Ahmad,Md. Imran Hossain Emu*

Main category: cs.CL

TL;DR: Transformer-based model effectively restores punctuation in Bangla text, achieving high accuracy and generalizing well across different data domains.


<details>
  <summary>Details</summary>
Motivation: Enhance readability and accuracy of ASR in low-resource language Bangla by restoring punctuation.

Method: Used XLM-RoBERTa-large transformer model trained on a newly constructed and augmented dataset for punctuation prediction.

Result: Achieved up to 97.1% accuracy on news text and strong generalization on reference and ASR sets.

Conclusion: Proposed approach establishes a solid baseline for Bangla punctuation restoration, with publicly available datasets and code for future research.

Abstract: Punctuation restoration enhances the readability of text and is critical for
post-processing tasks in Automatic Speech Recognition (ASR), especially for
low-resource languages like Bangla. In this study, we explore the application
of transformer-based models, specifically XLM-RoBERTa-large, to automatically
restore punctuation in unpunctuated Bangla text. We focus on predicting four
punctuation marks: period, comma, question mark, and exclamation mark across
diverse text domains. To address the scarcity of annotated resources, we
constructed a large, varied training corpus and applied data augmentation
techniques. Our best-performing model, trained with an augmentation factor of
alpha = 0.20%, achieves an accuracy of 97.1% on the News test set, 91.2% on the
Reference set, and 90.2% on the ASR set.
  Results show strong generalization to reference and ASR transcripts,
demonstrating the model's effectiveness in real-world, noisy scenarios. This
work establishes a strong baseline for Bangla punctuation restoration and
contributes publicly available datasets and code to support future research in
low-resource NLP.

</details>


### [38] [Generation of Synthetic Clinical Text: A Systematic Review](https://arxiv.org/abs/2507.18451)
*Basel Alshaikhdeeb,Ahmed Abdelmonem Hemedan,Soumyabrata Ghosh,Irina Balaur,Venkata Satagopam*

Main category: cs.CL

TL;DR: Systematic review of synthetic medical text generation highlighting techniques, evaluation methods, and privacy concerns.


<details>
  <summary>Details</summary>
Motivation: Address challenges like data sparsity and privacy in clinical NLP through synthetic text generation.

Method: Literature review and quantitative analysis of 94 relevant articles from multiple databases.

Result: Transformer models, especially GPTs, are predominant; utility is the main evaluation metric; synthetic text aids NLP tasks but privacy concerns remain.

Conclusion: Advances in synthetic medical text generation could accelerate clinical NLP workflows despite ongoing privacy issues.

Abstract: Generating clinical synthetic text represents an effective solution for
common clinical NLP issues like sparsity and privacy. This paper aims to
conduct a systematic review on generating synthetic medical free-text by
formulating quantitative analysis to three research questions concerning (i)
the purpose of generation, (ii) the techniques, and (iii) the evaluation
methods. We searched PubMed, ScienceDirect, Web of Science, Scopus, IEEE,
Google Scholar, and arXiv databases for publications associated with generating
synthetic medical unstructured free-text. We have identified 94 relevant
articles out of 1,398 collected ones. A great deal of attention has been given
to the generation of synthetic medical text from 2018 onwards, where the main
purpose of such a generation is towards text augmentation, assistive writing,
corpus building, privacy-preserving, annotation, and usefulness. Transformer
architectures were the main predominant technique used to generate the text,
especially the GPTs. On the other hand, there were four main aspects of
evaluation, including similarity, privacy, structure, and utility, where
utility was the most frequent method used to assess the generated synthetic
medical text. Although the generated synthetic medical text demonstrated a
moderate possibility to act as real medical documents in different downstream
NLP tasks, it has proven to be a great asset as augmented, complementary to the
real documents, towards improving the accuracy and overcoming
sparsity/undersampling issues. Yet, privacy is still a major issue behind
generating synthetic medical text, where more human assessments are needed to
check for the existence of any sensitive information. Despite that, advances in
generating synthetic medical text will considerably accelerate the adoption of
workflows and pipeline development, discarding the time-consuming legalities of
data transfer.

</details>


### [39] [Not All Features Deserve Attention: Graph-Guided Dependency Learning for Tabular Data Generation with Language Models](https://arxiv.org/abs/2507.18504)
*Zheyu Zhang,Shuo Yang,Bardh Prenkaj,Gjergji Kasneci*

Main category: cs.CL

TL;DR: GraDe improves LLM-based tabular data generation by integrating dependency graphs into the attention mechanism, leading to better performance on complex datasets.


<details>
  <summary>Details</summary>
Motivation: To address the mismatch between LLMs' attention and the inherent sparse dependencies in tabular data.

Method: Introducing GraDe, which incorporates externally guided dependency graphs into the attention process to focus on key feature interactions.

Result: Achieved up to 12% improvement on complex datasets and competitive results in synthetic data quality.

Conclusion: GraDe is an effective, minimally intrusive approach for structure-aware tabular data modeling with LLMs.

Abstract: Large Language Models (LLMs) have shown strong potential for tabular data
generation by modeling textualized feature-value pairs. However, tabular data
inherently exhibits sparse feature-level dependencies, where many feature
interactions are structurally insignificant. This creates a fundamental
mismatch as LLMs' self-attention mechanism inevitably distributes focus across
all pairs, diluting attention on critical relationships, particularly in
datasets with complex dependencies or semantically ambiguous features. To
address this limitation, we propose GraDe (Graph-Guided Dependency Learning), a
novel method that explicitly integrates sparse dependency graphs into LLMs'
attention mechanism. GraDe employs a lightweight dynamic graph learning module
guided by externally extracted functional dependencies, prioritizing key
feature interactions while suppressing irrelevant ones. Our experiments across
diverse real-world datasets demonstrate that GraDe outperforms existing
LLM-based approaches by up to 12% on complex datasets while achieving
competitive results with state-of-the-art approaches in synthetic data quality.
Our method is minimally intrusive yet effective, offering a practical solution
for structure-aware tabular data modeling with LLMs.

</details>


### [40] [The Moral Gap of Large Language Models](https://arxiv.org/abs/2507.18523)
*Maciej Skorski,Alina Landowska*

Main category: cs.CL

TL;DR: Large language models (LLMs) underperform in moral foundation detection compared to fine-tuned transformers, especially in identifying moral content.


<details>
  <summary>Details</summary>
Motivation: To evaluate and compare the performance of LLMs and fine-tuned transformers in moral foundation detection across social media datasets.

Method: The study conducts a comprehensive comparison using ROC, PR, and DET curve analysis on Twitter and Reddit datasets.

Result: LLMs show significant gaps, including high false negatives and under-detection of moral content, outperforming prompting but still inferior to fine-tuning.

Conclusion: Task-specific fine-tuning remains more effective than prompting for moral reasoning tasks.

Abstract: Moral foundation detection is crucial for analyzing social discourse and
developing ethically-aligned AI systems. While large language models excel
across diverse tasks, their performance on specialized moral reasoning remains
unclear.
  This study provides the first comprehensive comparison between
state-of-the-art LLMs and fine-tuned transformers across Twitter and Reddit
datasets using ROC, PR, and DET curve analysis.
  Results reveal substantial performance gaps, with LLMs exhibiting high false
negative rates and systematic under-detection of moral content despite prompt
engineering efforts. These findings demonstrate that task-specific fine-tuning
remains superior to prompting for moral reasoning applications.

</details>


### [41] [Effective Multi-Task Learning for Biomedical Named Entity Recognition](https://arxiv.org/abs/2507.18542)
*João Ruano,Gonçalo M. Correia,Leonor Barreiros,Afonso Mendes*

Main category: cs.CL

TL;DR: SRU-NER is a new model for biomedical NER that handles nested entities and integrates multiple datasets to improve generalization.


<details>
  <summary>Details</summary>
Motivation: Biomedical NER faces challenges due to complex terminology and inconsistent annotations across datasets.

Method: SRU-NER uses slot-based recurrent units and multi-task learning to handle nested entities and dataset inconsistencies, with dynamic loss adjustment.

Result: SRU-NER performs competitively on biomedical and general-domain NER tasks, with better cross-domain generalization.

Conclusion: The model effectively addresses annotation gaps and enhances NER performance across datasets.

Abstract: Biomedical Named Entity Recognition presents significant challenges due to
the complexity of biomedical terminology and inconsistencies in annotation
across datasets. This paper introduces SRU-NER (Slot-based Recurrent Unit NER),
a novel approach designed to handle nested named entities while integrating
multiple datasets through an effective multi-task learning strategy. SRU-NER
mitigates annotation gaps by dynamically adjusting loss computation to avoid
penalizing predictions of entity types absent in a given dataset. Through
extensive experiments, including a cross-corpus evaluation and human assessment
of the model's predictions, SRU-NER achieves competitive performance in
biomedical and general-domain NER tasks, while improving cross-domain
generalization.

</details>


### [42] [GLiNER2: An Efficient Multi-Task Information Extraction System with Schema-Driven Interface](https://arxiv.org/abs/2507.18546)
*Urchade Zaratiana,Gil Pasternak,Oliver Boyd,George Hurn-Maloney,Ash Lewis*

Main category: cs.CL

TL;DR: GLiNER2 is a unified, efficient framework for multiple NLP extraction tasks, offering competitive performance and easier deployment.


<details>
  <summary>Details</summary>
Motivation: Address the need for a versatile and computationally efficient model capable of handling various NLP extraction tasks, reducing reliance on large language models.

Method: Enhance the original GLiNER with a pretrained transformer encoder to support named entity recognition, text classification, and hierarchical data extraction within one model, using a schema-based interface.

Result: Achieves competitive performance across tasks, with improved deployment efficiency and accessibility compared to LLM-based solutions.

Conclusion: GLiNER2 is a practical, open-source tool that simplifies multi-task NLP extraction with high efficiency.

Abstract: Information extraction (IE) is fundamental to numerous NLP applications, yet
existing solutions often require specialized models for different tasks or rely
on computationally expensive large language models. We present GLiNER2, a
unified framework that enhances the original GLiNER architecture to support
named entity recognition, text classification, and hierarchical structured data
extraction within a single efficient model. Built pretrained transformer
encoder architecture, GLiNER2 maintains CPU efficiency and compact size while
introducing multi-task composition through an intuitive schema-based interface.
Our experiments demonstrate competitive performance across extraction and
classification tasks with substantial improvements in deployment accessibility
compared to LLM-based alternatives. We release GLiNER2 as an open-source
pip-installable library with pre-trained models and documentation at
https://github.com/fastino-ai/GLiNER2.

</details>


### [43] [GIIFT: Graph-guided Inductive Image-free Multimodal Machine Translation](https://arxiv.org/abs/2507.18562)
*Jiafeng Xiong,Yuting Zhao*

Main category: cs.CL

TL;DR: GIIFT is a novel framework that improves multimodal machine translation by using scene graphs and cross-modal attention, achieving state-of-the-art results without needing images at inference.


<details>
  <summary>Details</summary>
Motivation: To address challenges in leveraging visual information and domain generalization in multimodal machine translation.

Method: Constructs multimodal scene graphs and employs a graph-guided inductive approach with a cross-modal attention network to learn and generalize multimodal knowledge.

Result: Outperforms existing methods on Multi30K and WMT benchmarks, achieving state-of-the-art results and demonstrating effective image-free inference.

Conclusion: GIIFT advances multimodal machine translation by enabling better modality integration and generalization without reliance on images during inference.

Abstract: Multimodal Machine Translation (MMT) has demonstrated the significant help of
visual information in machine translation. However, existing MMT methods face
challenges in leveraging the modality gap by enforcing rigid visual-linguistic
alignment whilst being confined to inference within their trained multimodal
domains. In this work, we construct novel multimodal scene graphs to preserve
and integrate modality-specific information and introduce GIIFT, a two-stage
Graph-guided Inductive Image-Free MMT framework that uses a cross-modal Graph
Attention Network adapter to learn multimodal knowledge in a unified fused
space and inductively generalize it to broader image-free translation domains.
Experimental results on the Multi30K dataset of English-to-French and
English-to-German tasks demonstrate that our GIIFT surpasses existing
approaches and achieves the state-of-the-art, even without images during
inference. Results on the WMT benchmark show significant improvements over the
image-free translation baselines, demonstrating the strength of GIIFT towards
inductive image-free inference.

</details>


### [44] [AQuilt: Weaving Logic and Self-Inspection into Low-Cost, High-Relevance Data Synthesis for Specialist LLMs](https://arxiv.org/abs/2507.18584)
*Xiaopeng Ke,Hexuan Deng,Xuebo Liu,Jun Rao,Zhenxi Song,Jun Yu,Min Zhang*

Main category: cs.CL

TL;DR: AQuilt is a new framework for creating instruction-tuning data for specialized domains using unlabeled data, improving performance and efficiency.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing data synthesis methods in specialized domains, which are costly and limited in generalization.

Method: Constructing a data synthesis model from unlabeled data, incorporating logic, inspection, and customizable instructions for diverse tasks.

Result: Generated a large, relevant dataset for training, achieving comparable performance to state-of-the-art while reducing costs.

Conclusion: AQuilt effectively enhances LLM performance in specialized domains with efficient data generation.

Abstract: Despite the impressive performance of large language models (LLMs) in general
domains, they often underperform in specialized domains. Existing approaches
typically rely on data synthesis methods and yield promising results by using
unlabeled data to capture domain-specific features. However, these methods
either incur high computational costs or suffer from performance limitations,
while also demonstrating insufficient generalization across different tasks. To
address these challenges, we propose AQuilt, a framework for constructing
instruction-tuning data for any specialized domains from corresponding
unlabeled data, including Answer, Question, Unlabeled data, Inspection, Logic,
and Task type. By incorporating logic and inspection, we encourage reasoning
processes and self-inspection to enhance model performance. Moreover,
customizable task instructions enable high-quality data generation for any
task. As a result, we construct a dataset of 703k examples to train a powerful
data synthesis model. Experiments show that AQuilt is comparable to DeepSeek-V3
while utilizing just 17% of the production cost. Further analysis demonstrates
that our generated data exhibits higher relevance to downstream tasks. Source
code, models, and scripts are available at https://github.com/Krueske/AQuilt.

</details>


### [45] [Hybrid Tokenization Strategy for DNA Language Model using Byte Pair Encoding and K-MER Methods](https://arxiv.org/abs/2507.18570)
*Ganesh Sapkota,Md Hasibur Rahman*

Main category: cs.CL

TL;DR: A novel hybrid tokenization strategy combining 6-mer and BPE enhances DNA Language Models, improving prediction accuracy and capturing sequence context.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of traditional k-mer tokenization in capturing both local and global DNA sequence information.

Method: Merging 6-mer tokens with BPE tokens generated through 600 cycles, and training a DLM on this hybrid vocabulary, evaluated by next-k-mer prediction tasks.

Result: Achieved higher prediction accuracies for 3-mer, 4-mer, and 5-mer tasks compared to existing models like NT, DNABERT2, and GROVER.

Conclusion: Hybrid tokenization effectively captures local and global sequence features, setting a new foundation for genomic language modeling.

Abstract: This paper presents a novel hybrid tokenization strategy that enhances the
performance of DNA Language Models (DLMs) by combining 6-mer tokenization with
Byte Pair Encoding (BPE-600). Traditional k-mer tokenization is effective at
capturing local DNA sequence structures but often faces challenges, including
uneven token distribution and a limited understanding of global sequence
context. To address these limitations, we propose merging unique 6mer tokens
with optimally selected BPE tokens generated through 600 BPE cycles. This
hybrid approach ensures a balanced and context-aware vocabulary, enabling the
model to capture both short and long patterns within DNA sequences
simultaneously. A foundational DLM trained on this hybrid vocabulary was
evaluated using next-k-mer prediction as a fine-tuning task, demonstrating
significantly improved performance. The model achieved prediction accuracies of
10.78% for 3-mers, 10.1% for 4-mers, and 4.12% for 5-mers, outperforming
state-of-the-art models such as NT, DNABERT2, and GROVER. These results
highlight the ability of the hybrid tokenization strategy to preserve both the
local sequence structure and global contextual information in DNA modeling.
This work underscores the importance of advanced tokenization methods in
genomic language modeling and lays a robust foundation for future applications
in downstream DNA sequence analysis and biological research.

</details>


### [46] [Wide-In, Narrow-Out: Revokable Decoding for Efficient and Effective DLLMs](https://arxiv.org/abs/2507.18578)
*Feng Hong,Geng Yu,Yushi Ye,Haicheng Huang,Huangjie Zheng,Ya Zhang,Yanfeng Wang,Jiangchao Yao*

Main category: cs.CL

TL;DR: WINO is a decoding algorithm for DLLMs that improves the quality-speed trade-off by enabling revokable, parallel decoding using a draft-and-verify mechanism.


<details>
  <summary>Details</summary>
Motivation: Existing DLLMs suffer from a quality-speed trade-off due to irreversible decoding, leading to performance degradation at faster speeds.

Method: WINO employs a parallel draft-and-verify approach, drafting multiple tokens simultaneously and verifying them using bidirectional context to refine suspicious tokens.

Result: WINO significantly improves inference speed and accuracy on various benchmarks, such as GSM8K and Flickr30K, outperforming existing methods.

Conclusion: WINO effectively addresses the quality-speed trade-off in DLLMs, providing a decoding strategy that enhances both efficiency and performance.

Abstract: Diffusion Large Language Models (DLLMs) have emerged as a compelling
alternative to Autoregressive models, designed for fast parallel generation.
However, existing DLLMs are plagued by a severe quality-speed trade-off, where
faster parallel decoding leads to significant performance degradation. We
attribute this to the irreversibility of standard decoding in DLLMs, which is
easily polarized into the wrong decoding direction along with early error
context accumulation. To resolve this, we introduce Wide-In, Narrow-Out (WINO),
a training-free decoding algorithm that enables revokable decoding in DLLMs.
WINO employs a parallel draft-and-verify mechanism, aggressively drafting
multiple tokens while simultaneously using the model's bidirectional context to
verify and re-mask suspicious ones for refinement. Verified in open-source
DLLMs like LLaDA and MMaDA, WINO is shown to decisively improve the
quality-speed trade-off. For instance, on the GSM8K math benchmark, it
accelerates inference by 6$\times$ while improving accuracy by 2.58%; on
Flickr30K captioning, it achieves a 10$\times$ speedup with higher performance.
More comprehensive experiments are conducted to demonstrate the superiority and
provide an in-depth understanding of WINO.

</details>


### [47] [System Report for CCL25-Eval Task 10: SRAG-MAV for Fine-Grained Chinese Hate Speech Recognition](https://arxiv.org/abs/2507.18580)
*Jiahao Wang,Ramen Liu,Longhui Zhang,Jing Li*

Main category: cs.CL

TL;DR: The paper introduces a new framework, SRAG-MAV, for fine-grained Chinese hate speech recognition, achieving superior results on a benchmark dataset.


<details>
  <summary>Details</summary>
Motivation: Improve accuracy and stability in Chinese hate speech recognition by leveraging task reformulation, retrieval-augmented generation, and multi-round voting.

Method: Reformulating quadruplet extraction into triplet extraction, using dynamic retrieval for contextual prompts, and multi-round inference with voting, based on the Qwen2.5-7B model.

Result: The system significantly outperforms baselines, achieving higher scores on the STATE ToxiCN dataset.

Conclusion: The proposed SRAG-MAV framework effectively enhances Chinese hate speech recognition performance.

Abstract: This paper presents our system for CCL25-Eval Task 10, addressing
Fine-Grained Chinese Hate Speech Recognition (FGCHSR). We propose a novel
SRAG-MAV framework that synergistically integrates task reformulation(TR),
Self-Retrieval-Augmented Generation (SRAG), and Multi-Round Accumulative Voting
(MAV). Our method reformulates the quadruplet extraction task into triplet
extraction, uses dynamic retrieval from the training set to create contextual
prompts, and applies multi-round inference with voting to improve output
stability and performance. Our system, based on the Qwen2.5-7B model, achieves
a Hard Score of 26.66, a Soft Score of 48.35, and an Average Score of 37.505 on
the STATE ToxiCN dataset, significantly outperforming baselines such as GPT-4o
(Average Score 15.63) and fine-tuned Qwen2.5-7B (Average Score 35.365). The
code is available at https://github.com/king-wang123/CCL25-SRAG-MAV.

</details>


### [48] [TRPrompt: Bootstrapping Query-Aware Prompt Optimization from Textual Rewards](https://arxiv.org/abs/2507.18618)
*Andreea Nica,Ivan Zakazov,Nicolas Mario Baldwin,Saibo Geng,Robert West*

Main category: cs.CL

TL;DR: TRPrompt unifies heuristic-based and reward-based prompt optimization methods, using textual feedback to train a prompt model without prior datasets, achieving state-of-the-art results in math problem datasets.


<details>
  <summary>Details</summary>
Motivation: To improve prompt optimization for LLMs by integrating textual feedback into training, combining existing approaches for better performance.

Method: Introducing the TRPrompt framework that directly incorporates textual rewards into prompt training, iteratively refining prompts without prior datasets.

Result: TRPrompt effectively generates high-quality, query-specific prompts, achieving state-of-the-art performance on challenging math datasets GSMHard and MATH.

Conclusion: The framework demonstrates that textual reward-based prompt training enhances LLM reasoning, offering a unified, training-free approach with strong empirical results.

Abstract: Prompt optimization improves the reasoning abilities of large language models
(LLMs) without requiring parameter updates to the target model. Following
heuristic-based "Think step by step" approaches, the field has evolved in two
main directions: while one group of methods uses textual feedback to elicit
improved prompts from general-purpose LLMs in a training-free way, a concurrent
line of research relies on numerical rewards to train a special prompt model,
tailored for providing optimal prompts to the target model. In this paper, we
introduce the Textual Reward Prompt framework (TRPrompt), which unifies these
approaches by directly incorporating textual feedback into training of the
prompt model. Our framework does not require prior dataset collection and is
being iteratively improved with the feedback on the generated prompts. When
coupled with the capacity of an LLM to internalize the notion of what a "good"
prompt is, the high-resolution signal provided by the textual rewards allows us
to train a prompt model yielding state-of-the-art query-specific prompts for
the problems from the challenging math datasets GSMHard and MATH.

</details>


### [49] [Checklists Are Better Than Reward Models For Aligning Language Models](https://arxiv.org/abs/2507.18624)
*Vijay Viswanathan,Yanchao Sun,Shuang Ma,Xiang Kong,Meng Cao,Graham Neubig,Tongshuang Wu*

Main category: cs.CL

TL;DR: RLCF uses checklist-based feedback to improve language model alignment across various benchmarks, outperforming other methods.


<details>
  <summary>Details</summary>
Motivation: Enhance instruction-following capabilities of language models using flexible, instruction-specific feedback.

Method: Extracts checklists from instructions, evaluates responses with AI judges and verifier programs, then uses these scores as rewards for reinforcement learning.

Result: Improved performance across all tested benchmarks, including significant gains in satisfaction rate, InFoBench score, and win rate on Arena-Hard.

Conclusion: Checklist feedback is a valuable tool for broadening the capability of language models to follow diverse user instructions.

Abstract: Language models must be adapted to understand and follow user instructions.
Reinforcement learning is widely used to facilitate this -- typically using
fixed criteria such as "helpfulness" and "harmfulness". In our work, we instead
propose using flexible, instruction-specific criteria as a means of broadening
the impact that reinforcement learning can have in eliciting instruction
following. We propose "Reinforcement Learning from Checklist Feedback" (RLCF).
From instructions, we extract checklists and evaluate how well responses
satisfy each item - using both AI judges and specialized verifier programs -
then combine these scores to compute rewards for RL. We compare RLCF with other
alignment methods applied to a strong instruction following model
(Qwen2.5-7B-Instruct) on five widely-studied benchmarks -- RLCF is the only
method to improve performance on every benchmark, including a 4-point boost in
hard satisfaction rate on FollowBench, a 6-point increase on InFoBench, and a
3-point rise in win rate on Arena-Hard. These results establish checklist
feedback as a key tool for improving language models' support of queries that
express a multitude of needs.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [50] [An advanced AI driven database system](https://arxiv.org/abs/2507.17778)
*M. Tedeschi,S. Rizwan,C. Shringi,V. Devram Chandgir,S. Belich*

Main category: cs.DB

TL;DR: An AI-supported database system using NLP and ML to improve usability, automation, and performance, reducing technical expertise needs.


<details>
  <summary>Details</summary>
Motivation: Current databases are complex and difficult for non-experts to use, limiting their accessibility.

Method: Integrating Large Language Models (LLMs), NLP, and machine learning for automatic schema inference, query generation, and performance optimization.

Result: Development of a system that automatically manages schema, queries, and formats, simplifying database interaction.

Conclusion: The system alleviates traditional database limitations by automating complex tasks, making data management more accessible.

Abstract: Contemporary database systems, while effective, suffer severe issues related
to complexity and usability, especially among individuals who lack technical
expertise but are unfamiliar with query languages like Structured Query
Language (SQL). This paper presents a new database system supported by
Artificial Intelligence (AI), which is intended to improve the management of
data using natural language processing (NLP) - based intuitive interfaces, and
automatic creation of structured queries and semi-structured data formats like
yet another markup language (YAML), java script object notation (JSON), and
application program interface (API) documentation. The system is intended to
strengthen the potential of databases through the integration of Large Language
Models (LLMs) and advanced machine learning algorithms. The integration is
purposed to allow the automation of fundamental tasks such as data modeling,
schema creation, query comprehension, and performance optimization. We present
in this paper a system that aims to alleviate the main problems with current
database technologies. It is meant to reduce the need for technical skills,
manual tuning for better performance, and the potential for human error. The AI
database employs generative schema inference and format selection to build its
schema models and execution formats.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [51] [ASP-Assisted Symbolic Regression: Uncovering Hidden Physics in Fluid Mechanics](https://arxiv.org/abs/2507.17777)
*Theofanis Aravanis,Grigorios Chrimatopoulos,Mohammad Ferdows,Michalis Xenos,Efstratios Em Tzirtzilakis*

Main category: cs.AI

TL;DR: The paper demonstrates how symbolic regression (SR) can derive interpretable equations modeling 3D fluid flow, and proposes a hybrid SR/ASP framework to ensure models adhere to physical principles.


<details>
  <summary>Details</summary>
Motivation: To advance understanding of fluid physics and improve model interpretability compared to black-box machine learning methods.

Method: Applying SR with the PySR library to fluid simulation data and integrating it with Answer Set Programming (ASP) for domain knowledge enforcement.

Result: Derivation of symbolic equations accurately representing flow dynamics, consistent with analytical solutions; hybrid framework enhances physical plausibility.

Conclusion: SR can produce concise, interpretable models of complex flows, and combining SR with knowledge-representation methods like ASP enhances their reliability and adherence to domain principles.

Abstract: Unlike conventional Machine-Learning (ML) approaches, often criticized as
"black boxes", Symbolic Regression (SR) stands out as a powerful tool for
revealing interpretable mathematical relationships in complex physical systems,
requiring no a priori assumptions about models' structures. Motivated by the
recognition that, in fluid mechanics, an understanding of the underlying flow
physics is as crucial as accurate prediction, this study applies SR to model a
fundamental three-dimensional (3D) incompressible flow in a rectangular
channel, focusing on the (axial) velocity and pressure fields under laminar
conditions. By employing the PySR library, compact symbolic equations were
derived directly from numerical simulation data, revealing key characteristics
of the flow dynamics. These equations not only approximate the parabolic
velocity profile and pressure drop observed in the studied fluid flow, but also
perfectly coincide with analytical solutions from the literature. Furthermore,
we propose an innovative approach that integrates SR with the
knowledge-representation framework of Answer Set Programming (ASP), combining
the generative power of SR with the declarative reasoning strengths of ASP. The
proposed hybrid SR/ASP framework ensures that the SR-generated symbolic
expressions are not only statistically accurate, but also physically plausible,
adhering to domain-specific principles. Overall, the study highlights two key
contributions: SR's ability to simplify complex flow behaviours into concise,
interpretable equations, and the potential of knowledge-representation
approaches to improve the reliability and alignment of data-driven SR models
with domain principles. Insights from the examined 3D channel flow pave the way
for integrating such hybrid approaches into efficient frameworks, [...] where
explainable predictions and real-time data analysis are crucial.

</details>


### [52] [I2I-STRADA -- Information to Insights via Structured Reasoning Agent for Data Analysis](https://arxiv.org/abs/2507.17874)
*SaiBarath Sundar,Pranav Satheesan,Udayaadithya Avadhanam*

Main category: cs.AI

TL;DR: I2I-STRADA formalizes structured reasoning in data analysis with a modular, cognitive workflow, outperforming prior systems.


<details>
  <summary>Details</summary>
Motivation: To address the lack of structured reasoning processes in current agentic data analysis systems, which often rely on general-purpose LLMs.

Method: Introducing I2I-STRADA, an agentic architecture that models analytical reasoning through modular sub-tasks reflecting cognitive steps.

Result: I2I-STRADA outperforms previous systems in planning coherence and insight alignment on benchmarks.

Conclusion: Structured cognitive workflows are crucial for effective agentic data analysis systems.

Abstract: Recent advances in agentic systems for data analysis have emphasized
automation of insight generation through multi-agent frameworks, and
orchestration layers. While these systems effectively manage tasks like query
translation, data transformation, and visualization, they often overlook the
structured reasoning process underlying analytical thinking. Reasoning large
language models (LLMs) used for multi-step problem solving are trained as
general-purpose problem solvers. As a result, their reasoning or thinking steps
do not adhere to fixed processes for specific tasks. Real-world data analysis
requires a consistent cognitive workflow: interpreting vague goals, grounding
them in contextual knowledge, constructing abstract plans, and adapting
execution based on intermediate outcomes. We introduce I2I-STRADA
(Information-to-Insight via Structured Reasoning Agent for Data Analysis), an
agentic architecture designed to formalize this reasoning process. I2I-STRADA
focuses on modeling how analysis unfolds via modular sub-tasks that reflect the
cognitive steps of analytical reasoning. Evaluations on the DABstep and DABench
benchmarks show that I2I-STRADA outperforms prior systems in planning coherence
and insight alignment, highlighting the importance of structured cognitive
workflows in agent design for data analysis.

</details>


### [53] [SMARTAPS: Tool-augmented LLMs for Operations Management](https://arxiv.org/abs/2507.17927)
*Timothy Tin Long Yu,Mahdi Mostajabdaveh,Jabo Serge Byusa,Rindra Ramamonjison,Giuseppe Carenini,Kun Mao,Zirui Zhou,Yong Zhang*

Main category: cs.AI

TL;DR: SmartAPS is a conversational AI system built on LLMs that enables supply chain planners to interact with an advanced planning system via natural language, making it more accessible and user-friendly.


<details>
  <summary>Details</summary>
Motivation: To make advanced planning systems more accessible to supply chain planners by reducing reliance on costly consultants and enhancing usability.

Method: Developed a tool-augmented large language model system called SmartAPS that offers natural language query, reasoning, recommendations, and scenario analysis for operational planning.

Result: SmartAPS provides an intuitive interface for operations planning, demonstrated via a short video, showcasing its effectiveness in real-world scenarios.

Conclusion: SmartAPS enhances accessibility and usability of advanced planning systems through conversational AI, potentially broadening adoption among supply chain planners.

Abstract: Large language models (LLMs) present intriguing opportunities to enhance user
interaction with traditional algorithms and tools in real-world applications.
An advanced planning system (APS) is a sophisticated software that leverages
optimization to help operations planners create, interpret, and modify an
operational plan. While highly beneficial, many customers are priced out of
using an APS due to the ongoing costs of consultants responsible for
customization and maintenance. To address the need for a more accessible APS
expressed by supply chain planners, we present SmartAPS, a conversational
system built on a tool-augmented LLM. Our system provides operations planners
with an intuitive natural language chat interface, allowing them to query
information, perform counterfactual reasoning, receive recommendations, and
execute scenario analysis to better manage their operation. A short video
demonstrating the system has been released: https://youtu.be/KtIrJjlDbyw

</details>


### [54] [Synthesis of timeline-based planning strategies avoiding determinization](https://arxiv.org/abs/2507.17988)
*Dario Della Monica,Angelo Montanari,Pietro Sala*

Main category: cs.AI

TL;DR: This paper introduces a fragment of qualitative timeline-based planning with a plan-existence problem that can be directly solved using deterministic automata, enabling strategy synthesis.


<details>
  <summary>Details</summary>
Motivation: To address the computational challenges of synthesizing plans in qualitative timeline-based planning models, particularly overcoming the limitations of nondeterministic automata.

Method: The authors identify a fragment of the planning problem that maps onto deterministic finite automata and determine the maximal subset of Allen's relations that fit within this framework.

Result: They demonstrate that the plan-existence problem for this fragment can be solved directly with deterministic automata, facilitating strategy synthesis.

Conclusion: A new, more efficient class of timeline-based planning models is proposed, enhancing the feasibility of strategy synthesis in qualitative planning.

Abstract: Qualitative timeline-based planning models domains as sets of independent,
but
  interacting, components whose behaviors over time, the timelines, are
governed
  by sets of qualitative temporal constraints (ordering relations), called
  synchronization rules.
  Its plan-existence problem has been shown to be PSPACE-complete; in
  particular, PSPACE-membership has been proved via reduction to the
  nonemptiness problem for nondeterministic finite automata.
  However, nondeterministic automata cannot be directly used to synthesize
  planning strategies as a costly determinization step is needed.
  In this paper, we identify a fragment of qualitative timeline-based planning
  whose plan-existence problem can be directly mapped into the nonemptiness
  problem of deterministic finite automata, which can then
  synthesize strategies.
  In addition, we identify a maximal subset of Allen's relations that fits into
  such a deterministic fragment.

</details>


### [55] [E.A.R.T.H.: Structuring Creative Evolution through Model Error in Generative AI](https://arxiv.org/abs/2507.18004)
*Yusen Peng,Shuhua Mao*

Main category: cs.AI

TL;DR: The paper introduces the E.A.R.T.H. framework, a five-stage process that turns errors into creative outputs, significantly improving creativity, relevance, and style in AI-generated content through structured prompts and human-in-the-loop evaluation.


<details>
  <summary>Details</summary>
Motivation: To advance AI from merely imitative outputs to genuine creative outputs by leveraging errors as a source of creativity, inspired by cognitive science principles.

Method: A structured five-stage generative pipeline (Error generation, Amplification, Refine, Transform, Harness) built on multiple AI models and evaluation methods, incorporating feedback and semantic scoring to enhance creativity.

Result: Marked improvements in creativity scores, output quality, and relevance; shortened and more novel slogans; strong cross-modal alignment; positive human evaluations indicating stylistic and emotional resonance.

Conclusion: Error-driven, feedback-based generation enhances AI creativity and paves the way for scalable, human-aligned, self-evolving creative AI systems.

Abstract: How can AI move beyond imitation toward genuine creativity? This paper
proposes the E.A.R.T.H. framework, a five-stage generative pipeline that
transforms model-generated errors into creative assets through Error
generation, Amplification, Refine selection, Transform, and Harness feedback.
Drawing on cognitive science and generative modeling, we posit that "creative
potential hides in failure" and operationalize this via structured prompts,
semantic scoring, and human-in-the-loop evaluation. Implemented using
LLaMA-2-7B-Chat, SBERT, BERTScore, CLIP, BLIP-2, and Stable Diffusion, the
pipeline employs a composite reward function based on novelty, surprise, and
relevance. At the Refine stage, creativity scores increase by 52.5% (1.179 to
1.898, t = -5.56, p < 0.001), with final outputs reaching 2.010 - a 70.4%
improvement. Refined slogans are 48.4% shorter, 40.7% more novel, with only a
4.0% drop in relevance. Cross-modal tests show strong slogan-to-image alignment
(CLIPScore: 0.249; BERTScore F1: 0.816). In human evaluations, 60% of outputs
scored >= 4.0, with metaphorical slogans (avg. 4.09) outperforming literal ones
(3.99). Feedback highlights stylistic precision and emotional resonance. These
results demonstrate that error-centered, feedback-driven generation enhances
creativity, offering a scalable path toward self-evolving, human-aligned
creative AI.

</details>


### [56] [Does visualization help AI understand data?](https://arxiv.org/abs/2507.18022)
*Victoria R. Li,Johnathan Sun,Martin Wattenberg*

Main category: cs.AI

TL;DR: AI systems benefit from visualizations like charts, improving data analysis accuracy.


<details>
  <summary>Details</summary>
Motivation: To explore whether visual representations like charts can enhance AI data analysis capabilities.

Method: Experiments with GPT 4.1 and Claude 3.5 on analysis tasks using datasets with different visualizations.

Result: AI models perform better at analyzing datasets when charts accompany raw data, especially with increasing data complexity.

Conclusion: Visualizations can aid AI systems in understanding data, similar to their benefit to humans.

Abstract: Charts and graphs help people analyze data, but can they also be useful to AI
systems? To investigate this question, we perform a series of experiments with
two commercial vision-language models: GPT 4.1 and Claude 3.5. Across three
representative analysis tasks, the two systems describe synthetic datasets more
precisely and accurately when raw data is accompanied by a scatterplot,
especially as datasets grow in complexity. Comparison with two baselines --
providing a blank chart and a chart with mismatched data -- shows that the
improved performance is due to the content of the charts. Our results are
initial evidence that AI systems, like humans, can benefit from visualization.

</details>


### [57] [Multi-Agent Guided Policy Optimization](https://arxiv.org/abs/2507.18059)
*Yueheng Li,Guangming Xie,Zongqing Lu*

Main category: cs.AI

TL;DR: MAGPO is a new framework in multi-agent reinforcement learning that combines centralized training with decentralized execution, providing better coordination and theoretical guarantees.


<details>
  <summary>Details</summary>
Motivation: To address limitations in existing CTDE methods such as underutilization of centralized training and lack of theoretical guarantees.

Method: Integrating centralized guidance with decentralized execution through an auto-regressive joint policy and ensuring alignment with decentralized policies.

Result: MAGPO outperforms existing CTDE baselines and matches or surpasses fully centralized methods in various tasks and environments.

Conclusion: MAGPO offers a more effective and principled approach for decentralized multi-agent learning by leveraging centralized training with strong theoretical and empirical performance.

Abstract: Due to practical constraints such as partial observability and limited
communication, Centralized Training with Decentralized Execution (CTDE) has
become the dominant paradigm in cooperative Multi-Agent Reinforcement Learning
(MARL). However, existing CTDE methods often underutilize centralized training
or lack theoretical guarantees. We propose Multi-Agent Guided Policy
Optimization (MAGPO), a novel framework that better leverages centralized
training by integrating centralized guidance with decentralized execution.
MAGPO uses an auto-regressive joint policy for scalable, coordinated
exploration and explicitly aligns it with decentralized policies to ensure
deployability under partial observability. We provide theoretical guarantees of
monotonic policy improvement and empirically evaluate MAGPO on 43 tasks across
6 diverse environments. Results show that MAGPO consistently outperforms strong
CTDE baselines and matches or surpasses fully centralized approaches, offering
a principled and practical solution for decentralized multi-agent learning. Our
code and experimental data can be found in https://github.com/liyheng/MAGPO.

</details>


### [58] [AlphaGo Moment for Model Architecture Discovery](https://arxiv.org/abs/2507.18074)
*Yixiu Liu,Yang Nan,Weixian Xu,Xiangkun Hu,Lyumanshan Ye,Zhen Qin,Pengfei Liu*

Main category: cs.AI

TL;DR: AI system ASI-Arch autonomously discovers novel neural architectures, surpassing human designs and establishing a new scale for scientific discovery.


<details>
  <summary>Details</summary>
Motivation: The research addresses the bottleneck of human cognitive limits in AI development, aiming to automate not just optimization but innovation in neural architecture discovery.

Method: ASI-Arch conducts autonomous, end-to-end scientific research, hypothesizing and testing neural architectural concepts through extensive experiments.

Result: ASI-Arch conducted 1,773 experiments, discovering 106 state-of-the-art architectures with emergent principles, surpassing human-designed baselines.

Conclusion: The study demonstrates that AI-driven scientific discovery can be scaled computationally, providing a blueprint for self-accelerating AI research systems.

Abstract: While AI systems demonstrate exponentially improving capabilities, the pace
of AI research itself remains linearly bounded by human cognitive capacity,
creating an increasingly severe development bottleneck. We present ASI-Arch,
the first demonstration of Artificial Superintelligence for AI research
(ASI4AI) in the critical domain of neural architecture discovery--a fully
autonomous system that shatters this fundamental constraint by enabling AI to
conduct its own architectural innovation. Moving beyond traditional Neural
Architecture Search (NAS), which is fundamentally limited to exploring
human-defined spaces, we introduce a paradigm shift from automated optimization
to automated innovation. ASI-Arch can conduct end-to-end scientific research in
the domain of architecture discovery, autonomously hypothesizing novel
architectural concepts, implementing them as executable code, training and
empirically validating their performance through rigorous experimentation and
past experience. ASI-Arch conducted 1,773 autonomous experiments over 20,000
GPU hours, culminating in the discovery of 106 innovative, state-of-the-art
(SOTA) linear attention architectures. Like AlphaGo's Move 37 that revealed
unexpected strategic insights invisible to human players, our AI-discovered
architectures demonstrate emergent design principles that systematically
surpass human-designed baselines and illuminate previously unknown pathways for
architectural innovation. Crucially, we establish the first empirical scaling
law for scientific discovery itself--demonstrating that architectural
breakthroughs can be scaled computationally, transforming research progress
from a human-limited to a computation-scalable process. We provide
comprehensive analysis of the emergent design patterns and autonomous research
capabilities that enabled these breakthroughs, establishing a blueprint for
self-accelerating AI systems.

</details>


### [59] [Agentic AI framework for End-to-End Medical Data Inference](https://arxiv.org/abs/2507.18115)
*Soorya Ram Shimgekar,Shayan Vassef,Abhay Goyal,Navin Kumar,Koustuv Saha*

Main category: cs.AI

TL;DR: An Agentic AI framework automates the entire clinical data pipeline, handling data ingestion, anonymization, feature extraction, model selection, preprocessing, and inference, reducing manual effort and increasing scalability in healthcare ML applications.


<details>
  <summary>Details</summary>
Motivation: To address the high costs and labor intensity of building and deploying ML solutions in healthcare due to fragmented workflows and privacy constraints.

Method: Development of a modular multi-agent system that automates data handling, feature extraction, model selection, preprocessing, and inference for clinical data.

Result: The system successfully processes both structured and unstructured healthcare data, automates ML pipeline components, and produces interpretable results, demonstrating its effectiveness on various datasets.

Conclusion: Automating the ML pipeline with agent-based modules can significantly lower barriers to deploying AI in clinical settings, making it more scalable and cost-effective.

Abstract: Building and deploying machine learning solutions in healthcare remains
expensive and labor-intensive due to fragmented preprocessing workflows, model
compatibility issues, and stringent data privacy constraints. In this work, we
introduce an Agentic AI framework that automates the entire clinical data
pipeline, from ingestion to inference, through a system of modular,
task-specific agents. These agents handle both structured and unstructured
data, enabling automatic feature selection, model selection, and preprocessing
recommendation without manual intervention. We evaluate the system on publicly
available datasets from geriatrics, palliative care, and colonoscopy imaging.
For example, in the case of structured data (anxiety data) and unstructured
data (colonoscopy polyps data), the pipeline begins with file-type detection by
the Ingestion Identifier Agent, followed by the Data Anonymizer Agent ensuring
privacy compliance, where we first identify the data type and then anonymize
it. The Feature Extraction Agent identifies features using an embedding-based
approach for tabular data, extracting all column names, and a multi-stage
MedGemma-based approach for image data, which infers modality and disease name.
These features guide the Model-Data Feature Matcher Agent in selecting the
best-fit model from a curated repository. The Preprocessing Recommender Agent
and Preprocessing Implementor Agent then apply tailored preprocessing based on
data type and model requirements. Finally, the ``Model Inference Agent" runs
the selected model on the uploaded data and generates interpretable outputs
using tools like SHAP, LIME, and DETR attention maps. By automating these
high-friction stages of the ML lifecycle, the proposed framework reduces the
need for repeated expert intervention, offering a scalable, cost-efficient
pathway for operationalizing AI in clinical environments.

</details>


### [60] [Actively evaluating and learning the distinctions that matter: Vaccine safety signal detection from emergency triage notes](https://arxiv.org/abs/2507.18123)
*Sedigh Khademi,Christopher Palmer,Muhammad Javed,Hazel Clothier,Jim Buttery,Gerardo Luis Dimaguila,Jim Black*

Main category: cs.AI

TL;DR: This paper presents a method using NLP and Active Learning to develop a classifier for detecting potential vaccine safety issues from emergency department notes, enhancing post-vaccination surveillance.


<details>
  <summary>Details</summary>
Motivation: The need for rapid, accurate post-licensure vaccine safety monitoring due to limited clinical trial data and early widespread vaccine rollout.

Method: Combining NLP, Active Learning, and data augmentation to build an effective classifier that analyzes ED triage notes for vaccine safety signals.

Result: The integrated approach enables efficient development of a classifier that improves vaccine safety signal detection from emergency department data.

Conclusion: The study demonstrates that combining NLP, Active Learning, and data augmentation can enhance vaccine safety surveillance by providing timely and accurate detection of safety issues.

Abstract: The rapid development of COVID-19 vaccines has showcased the global
communitys ability to combat infectious diseases. However, the need for
post-licensure surveillance systems has grown due to the limited window for
safety data collection in clinical trials and early widespread implementation.
This study aims to employ Natural Language Processing techniques and Active
Learning to rapidly develop a classifier that detects potential vaccine safety
issues from emergency department notes. ED triage notes, containing expert,
succinct vital patient information at the point of entry to health systems, can
significantly contribute to timely vaccine safety signal surveillance. While
keyword-based classification can be effective, it may yield false positives and
demand extensive keyword modifications. This is exacerbated by the infrequency
of vaccination-related ED presentations and their similarity to other reasons
for ED visits. NLP offers a more accurate and efficient alternative, albeit
requiring annotated data, which is often scarce in the medical field. Active
learning optimizes the annotation process and the quality of annotated data,
which can result in faster model implementation and improved model performance.
This work combines active learning, data augmentation, and active learning and
evaluation techniques to create a classifier that is used to enhance vaccine
safety surveillance from ED triage notes.

</details>


### [61] [Logical Characterizations of GNNs with Mean Aggregation](https://arxiv.org/abs/2507.18145)
*Moritz Schönherr,Carsten Lutz*

Main category: cs.AI

TL;DR: The paper compares the expressive power of mean-based GNNs to various modal logics, showing that their expressiveness varies with the setting and aggregation methods.


<details>
  <summary>Details</summary>
Motivation: To understand how different aggregation functions in GNNs affect their expressive power in relation to logical formalisms.

Method: Theoretical analysis linking GNNs with modal logics and MSO under various settings.

Result: Mean GNNs are as expressive as ratio modal logic in non-uniform settings and less expressive than sum and max GNNs; in the uniform setting, their expressive power is characterized by alternation-free modal logic, with variations when assumptions are dropped.

Conclusion: Aggregation functions and setting assumptions significantly influence the expressive power of GNNs, with mean GNNs having limited expressiveness compared to sum and max GNNs.

Abstract: We study the expressive power of graph neural networks (GNNs) with mean as
the aggregation function. In the non-uniform setting, we show that such GNNs
have exactly the same expressive power as ratio modal logic, which has modal
operators expressing that at least a certain ratio of the successors of a
vertex satisfies a specified property. The non-uniform expressive power of mean
GNNs is thus higher than that of GNNs with max aggregation, but lower than for
sum aggregation--the latter are characterized by modal logic and graded modal
logic, respectively. In the uniform setting, we show that the expressive power
relative to MSO is exactly that of alternation-free modal logic, under the
natural assumptions that combination functions are continuous and
classification functions are thresholds. This implies that, relative to MSO and
in the uniform setting, mean GNNs are strictly less expressive than sum GNNs
and max GNNs. When any of the assumptions is dropped, the expressive power
increases.

</details>


### [62] [Decoupling Knowledge and Reasoning in LLMs: An Exploration Using Cognitive Dual-System Theory](https://arxiv.org/abs/2507.18178)
*Mutian Yang,Jiandong Gao,Ji Wu*

Main category: cs.AI

TL;DR: This paper proposes a cognition attribution framework inspired by dual-system theory to differentiate and analyze the contributions of knowledge and reasoning in large language models, revealing insights on domain specificity, parameter effects, and layer localization.


<details>
  <summary>Details</summary>
Motivation: To improve understanding and interpretability of LLMs by distinguishing the roles of knowledge and reasoning during inference.

Method: Decomposing LLM cognition into knowledge retrieval and reasoning adjustment phases, prompting models in different cognitive modes, and analyzing performance across multiple models and datasets.

Result: Reasoning is domain-specific; parameter scaling enhances both knowledge and reasoning (more so knowledge); knowledge is stored in lower layers, reasoning in higher layers.

Conclusion: Decoupling knowledge and reasoning offers new insights, improves interpretability, and aids in understanding LLM behaviors and limitations.

Abstract: While large language models (LLMs) leverage both knowledge and reasoning
during inference, the capacity to distinguish between them plays a pivotal role
in model analysis, interpretability, and development. Inspired by dual-system
cognitive theory, we propose a cognition attribution framework to decouple the
contribution of knowledge and reasoning. In particular, the cognition of LLMs
is decomposed into two distinct yet complementary phases: knowledge retrieval
(Phase 1) and reasoning adjustment (Phase 2). To separate these phases, LLMs
are prompted to generate answers under two different cognitive modes, fast
thinking and slow thinking, respectively. The performance under different
cognitive modes is analyzed to quantify the contribution of knowledge and
reasoning. This architecture is employed to 15 LLMs across 3 datasets. Results
reveal: (1) reasoning adjustment is domain-specific, benefiting
reasoning-intensive domains (e.g., mathematics, physics, and chemistry) and
potentially imparing knowledge-intensive domains. (2) Parameter scaling
improves both knowledge and reasoning, with knowledge improvements being more
pronounced. Additionally, parameter scaling make LLMs reasoning significantly
more prudent, while moderately more intelligent. (3) Knowledge primarily
resides in lower network layers, while reasoning operates in higher layers. Our
framework not only helps understand LLMs from a "decoupling" perspective, but
also provides new insights into existing research, including scaling laws,
hierarchical knowledge editing, and limitations of small-model reasoning.

</details>


### [63] [Comparing Non-minimal Semantics for Disjunction in Answer Set Programming](https://arxiv.org/abs/2507.18198)
*Felicidad Aguado,Pedro Cabalar,Brais Muñiz,Gilberto Pérez,Concepción Vidal*

Main category: cs.AI

TL;DR: The paper compares four non-minimal answer set semantics for disjunction in ASP, showing three approaches coincide and differ from the fourth, which aligns with classical logic.


<details>
  <summary>Details</summary>
Motivation: To understand how alternative semantics for disjunction in answer set programming (ASP) compare, especially those not adhering to minimality.

Method: The authors analyze and compare four different semantics for disjunction in ASP, including proving their equivalences and differences.

Result: Three semantics (Forks, Justified Models, and a relaxed DI) coincide, forming a unified approach that is a superset of stable models and stronger than the fourth approach, which aligns with classical logic.

Conclusion: The study reveals a common underlying semantics for three approaches and clarifies their relation to stable models and classical logic.

Abstract: In this paper, we compare four different semantics for disjunction in Answer
Set Programming that, unlike stable models, do not adhere to the principle of
model minimality. Two of these approaches, Cabalar and Mu\~niz' \emph{Justified
Models} and Doherty and Szalas' \emph{Strongly Supported Models}, directly
provide an alternative non-minimal semantics for disjunction. The other two,
Aguado et al's \emph{Forks} and Shen and Eiter's \emph{Determining Inference}
(DI) semantics, actually introduce a new disjunction connective, but are
compared here as if they constituted new semantics for the standard disjunction
operator. We are able to prove that three of these approaches (Forks, Justified
Models and a reasonable relaxation of the DI semantics) actually coincide,
constituting a common single approach under different definitions. Moreover,
this common semantics always provides a superset of the stable models of a
program (in fact, modulo any context) and is strictly stronger than the fourth
approach (Strongly Supported Models), that actually treats disjunctions as in
classical logic.

</details>


### [64] [Foundations for Risk Assessment of AI in Protecting Fundamental Rights](https://arxiv.org/abs/2507.18290)
*Antonino Rotolo,Beatrice Ferrigno,Jose Miguel Angel Garcia Godinez,Claudio Novelli,Giovanni Sartor*

Main category: cs.AI

TL;DR: A conceptual framework for qualitative AI risk assessment focusing on legal compliance and rights protection, using balancing and defeasible reasoning within the EU AI Act context.


<details>
  <summary>Details</summary>
Motivation: Addressing the complexities of legal compliance and fundamental rights in AI risk assessment.

Method: Integrating definitional balancing and defeasible reasoning into a layered philosophical and logical framework.

Result: Development of a layered approach facilitating assessment of high-risk and general-purpose AI systems, with future plans for formal models and algorithms.

Conclusion: The framework enhances AI governance by linking theoretical insights with practical assessment models for responsible AI deployment.

Abstract: This chapter introduces a conceptual framework for qualitative risk
assessment of AI, particularly in the context of the EU AI Act. The framework
addresses the complexities of legal compliance and fundamental rights
protection by itegrating definitional balancing and defeasible reasoning.
Definitional balancing employs proportionality analysis to resolve conflicts
between competing rights, while defeasible reasoning accommodates the dynamic
nature of legal decision-making. Our approach stresses the need for an analysis
of AI deployment scenarios and for identifying potential legal violations and
multi-layered impacts on fundamental rights. On the basis of this analysis, we
provide philosophical foundations for a logical account of AI risk analysis. In
particular, we consider the basic building blocks for conceptually grasping the
interaction between AI deployment scenarios and fundamental rights,
incorporating in defeasible reasoning definitional balancing and arguments
about the contextual promotion or demotion of rights. This layered approach
allows for more operative models of assessment of both high-risk AI systems and
General Purpose AI (GPAI) systems, emphasizing the broader applicability of the
latter. Future work aims to develop a formal model and effective algorithms to
enhance AI risk assessment, bridging theoretical insights with practical
applications to support responsible AI governance.

</details>


### [65] [The AlphaPhysics Term Rewriting System for Marking Algebraic Expressions in Physics Exams](https://arxiv.org/abs/2507.18337)
*Peter Baumgartner,Lachlan McGinness*

Main category: cs.AI

TL;DR: This paper presents an automated method to grade physics exams by combining AI interpretation, computer algebra, SMT solving, and term rewriting to analyze student answers and assess correctness.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient, automated grading system for physics exams that accurately evaluates complex student responses.

Method: Using a large language model for response interpretation, then applying formal reasoning techniques including SMT solving and specialized term rewriting systems.

Result: The system was successfully evaluated on over 1500 real student responses, demonstrating its capability to accurately assess physics solutions.

Conclusion: The integrated approach effectively automates physics exam grading, leveraging advanced AI and formal methods.

Abstract: We present our method for automatically marking Physics exams. The marking
problem consists in assessing typed student answers for correctness with
respect to a ground truth solution. This is a challenging problem that we seek
to tackle using a combination of a computer algebra system, an SMT solver and a
term rewriting system. A Large Language Model is used to interpret and remove
errors from student responses and rewrite these in a machine readable format.
Once formalized and language-aligned, the next step then consists in applying
automated reasoning techniques for assessing student solution correctness. We
consider two methods of automated theorem proving: off-the-shelf SMT solving
and term rewriting systems tailored for physics problems involving
trigonometric expressions. The development of the term rewrite system and
establishing termination and confluence properties was not trivial, and we
describe it in some detail in the paper. We evaluate our system on a rich pool
of over 1500 real-world student exam responses from the 2023 Australian Physics
Olympiad.

</details>


### [66] [Reasoning Beyond the Obvious: Evaluating Divergent and Convergent Thinking in LLMs for Financial Scenarios](https://arxiv.org/abs/2507.18368)
*Zhuang Qiang Bok,Watson Wei Khong Chua*

Main category: cs.AI

TL;DR: The paper introduces ConDiFi, a benchmark for evaluating divergent and convergent reasoning in LLMs within finance, highlighting model performance variations and the importance of reasoning in financial decision-making.


<details>
  <summary>Details</summary>
Motivation: To address the need for assessing reasoning skills in LLMs applied to finance, beyond factual accuracy.

Method: Development of ConDiFi benchmark with macro-financial prompts and multi-hop MCQs; evaluation of 14 models.

Result: Models show varied performance; GPT-4o struggles with novelty and actionability, while DeepSeek-R1 and Cohere Command R+ excel in generating actionable insights.

Conclusion: ConDiFi offers a new perspective for evaluating reasoning in LLMs, crucial for safe and strategic financial deployments.

Abstract: Most reasoning benchmarks for LLMs emphasize factual accuracy or step-by-step
logic. In finance, however, professionals must not only converge on optimal
decisions but also generate creative, plausible futures under uncertainty. We
introduce ConDiFi, a benchmark that jointly evaluates divergent and convergent
thinking in LLMs for financial tasks.
  ConDiFi features 607 macro-financial prompts for divergent reasoning and 990
multi-hop adversarial MCQs for convergent reasoning. Using this benchmark, we
evaluated 14 leading models and uncovered striking differences. Despite high
fluency, GPT-4o underperforms on Novelty and Actionability. In contrast, models
like DeepSeek-R1 and Cohere Command R+ rank among the top for generating
actionable, insights suitable for investment decisions. ConDiFi provides a new
perspective to assess reasoning capabilities essential to safe and strategic
deployment of LLMs in finance.

</details>


### [67] [Revisiting LLM Reasoning via Information Bottleneck](https://arxiv.org/abs/2507.18391)
*Shiye Lei,Zhihao Cheng,Kai Jia,Dacheng Tao*

Main category: cs.AI

TL;DR: The paper introduces IBRO, a reasoning optimization framework for LLMs based on the information bottleneck principle, leading to improved reasoning performance.


<details>
  <summary>Details</summary>
Motivation: To develop a principled, theoretically grounded method for optimizing LLM reasoning trajectories rather than relying on heuristic approaches.

Method: The authors propose IBRO, an IB-inspired regularization framework that is simple to implement and computationally efficient, incorporating a token-level surrogate objective.

Result: IB regularization consistently improves LLM reasoning performance across multiple benchmarks and RL algorithms.

Conclusion: IBRO provides a theoretically justified, practical enhancement for LLM reasoning capabilities without added computational complexity.

Abstract: Large language models (LLMs) have recently demonstrated remarkable progress
in reasoning capabilities through reinforcement learning with verifiable
rewards (RLVR). By leveraging simple rule-based rewards, RL effectively
incentivizes LLMs to produce extended chain-of-thought (CoT) reasoning
trajectories, progressively guiding them toward correct answers. However,
existing approaches remain largely heuristic and intuition-driven, limiting the
development of principled methodologies. In this paper, we present a
theoretical characterization of LLM reasoning grounded in information
bottleneck (IB) principle, introducing IB-aware reasoning optimization (IBRO),
a framework that encourages reasoning trajectories to be both informative about
the final correct answer and generalizable across diverse prompts. We derive a
practical token-level surrogate objective and propose an efficient
approximation, resulting in the lightweight IB regularization method. This
technique integrates seamlessly into existing RL-based post-training frameworks
without additional computational overhead, requiring only a one-line code
modification. Empirically, we validate IB regularization across multiple
mathematical reasoning benchmarks and RL algorithms, demonstrating consistent
improvements in LLM reasoning performance.

</details>


### [68] [Optimising Call Centre Operations using Reinforcement Learning: Value Iteration versus Proximal Policy Optimisation](https://arxiv.org/abs/2507.18398)
*Kwong Ho Li,Wathsala Karunarathne*

Main category: cs.AI

TL;DR: The paper compares model-based and model-free Reinforcement Learning methods for call centre call routing, finding PPO the most effective after extensive testing.


<details>
  <summary>Details</summary>
Motivation: To improve call centre efficiency by minimizing client waiting and staff idle times through advanced RL techniques.

Method: Comparing a model-based approach (Value Iteration) with a model-free approach (Proximal Policy Optimization) using simulation environments under an MDP framework.

Result: PPO outperforms VI in reward, client waiting time, and staff idle time after 1,000 episodes, despite longer training.

Conclusion: Reinforcement Learning, particularly PPO, effectively optimizes call routing, offering superior performance over traditional model-based methods.

Abstract: This paper investigates the application of Reinforcement Learning (RL) to
optimise call routing in call centres to minimise client waiting time and staff
idle time. Two methods are compared: a model-based approach using Value
Iteration (VI) under known system dynamics, and a model-free approach using
Proximal Policy Optimisation (PPO) that learns from experience. For the
model-based approach, a theoretical model is used, while a simulation model
combining Discrete Event Simulation (DES) with the OpenAI Gym environment is
developed for model-free learning. Both models frame the problem as a Markov
Decision Process (MDP) within a Skills-Based Routing (SBR) framework, with
Poisson client arrivals and exponentially distributed service and abandonment
times. For policy evaluation, random, VI, and PPO policies are evaluated using
the simulation model. After 1,000 test episodes, PPO consistently achives the
highest rewards, along with the lowest client waiting time and staff idle time,
despite requiring longer training time.

</details>


### [69] [GPU Accelerated Compact-Table Propagation](https://arxiv.org/abs/2507.18413)
*Enrico Santi,Fabio Tardivo,Agostino Dovier,Andrea Formisano*

Main category: cs.AI

TL;DR: The paper discusses enhancing the Compact-Table algorithm for table constraints in constraint programming by leveraging GPU power to handle large-scale constraints more efficiently.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of CPU-based approaches in managing large sets of table constraints in real-world problems.

Method: Development and integration of a GPU-accelerated version of the CT algorithm, along with experimental validation.

Result: Successful implementation of GPU-accelerated CT that improves handling large table constraints, validated through experiments.

Conclusion: Leveraging GPU computing significantly enhances the efficiency and scalability of handling extensive table constraints in constraint programming.

Abstract: Constraint Programming developed within Logic Programming in the Eighties;
nowadays all Prolog systems encompass modules capable of handling constraint
programming on finite domains demanding their solution to a constraint solver.
This work focuses on a specific form of constraint, the so-called table
constraint, used to specify conditions on the values of variables as an
enumeration of alternative options. Since every condition on a set of finite
domain variables can be ultimately expressed as a finite set of cases, Table
can, in principle, simulate any other constraint. These characteristics make
Table one of the most studied constraints ever, leading to a series of
increasingly efficient propagation algorithms. Despite this, it is not uncommon
to encounter real-world problems with hundreds or thousands of valid cases that
are simply too many to be handled effectively with standard CPU-based
approaches. In this paper, we deal with the Compact-Table (CT) algorithm, the
state-of-the-art propagation algorithms for Table. We describe how CT can be
enhanced by exploiting the massive computational power offered by modern GPUs
to handle large Table constraints. In particular, we report on the design and
implementation of GPU-accelerated CT, on its integration into an existing
constraint solver, and on an experimental validation performed on a significant
set of instances.

</details>


### [70] [On the Performance of Concept Probing: The Influence of the Data (Extended Version)](https://arxiv.org/abs/2507.18550)
*Manuel de Sousa Ribeiro,Afonso Leote,João Leite*

Main category: cs.AI

TL;DR: The paper investigates how the training data for probing models affects their performance in interpreting neural networks, focusing on image classification.


<details>
  <summary>Details</summary>
Motivation: To understand the impact of training data on concept probing performance, addressing a gap in current research.

Method: Empirical analysis of the data used for training probing models in image classification, with the provision of concept labels for two datasets.

Result: The study sheds light on the importance of data selection in training effective probing models.

Conclusion: Choosing appropriate training data is crucial for effective concept probing in neural networks.

Abstract: Concept probing has recently garnered increasing interest as a way to help
interpret artificial neural networks, dealing both with their typically large
size and their subsymbolic nature, which ultimately renders them unfeasible for
direct human interpretation. Concept probing works by training additional
classifiers to map the internal representations of a model into human-defined
concepts of interest, thus allowing humans to peek inside artificial neural
networks. Research on concept probing has mainly focused on the model being
probed or the probing model itself, paying limited attention to the data
required to train such probing models. In this paper, we address this gap.
Focusing on concept probing in the context of image classification tasks, we
investigate the effect of the data used to train probing models on their
performance. We also make available concept labels for two widely used
datasets.

</details>


### [71] [SafeWork-R1: Coevolving Safety and Intelligence under the AI-45$^{\circ}$ Law](https://arxiv.org/abs/2507.18576)
*Shanghai AI Lab,:,Yicheng Bao,Guanxu Chen,Mingkang Chen,Yunhao Chen,Chiyu Chen,Lingjie Chen,Sirui Chen,Xinquan Chen,Jie Cheng,Yu Cheng,Dengke Deng,Yizhuo Ding,Dan Ding,Xiaoshan Ding,Yi Ding,Zhichen Dong,Lingxiao Du,Yuyu Fan,Xinshun Feng,Yanwei Fu,Yuxuan Gao,Ruijun Ge,Tianle Gu,Lujun Gui,Jiaxuan Guo,Qianxi He,Yuenan Hou,Xuhao Hu,Hong Huang,Kaichen Huang,Shiyang Huang,Yuxian Jiang,Shanzhe Lei,Jie Li,Lijun Li,Hao Li,Juncheng Li,Xiangtian Li,Yafu Li,Lingyu Li,Xueyan Li,Haotian Liang,Dongrui Liu,Qihua Liu,Zhixuan Liu,Bangwei Liu,Huacan Liu,Yuexiao Liu,Zongkai Liu,Chaochao Lu,Yudong Lu,Xiaoya Lu,Zhenghao Lu,Qitan Lv,Caoyuan Ma,Jiachen Ma,Xiaoya Ma,Zhongtian Ma,Lingyu Meng,Ziqi Miao,Yazhe Niu,Yuezhang Peng,Yuan Pu,Han Qi,Chen Qian,Xingge Qiao,Jingjing Qu,Jiashu Qu,Wanying Qu,Wenwen Qu,Xiaoye Qu,Qihan Ren,Qingnan Ren,Qingyu Ren,Jing Shao,Wenqi Shao,Shuai Shao,Dongxing Shi,Xin Song,Xinhao Song,Yan Teng,Xuan Tong,Yingchun Wang,Xuhong Wang,Shujie Wang,Xin Wang,Yige Wang,Yixu Wang,Yuanfu Wang,Futing Wang,Ruofan Wang,Wenjie Wang,Yajie Wang,Muhao Wei,Xiaoyu Wen,Fenghua Weng,Yuqi Wu,Yingtong Xiong,Xingcheng Xu,Chao Yang,Yue Yang,Yang Yao,Yulei Ye,Zhenyun Yin,Yi Yu,Bo Zhang,Qiaosheng Zhang,Jinxuan Zhang,Yexin Zhang,Yinqiang Zheng,Hefeng Zhou,Zhanhui Zhou,Pengyu Zhu,Qingzi Zhu,Yubo Zhu,Bowen Zhou*

Main category: cs.AI

TL;DR: SafeWork-R1 is a multimodal reasoning model developed via the SafeLadder framework that enhances safety capabilities alongside performance, using reinforcement learning and verification techniques.


<details>
  <summary>Details</summary>
Motivation: To develop an AI model that coevolves safety and capability for robust, trustworthy performance.

Method: SafeLadder framework combines large-scale reinforcement learning, multi-principled verifiers, and inference-time interventions to improve safety reasoning without sacrificing general capabilities.

Result: SafeWork-R1 outperforms baseline models in safety benchmarks by 46.54%, surpasses proprietary models like GPT-4.1 in safety, and maintains strong general performance.

Conclusion: The framework demonstrates that safety and capability can co-evolve synergistically, leading to more reliable and trustworthy AI.

Abstract: We introduce SafeWork-R1, a cutting-edge multimodal reasoning model that
demonstrates the coevolution of capabilities and safety. It is developed by our
proposed SafeLadder framework, which incorporates large-scale, progressive,
safety-oriented reinforcement learning post-training, supported by a suite of
multi-principled verifiers. Unlike previous alignment methods such as RLHF that
simply learn human preferences, SafeLadder enables SafeWork-R1 to develop
intrinsic safety reasoning and self-reflection abilities, giving rise to safety
`aha' moments. Notably, SafeWork-R1 achieves an average improvement of
$46.54\%$ over its base model Qwen2.5-VL-72B on safety-related benchmarks
without compromising general capabilities, and delivers state-of-the-art safety
performance compared to leading proprietary models such as GPT-4.1 and Claude
Opus 4. To further bolster its reliability, we implement two distinct
inference-time intervention methods and a deliberative search mechanism,
enforcing step-level verification. Finally, we further develop
SafeWork-R1-InternVL3-78B, SafeWork-R1-DeepSeek-70B, and
SafeWork-R1-Qwen2.5VL-7B. All resulting models demonstrate that safety and
capability can co-evolve synergistically, highlighting the generalizability of
our framework in building robust, reliable, and trustworthy general-purpose AI.

</details>
