<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 81]
- [cs.DB](#cs.DB) [Total: 5]
- [cs.AI](#cs.AI) [Total: 51]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Advancing Mental Disorder Detection: A Comparative Evaluation of Transformer and LSTM Architectures on Social Media](https://arxiv.org/abs/2507.19511)
*Khalid Hasan,Jamil Saquer,Mukulika Ghosh*

Main category: cs.CL

TL;DR: Transformer-based NLP models, especially RoBERTa, excel in classifying mental health disorders from Reddit data, outperforming traditional LSTM approaches.


<details>
  <summary>Details</summary>
Motivation: The need for automated, scalable tools for early detection and monitoring of mental health disorders.

Method: Evaluation of transformer models (BERT, RoBERTa, etc.) and LSTM with various embeddings on a large, annotated Reddit dataset.

Result: Transformer models, particularly RoBERTa, achieved up to 99.54% F1 score, with LSTM + BERT embeddings also highly competitive.

Conclusion: Transformer models are effective for real-time mental health monitoring, offering valuable insights for clinical and digital interventions.

Abstract: The rising prevalence of mental health disorders necessitates the development
of robust, automated tools for early detection and monitoring. Recent advances
in Natural Language Processing (NLP), particularly transformer-based
architectures, have demonstrated significant potential in text analysis. This
study provides a comprehensive evaluation of state-of-the-art transformer
models (BERT, RoBERTa, DistilBERT, ALBERT, and ELECTRA) against Long Short-Term
Memory (LSTM) based approaches using different text embedding techniques for
mental health disorder classification on Reddit. We construct a large annotated
dataset, validating its reliability through statistical judgmental analysis and
topic modeling. Experimental results demonstrate the superior performance of
transformer models over traditional deep-learning approaches. RoBERTa achieved
the highest classification performance, with a 99.54% F1 score on the hold-out
test set and a 96.05% F1 score on the external test set. Notably, LSTM models
augmented with BERT embeddings proved highly competitive, achieving F1 scores
exceeding 94% on the external dataset while requiring significantly fewer
computational resources. These findings highlight the effectiveness of
transformer-based models for real-time, scalable mental health monitoring. We
discuss the implications for clinical applications and digital mental health
interventions, offering insights into the capabilities and limitations of
state-of-the-art NLP methodologies in mental disorder detection.

</details>


### [2] [Setting The Table with Intent: Intent-aware Schema Generation and Editing for Literature Review Tables](https://arxiv.org/abs/2507.19521)
*Vishakh Padmakumar,Joseph Chee Chang,Kyle Lo,Doug Downey,Aakanksha Naik*

Main category: cs.CL

TL;DR: The paper enhances schema generation for organizing academic papers using large language models by reducing ambiguity through synthesized intents and developing editing techniques for improvement.


<details>
  <summary>Details</summary>
Motivation: To improve the organization and comparison of scientific literature via better schema generation methods using LLMs.

Method: The authors augment unannotated corpora with synthesized intents, create datasets conditioned on information needs, benchmark different LLM-based schema generation methods, and develop editing techniques for schemas.

Result: Incorporating table intents improves schema reconstruction, smaller models become competitive, and their editing techniques effectively refine schemas.

Conclusion: The proposed methods advance schema generation and editing, facilitating better organization and comparison of academic literature.

Abstract: The increasing volume of academic literature makes it essential for
researchers to organize, compare, and contrast collections of documents. Large
language models (LLMs) can support this process by generating schemas defining
shared aspects along which to compare papers. However, progress on schema
generation has been slow due to: (i) ambiguity in reference-based evaluations,
and (ii) lack of editing/refinement methods. Our work is the first to address
both issues. First, we present an approach for augmenting unannotated table
corpora with synthesized intents and apply it to create a dataset for studying
schema generation conditioned on a given information need, thus reducing
ambiguity. With this dataset, we show how incorporating table intents
significantly improves baseline performance in reconstructing reference
schemas. Next, we propose several LLM-based schema editing techniques. We start
by comprehensively benchmarking several single-shot schema generation methods,
including prompted LLM workflows and fine-tuned models, showing that smaller,
open-weight models can be fine-tuned to be competitive with state-of-the-art
prompted LLMs. Then we demonstrate that our editing techniques can further
improve schemas generated by these methods.

</details>


### [3] [Mind the Language Gap in Digital Humanities: LLM-Aided Translation of SKOS Thesauri](https://arxiv.org/abs/2507.19537)
*Felix Kraus,Nicolas Blumenr√∂hr,Danah Tonne,Achim Streit*

Main category: cs.CL

TL;DR: WOKIE is an open-source pipeline for automated translation of SKOS thesauri, improving accessibility and interoperability in Digital Humanities.


<details>
  <summary>Details</summary>
Motivation: To address language diversity challenges in Digital Humanities resources, enhancing access, reuse, and semantic interoperability.

Method: Combining external translation services with Large Language Models (LLMs) for targeted refinement, evaluated across various languages and parameters.

Result: WOKIE effectively improves translation quality, performance, and ontology matching, facilitating multilingual access and reuse.

Conclusion: WOKIE supports more inclusive, multilingual research infrastructures by enabling accessible, automated translation of knowledge resources.

Abstract: We introduce WOKIE, an open-source, modular, and ready-to-use pipeline for
the automated translation of SKOS thesauri. This work addresses a critical need
in the Digital Humanities (DH), where language diversity can limit access,
reuse, and semantic interoperability of knowledge resources. WOKIE combines
external translation services with targeted refinement using Large Language
Models (LLMs), balancing translation quality, scalability, and cost. Designed
to run on everyday hardware and be easily extended, the application requires no
prior expertise in machine translation or LLMs. We evaluate WOKIE across
several DH thesauri in 15 languages with different parameters, translation
services and LLMs, systematically analysing translation quality, performance,
and ontology matching improvements. Our results show that WOKIE is suitable to
enhance the accessibility, reuse, and cross-lingual interoperability of
thesauri by hurdle-free automated translation and improved ontology matching
performance, supporting more inclusive and multilingual research
infrastructures.

</details>


### [4] [Mitigating Geospatial Knowledge Hallucination in Large Language Models: Benchmarking and Dynamic Factuality Aligning](https://arxiv.org/abs/2507.19586)
*Shengyuan Wang,Jie Feng,Tianhui Liu,Dan Pei,Yong Li*

Main category: cs.CL

TL;DR: The paper presents a framework for evaluating and mitigating geospatial hallucinations in large language models, improving their accuracy and trustworthiness.


<details>
  <summary>Details</summary>
Motivation: To address the widespread issue of inaccurate geospatial knowledge in LLMs, which impacts their reliability.

Method: Developing a structured evaluation framework and a dynamic factuality aligning method based on Kahneman-Tversky Optimization.

Result: Significant improvement (over 29.6%) in performance on the proposed benchmark, validating the effectiveness of the evaluation and mitigation techniques.

Conclusion: The proposed framework and method effectively enhance the trustworthiness of LLMs in geospatial tasks.

Abstract: Large language models (LLMs) possess extensive world knowledge, including
geospatial knowledge, which has been successfully applied to various geospatial
tasks such as mobility prediction and social indicator prediction. However,
LLMs often generate inaccurate geospatial knowledge, leading to geospatial
hallucinations (incorrect or inconsistent representations of geospatial
information) that compromise their reliability. While the phenomenon of general
knowledge hallucination in LLMs has been widely studied, the systematic
evaluation and mitigation of geospatial hallucinations remain largely
unexplored. To address this gap, we propose a comprehensive evaluation
framework for geospatial hallucinations, leveraging structured geospatial
knowledge graphs for controlled assessment. Through extensive evaluation across
20 advanced LLMs, we uncover the hallucinations in their geospatial knowledge.
Building on these insights, we introduce a dynamic factuality aligning method
based on Kahneman-Tversky Optimization (KTO) to mitigate geospatial
hallucinations in LLMs, leading to a performance improvement of over 29.6% on
the proposed benchmark. Extensive experimental results demonstrate the
effectiveness of our benchmark and learning algorithm in enhancing the
trustworthiness of LLMs in geospatial knowledge and reasoning tasks.

</details>


### [5] [Efficient Attention Mechanisms for Large Language Models: A Survey](https://arxiv.org/abs/2507.19595)
*Yutao Sun,Zhenyu Li,Yike Zhang,Tengyu Pan,Bowen Dong,Yuyi Guo,Jianyong Wang*

Main category: cs.CL

TL;DR: Survey of efficient attention mechanisms in transformer models, focusing on linear and sparse methods that enhance scalability and efficiency.


<details>
  <summary>Details</summary>
Motivation: To address the quadratic complexity of self-attention in large language models, which limits long-context modeling.

Method: Systematic review of algorithmic developments and hardware considerations, with analysis of integration in large-scale pre-trained models.

Result: Comprehensive categorization and understanding of efficient attention techniques, including hybrid models.

Conclusion: Provides a foundational reference for designing scalable and efficient language models by aligning theoretical and practical strategies.

Abstract: Transformer-based architectures have become the prevailing backbone of large
language models. However, the quadratic time and memory complexity of
self-attention remains a fundamental obstacle to efficient long-context
modeling. To address this limitation, recent research has introduced two
principal categories of efficient attention mechanisms. Linear attention
methods achieve linear complexity through kernel approximations, recurrent
formulations, or fastweight dynamics, thereby enabling scalable inference with
reduced computational overhead. Sparse attention techniques, in contrast, limit
attention computation to selected subsets of tokens based on fixed patterns,
block-wise routing, or clustering strategies, enhancing efficiency while
preserving contextual coverage. This survey provides a systematic and
comprehensive overview of these developments, integrating both algorithmic
innovations and hardware-level considerations. In addition, we analyze the
incorporation of efficient attention into largescale pre-trained language
models, including both architectures built entirely on efficient attention and
hybrid designs that combine local and global components. By aligning
theoretical foundations with practical deployment strategies, this work aims to
serve as a foundational reference for advancing the design of scalable and
efficient language models.

</details>


### [6] [MOCHA: Are Code Language Models Robust Against Multi-Turn Malicious Coding Prompts?](https://arxiv.org/abs/2507.19598)
*Muntasir Wahed,Xiaona Zhou,Kiet A. Nguyen,Tianjiao Yu,Nirav Diwan,Gang Wang,Dilek Hakkani-T√ºr,Ismini Lourentzou*

Main category: cs.CL

TL;DR: The paper introduces code decomposition attacks to evade safety filters in LLMs and presents enchmarkname{}, a benchmark for evaluating robustness against malicious prompts, showing vulnerabilities especially in multi-turn scenarios.


<details>
  <summary>Details</summary>
Motivation: To explore and address the vulnerability of large language models in code generation against multi-turn malicious prompts.

Method: Developing code decomposition attacks, creating enchmarkname{} for systematic evaluation, and fine-tuning models on MOCHA to improve robustness.

Result: Vulnerabilities persist in models, especially under multi-turn prompts; fine-tuning increases rejection rates and robustness without losing coding abilities.

Conclusion: Current models are vulnerable to multi-turn attacks, but targeted fine-tuning enhances their robustness effectively.

Abstract: Recent advancements in Large Language Models (LLMs) have significantly
enhanced their code generation capabilities. However, their robustness against
adversarial misuse, particularly through multi-turn malicious coding prompts,
remains underexplored. In this work, we introduce code decomposition attacks,
where a malicious coding task is broken down into a series of seemingly benign
subtasks across multiple conversational turns to evade safety filters. To
facilitate systematic evaluation, we introduce \benchmarkname{}, a large-scale
benchmark designed to evaluate the robustness of code LLMs against both
single-turn and multi-turn malicious prompts. Empirical results across open-
and closed-source models reveal persistent vulnerabilities, especially under
multi-turn scenarios. Fine-tuning on MOCHA improves rejection rates while
preserving coding ability, and importantly, enhances robustness on external
adversarial datasets with up to 32.4% increase in rejection rates without any
additional supervision.

</details>


### [7] [HITSZ's End-To-End Speech Translation Systems Combining Sequence-to-Sequence Auto Speech Recognition Model and Indic Large Language Model for IWSLT 2025 in Indic Track](https://arxiv.org/abs/2507.19616)
*Xuchen Wei,Yangxin Wu,Yaoyin Zhang,Henglyu Liu,Kehai Chen,Xuefeng Bai,Min Zhang*

Main category: cs.CL

TL;DR: This paper describes HITSZ's speech-to-text translation system for English-Indic languages using Whisper and Krutrim, achieving notable BLEU scores, and explores Chain-of-Thought methods with mixed success.


<details>
  <summary>Details</summary>
Motivation: To improve translation quality for low-resource English-Indic language pairs by integrating advanced models and techniques.

Method: An end-to-end system combining Whisper ASR with Krutrim LLM, and investigation of Chain-of-Thought prompting.

Result: Achieved BLEU scores of 28.88 (English-Indic) and 27.86 (Indic-English). Chain-of-Thought improved some translations but faced consistency challenges.

Conclusion: The integrated system shows promise for low-resource translation, but further work is needed to reliably utilize Chain-of-Thought methods.

Abstract: This paper presents HITSZ's submission for the IWSLT 2025 Indic track,
focusing on speech-to-text translation (ST) for English-to-Indic and
Indic-to-English language pairs. To enhance translation quality in this
low-resource scenario, we propose an end-to-end system integrating the
pre-trained Whisper automated speech recognition (ASR) model with Krutrim, an
Indic-specialized large language model (LLM). Experimental results demonstrate
that our end-to-end system achieved average BLEU scores of $28.88$ for
English-to-Indic directions and $27.86$ for Indic-to-English directions.
Furthermore, we investigated the Chain-of-Thought (CoT) method. While this
method showed potential for significant translation quality improvements on
successfully parsed outputs (e.g. a $13.84$ BLEU increase for
Tamil-to-English), we observed challenges in ensuring the model consistently
adheres to the required CoT output format.

</details>


### [8] [MCIF: Multimodal Crosslingual Instruction-Following Benchmark from Scientific Talks](https://arxiv.org/abs/2507.19634)
*Sara Papi,Maike Z√ºfle,Marco Gaido,Beatrice Savoldi,Danni Liu,Ioannis Douros,Luisa Bentivogli,Jan Niehues*

Main category: cs.CL

TL;DR: Introduction of MCIF, a comprehensive multilingual, multimodal benchmark for evaluating large language models' instruction-following capabilities.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks are limited in assessing the joint multilingual and multimodal abilities of MLLMs, hindering progress in their development.

Method: Developing MCIF, a human-annotated benchmark based on scientific talks, covering speech, vision, and text across four languages, with both short and long contexts.

Result: MCIF provides a new platform for thorough evaluation of MLLMs' multilingual and multimodal instruction-following performance.

Conclusion: MCIF aims to accelerate research and development of more capable, multimodal, and multilingual large language models.

Abstract: Recent advances in large language models have catalyzed the development of
multimodal LLMs (MLLMs) that integrate text, speech, and vision within unified
frameworks. As MLLMs evolve from narrow, monolingual, task-specific systems to
general-purpose instruction-following models, a key frontier lies in evaluating
their multilingual and multimodal capabilities over both long and short
contexts. However, existing benchmarks fall short in evaluating these
dimensions jointly: they are often limited to English, mostly focus on one
single modality at a time, rely on short-form contexts, or lack human
annotations -- hindering comprehensive assessment of model performance across
languages, modalities, and task complexity. To address these gaps, we introduce
MCIF (Multimodal Crosslingual Instruction Following), the first multilingual
human-annotated benchmark based on scientific talks that is designed to
evaluate instruction-following in crosslingual, multimodal settings over both
short- and long-form inputs. MCIF spans three core modalities -- speech,
vision, and text -- and four diverse languages (English, German, Italian, and
Chinese), enabling a comprehensive evaluation of MLLMs' abilities to interpret
instructions across languages and combine them with multimodal contextual
information. MCIF is released under a CC-BY 4.0 license to encourage open
research and progress in MLLMs development.

</details>


### [9] [RoD-TAL: A Benchmark for Answering Questions in Romanian Driving License Exams](https://arxiv.org/abs/2507.19666)
*Andrei Vlad Man,RƒÉzvan-Alexandru SmƒÉdu,Cristian-George Craciun,Dumitru-Clementin Cercel,Florin Pop,Mihaela-Claudia Cercel*

Main category: cs.CL

TL;DR: This study evaluates Large Language and Vision-Language Models in understanding Romanian driving law, introducing a new multimodal dataset, RoD-TAL, and exploring various AI pipelines. Results show domain-specific fine-tuning improves retrieval, and reasoning techniques enhance question-answering, but visual reasoning remains challenging.


<details>
  <summary>Details</summary>
Motivation: Address the need for AI tools to support legal education in under-resourced languages like Romanian.

Method: Developed RoD-TAL dataset, implemented RAG pipelines, dense retrievers, and reasoning models to perform information retrieval and question answering in textual and visual formats.

Result: Fine-tuning improves retrieval; chain-of-thought and specialized models boost QA accuracy; visual reasoning is still difficult.

Conclusion: While promising techniques enhance legal question-answering, visual reasoning in legal contexts remains a challenge, indicating the potential and current limits of AI in legal education.

Abstract: The intersection of AI and legal systems presents a growing need for tools
that support legal education, particularly in under-resourced languages such as
Romanian. In this work, we aim to evaluate the capabilities of Large Language
Models (LLMs) and Vision-Language Models (VLMs) in understanding and reasoning
about Romanian driving law through textual and visual question-answering tasks.
To facilitate this, we introduce RoD-TAL, a novel multimodal dataset comprising
Romanian driving test questions, text-based and image-based, alongside
annotated legal references and human explanations. We implement and assess
retrieval-augmented generation (RAG) pipelines, dense retrievers, and
reasoning-optimized models across tasks including Information Retrieval (IR),
Question Answering (QA), Visual IR, and Visual QA. Our experiments demonstrate
that domain-specific fine-tuning significantly enhances retrieval performance.
At the same time, chain-of-thought prompting and specialized reasoning models
improve QA accuracy, surpassing the minimum grades required to pass driving
exams. However, visual reasoning remains challenging, highlighting the
potential and the limitations of applying LLMs and VLMs to legal education.

</details>


### [10] [Towards Inclusive NLP: Assessing Compressed Multilingual Transformers across Diverse Language Benchmarks](https://arxiv.org/abs/2507.19699)
*Maitha Alshehhi,Ahmed Sharshar,Mohsen Guizani*

Main category: cs.CL

TL;DR: The paper benchmarks multilingual and monolingual LLMs across several languages, highlighting that multilingual models outperform monolingual ones due to cross-lingual transfer, and discusses the effects of model compression techniques like pruning and quantization.


<details>
  <summary>Details</summary>
Motivation: To understand the performance of LLMs in low-resource languages and evaluate the impact of model compression strategies.

Method: Benchmarking various LLMs across Arabic, English, and Indic languages, analyzing the effects of pruning and quantization on performance.

Result: Multilingual models outperform monolingual ones, quantization maintains accuracy, while aggressive pruning reduces performance, especially in larger models.

Conclusion: Multilingual models show superior performance, and quantization is effective for efficiency, but pruning needs careful application; further interventions are needed for low-resource language robustness.

Abstract: Although LLMs have attained significant success in high-resource languages,
their capacity in low-resource linguistic environments like Kannada and Arabic
is not yet fully understood. This work benchmarking the performance of
multilingual and monolingual Large Language Models (LLMs) across Arabic,
English, and Indic languages, with particular emphasis on the effects of model
compression strategies such as pruning and quantization. Findings shows
significant performance differences driven by linguistic diversity and resource
availability on SOTA LLMS as BLOOMZ, AceGPT, Jais, LLaMA-2, XGLM, and AraGPT2.
We find that multilingual versions of the model outperform their
language-specific counterparts across the board, indicating substantial
cross-lingual transfer benefits. Quantization (4-bit and 8-bit) is effective in
maintaining model accuracy while promoting efficiency, but aggressive pruning
significantly compromises performance, especially in bigger models. Our
findings pinpoint key strategies to construct scalable and fair multilingual
NLP solutions and underscore the need for interventions to address
hallucination and generalization errors in the low-resource setting.

</details>


### [11] [Ta-G-T: Subjectivity Capture in Table to Text Generation via RDF Graphs](https://arxiv.org/abs/2507.19710)
*Ronak Upasham,Tathagata Dey,Pushpak Bhattacharyya*

Main category: cs.CL

TL;DR: A novel pipeline for table-to-text generation combines RDF extraction, narrative aggregation, and subjectivity infusion, balancing factual accuracy with interpretative subjectivity using smaller models.


<details>
  <summary>Details</summary>
Motivation: Address the underexplored area of subjective text generation from tabular data.

Method: A three-stage pipeline involving RDF triple extraction, narrative aggregation, and subjectivity infusion, using fine-tuned T5 models.

Result: Achieves performance comparable to GPT-3.5 and better than some larger models in balancing factuality and subjectivity.

Conclusion: The proposed structured pipeline is the first to integrate intermediate representations for balanced objective and subjective table-to-text generation.

Abstract: In Table-to-Text (T2T) generation, existing approaches predominantly focus on
providing objective descriptions of tabular data. However, generating text that
incorporates subjectivity, where subjectivity refers to interpretations beyond
raw numerical data, remains underexplored. To address this, we introduce a
novel pipeline that leverages intermediate representations to generate both
objective and subjective text from tables. Our three-stage pipeline consists
of: 1) extraction of Resource Description Framework (RDF) triples, 2)
aggregation of text into coherent narratives, and 3) infusion of subjectivity
to enrich the generated text. By incorporating RDFs, our approach enhances
factual accuracy while maintaining interpretability. Unlike large language
models (LLMs) such as GPT-3.5, Mistral-7B, and Llama-2, our pipeline employs
smaller, fine-tuned T5 models while achieving comparable performance to GPT-3.5
and outperforming Mistral-7B and Llama-2 in several metrics. We evaluate our
approach through quantitative and qualitative analyses, demonstrating its
effectiveness in balancing factual accuracy with subjective interpretation. To
the best of our knowledge, this is the first work to propose a structured
pipeline for T2T generation that integrates intermediate representations to
enhance both factual correctness and subjectivity.

</details>


### [12] [Basic Reading Distillation](https://arxiv.org/abs/2507.19741)
*Zhi Zhou,Sirui Miao,Xiangyu Duan,Hao Yang,Min Zhang*

Main category: cs.CL

TL;DR: Introducing Basic Reading Distillation (BRD), a new approach that trains small models on basic reading tasks to improve performance on downstream NLP tasks, outperforming or matching much larger models.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing distillation methods that neglect basic reading skills, aiming to enhance small models' general capabilities.

Method: Training small models on basic reading behaviors like named entity recognition and question answering, then applying them to various tasks.

Result: Small models trained with BRD outperform or match much larger models across multiple benchmarks, with orthogonal benefits to existing distillation approaches.

Conclusion: BRD effectively enhances small model performance by focusing on fundamental reading skills, offering a new direction for model distillation.

Abstract: Large language models (LLMs) have demonstrated remarkable abilities in
various natural language processing areas, but they demand high computation
resources which limits their deployment in real-world. Distillation is one
technique to solve this problem through either knowledge distillation or task
distillation. Both distillation approaches train small models to imitate
specific features of LLMs, but they all neglect basic reading education for
small models on generic texts that are \emph{unrelated} to downstream tasks. In
this paper, we propose basic reading distillation (BRD) which educates a small
model to imitate LLMs basic reading behaviors, such as named entity
recognition, question raising and answering, on each sentence. After such basic
education, we apply the small model on various tasks including language
inference benchmarks and BIG-bench tasks. It shows that the small model can
outperform or perform comparable to over 20x bigger LLMs. Analysis reveals that
BRD effectively influences the probability distribution of the small model, and
has orthogonality to either knowledge distillation or task distillation.

</details>


### [13] [JT-Math: A Multi-Stage Framework for Advanced Mathematical Reasoning in Large Language Models](https://arxiv.org/abs/2507.19748)
*Yifan Hao,Fangning Chao,Yaqian Hao,Zhaojun Cui,Huan Bai,Haiyu Zhang,Yankai Liu,Chao Deng,Junlan Feng*

Main category: cs.CL

TL;DR: JT-Math-8B is an open-source model series tailored for mathematical reasoning, utilizing multi-stage optimization, a high-quality dataset, and specific training strategies to outperform comparable models and excel in complex mathematical tasks.


<details>
  <summary>Details</summary>
Motivation: The paper aims to improve the mathematical reasoning capabilities of Large Language Models (LLMs), a key aspect of artificial general intelligence.

Method: The authors develop the JT-Math-8B series using a multi-stage training framework, including high-quality data curation, supervised fine-tuning, and reinforcement learning with a curriculum that targets complex problem-solving and extended context handling.

Result: JT-Math-8B achieves state-of-the-art results among open-source models for mathematical reasoning, surpassing models like O1-mini and GPT-4o, especially in competition-level tasks.

Conclusion: The proposed approach demonstrates that structured, multi-stage training and high-quality datasets can significantly enhance LLMs' ability to perform complex mathematical reasoning, advancing open-source AI research.

Abstract: Mathematical reasoning is a cornerstone of artificial general intelligence
and a primary benchmark for evaluating the capabilities of Large Language
Models (LLMs). While state-of-the-art models show promise, they often falter
when faced with complex problems that demand deep conceptual understanding and
intricate, multi-step deliberation. To address this challenge, we introduce
JT-Math-8B, a series of open-source models comprising base, instruct, and
thinking versions, built upon a systematic, multi-stage optimization framework.
Our pre-training corpus is a high-quality, 210B-token dataset curated through a
dedicated data pipeline that uses model-based validation to ensure quality and
diversity. The Instruct Model is optimized for direct, concise answers through
Supervised Fine-Tuning (SFT) and a GRPO-based reinforcement learning (RL)
method. The Thinking Model is trained for complex problem-solving using a Long
Chain-of-Thought (Long CoT) approach, combining SFT with a novel, multi-stage
RL curriculum that progressively increases task difficulty and context length
up to 32K tokens. JT-Math-8B achieves state-of-the-art results among
open-source models of similar size, surpassing prominent models like OpenAI's
O1-mini and GPT-4o , and demonstrating superior performance on
competition-level mathematics.

</details>


### [14] [Are You There God? Lightweight Narrative Annotation of Christian Fiction with LMs](https://arxiv.org/abs/2507.19756)
*Rebecca M. M. Hicke,Brian Haggard,Mia Ferrante,Rayhan Khanna,David Mimno*

Main category: cs.CL

TL;DR: This paper explores Christian Fiction, especially how divine acts are depicted, using computational tools and annotators, revealing notable differences based on genre and author gender.


<details>
  <summary>Details</summary>
Motivation: To study the cultural and literary aspects of the Evangelical movement, beyond its political activities, focusing on Christian Fiction.

Method: Developed a codebook for 'acts of God' with human annotators, then adapted these annotations for a lightweight language model to analyze the genre.

Result: The study found significant differences between the Left Behind series and broader Christian Fiction, as well as distinctions based on the authors' gender.

Conclusion: Computational tools can effectively analyze subtle literary themes in Christian Fiction, highlighting genre-specific and gender-based variations.

Abstract: In addition to its more widely studied political activities, the American
Evangelical movement has a well-developed but less externally visible cultural
and literary side. Christian Fiction, however, has been little studied, and
what scholarly attention there is has focused on the explosively popular Left
Behind series. In this work, we use computational tools to provide both a broad
topical overview of Christian Fiction as a genre and a more directed
exploration of how its authors depict divine acts. Working with human
annotators we first developed definitions and a codebook for "acts of God." We
then adapted those instructions designed for human annotators for use by a
recent, lightweight LM with the assistance of a much larger model. The
laptop-scale LM is capable of matching human annotations, even when the task is
subtle and challenging. Using these annotations, we show that significant and
meaningful differences exist between the Left Behind books and Christian
Fiction more broadly and between books by male and female authors.

</details>


### [15] [UloRL:An Ultra-Long Output Reinforcement Learning Approach for Advancing Large Language Models' Reasoning Abilities](https://arxiv.org/abs/2507.19766)
*Dong Du,Shulin Liu,Tao Yang,Shaohua Chen,Yang Li*

Main category: cs.CL

TL;DR: UloRL enhances LLM reasoning by dividing long outputs into segments and masking MPTs, improving training speed and reasoning accuracy.


<details>
  <summary>Details</summary>
Motivation: Address inefficiencies in traditional RL when generating ultra-long outputs for LLMs.

Method: Segmenting long outputs and dynamically masking well-mastered tokens during RL training.

Result: Significant improvements in training speed and reasoning performance on benchmark datasets, surpassing larger models.

Conclusion: Proposed UloRL effectively advances ultra-long output reasoning in LLMs and will share code and models.

Abstract: Recent advances in large language models (LLMs) have highlighted the
potential of reinforcement learning with verifiable rewards (RLVR) to enhance
reasoning capabilities through extended output sequences. However, traditional
RL frameworks face inefficiencies when handling ultra-long outputs due to
long-tail sequence distributions and entropy collapse during training. To
address these challenges, we propose an Ultra-Long Output Reinforcement
Learning (UloRL) approach for advancing large language models' reasoning
abilities. Specifically, we divide ultra long output decoding into short
segments, enabling efficient training by mitigating delays caused by long-tail
samples. Additionally, we introduce dynamic masking of well-Mastered Positive
Tokens (MPTs) to prevent entropy collapse. Experimental results demonstrate the
effectiveness of our approach. On the Qwen3-30B-A3B model, RL with segment
rollout achieved 2.06x increase in training speed, while RL training with
128k-token outputs improves the model's performance on AIME2025 from 70.9\% to
85.1\% and on BeyondAIME from 50.7\% to 61.9\%, even surpassing Qwen3-235B-A22B
with remarkable gains. These findings underscore the potential of our methods
to advance the reasoning capabilities of LLMs with ultra-long sequence
generation. We will release our code and model for further use by the
community.

</details>


### [16] [Flora: Effortless Context Construction to Arbitrary Length and Scale](https://arxiv.org/abs/2507.19786)
*Tianxiang Chen,Zhentao Tan,Xiaofan Bo,Yue Wu,Tao Gong,Qi Chu,Jieping Ye,Nenghai Yu*

Main category: cs.CL

TL;DR: Flora is a new method that improves LLMs' ability to handle long contexts by assembling short instructions to generate diverse, long contexts without additional human or LLM effort.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of handling long contexts in LLMs, which is limited by high computational costs and performance drop on short contexts in existing methods.

Method: Flora constructs long contexts by assembling short instructions based on categories and guides LLMs to generate responses from meta-instructions, enabling arbitrary, diverse context creation.

Result: Flora enhances Llama3 and QwQ models' performance on long-context benchmarks with minimal impact on short-context tasks.

Conclusion: Flora offers an effective, human/LLM-free solution for improving long-context handling in LLMs, with publicly available code.

Abstract: Effectively handling long contexts is challenging for Large Language Models
(LLMs) due to the rarity of long texts, high computational demands, and
substantial forgetting of short-context abilities. Recent approaches have
attempted to construct long contexts for instruction tuning, but these methods
often require LLMs or human interventions, which are both costly and limited in
length and diversity. Also, the drop in short-context performances of present
long-context LLMs remains significant. In this paper, we introduce Flora, an
effortless (human/LLM-free) long-context construction strategy. Flora can
markedly enhance the long-context performance of LLMs by arbitrarily assembling
short instructions based on categories and instructing LLMs to generate
responses based on long-context meta-instructions. This enables Flora to
produce contexts of arbitrary length and scale with rich diversity, while only
slightly compromising short-context performance. Experiments on
Llama3-8B-Instruct and QwQ-32B show that LLMs enhanced by Flora excel in three
long-context benchmarks while maintaining strong performances in short-context
tasks. Our data-construction code is available at
\href{https://github.com/txchen-USTC/Flora}{https://github.com/txchen-USTC/Flora}.

</details>


### [17] [HCAttention: Extreme KV Cache Compression via Heterogeneous Attention Computing for LLMs](https://arxiv.org/abs/2507.19823)
*Dongquan Yang,Yifan Yang,Xiaotian Yu,Xianbiao Qi,Rong Xiao*

Main category: cs.CL

TL;DR: HCAttention significantly reduces KV cache memory usage in large language models without losing accuracy, enabling processing of long contexts on minimal hardware.


<details>
  <summary>Details</summary>
Motivation: The challenge of efficiently processing long-context inputs with large language models under severe memory constraints.

Method: Integrates key quantization, value offloading, and dynamic KV eviction in a heterogeneous attention framework compatible with existing architectures, without requiring fine-tuning.

Result: Reduces KV cache to 25% of original size while maintaining accuracy; achieves state-of-the-art compression, enabling processing of 4 million tokens on a single GPU.

Conclusion: HCAttention effectively addresses memory limitations in long-context processing, broadening the applicability of large language models.

Abstract: Processing long-context inputs with large language models presents a
significant challenge due to the enormous memory requirements of the Key-Value
(KV) cache during inference. Existing KV cache compression methods exhibit
noticeable performance degradation when memory is reduced by more than 85%.
Additionally, strategies that leverage GPU-CPU collaboration for approximate
attention remain underexplored in this setting. We propose HCAttention, a
heterogeneous attention computation framework that integrates key quantization,
value offloading, and dynamic KV eviction to enable efficient inference under
extreme memory constraints. The method is compatible with existing transformer
architectures and does not require model fine-tuning. Experimental results on
the LongBench benchmark demonstrate that our approach preserves the accuracy of
full-attention model while shrinking the KV cache memory footprint to 25% of
its original size. Remarkably, it stays competitive with only 12.5% of the
cache, setting a new state-of-the-art in LLM KV cache compression. To the best
of our knowledge, HCAttention is the first to extend the Llama-3-8B model to
process 4 million tokens on a single A100 GPU with 80GB memory.

</details>


### [18] [DRIVE: Disfluency-Rich Synthetic Dialog Data Generation Framework for Intelligent Vehicle Environments](https://arxiv.org/abs/2507.19867)
*Anshul Chavda,M Jagadeesh,Chintalapalli Raja Kullayappa,B Jayaprakash,Medchalimi Sruthi,Pushpak Bhattacharyya*

Main category: cs.CL

TL;DR: DiscoDrive is a synthetic dataset of 3500 in-car dialogs with disfluencies, improving conversational AI training and evaluation.


<details>
  <summary>Details</summary>
Motivation: Current datasets lack spontaneous disfluencies in driver-AI interactions; this limits real-world applicability.

Method: Generated a synthetic corpus using a two-stage, prompt-driven pipeline that integrates disfluencies during synthesis.

Result: DiscoDrive enhances model performance on benchmark datasets, serving as effective training and data augmentation resource, with higher naturalness and coherence in human evaluations.

Conclusion: DiscoDrive addresses a critical gap, enabling more robust and natural in-car conversational AI.

Abstract: In-car conversational AI is becoming increasingly critical as autonomous
vehicles and smart assistants gain widespread adoption. Yet, existing datasets
fail to capture the spontaneous disfluencies such as hesitations, false starts,
repetitions, and self-corrections that characterize real driver-AI dialogs. To
address this, we introduce DiscoDrive, a synthetic corpus of 3500 multi-turn
dialogs across seven automotive domains, generated using a two-stage,
prompt-driven pipeline that dynamically integrates disfluencies during
synthesis. We show that DiscoDrive is effective both as a training resource,
enabling DialoGPT-Medium and T5-Base to match or exceed KVRET-trained models on
the MultiWOZ 2.2 and Schema-Guided Dialogue (SGD) relevant test sets (BLEU-4
improvements of 0.26 to 0.61; METEOR +2.10; ROUGE-L +3.48; BERTScore F1
improvements of 1.35 to 3.48), and as a data augmentation resource in
low-resource scenarios, delivering additional gains of up to BLEU-4 +0.38,
METEOR +1.95, ROUGE-L +2.87, and BERTScore F1 +4.00 when combined with 10
percent of KVRET. Human evaluations further confirm that dialogs sampled from
DiscoDrive are rated higher than KVRET's human-collected dialogs in naturalness
(3.8 vs 3.6) and coherence (4.1 vs 4.0), and are perceived as more
context-appropriate than leading post-hoc methods (such as LARD), without
compromising clarity. DiscoDrive fills a critical gap in existing resources and
serves as a versatile corpus for both training and augmenting conversational
AI, enabling robust handling of real-world, disfluent in-car interactions.

</details>


### [19] [The Polish Vocabulary Size Test: A Novel Adaptive Test for Receptive Vocabulary Assessment](https://arxiv.org/abs/2507.19869)
*Danil Fokin,Monika P≈Çu≈ºyczka,Grigory Golovin*

Main category: cs.CL

TL;DR: The PVST is an adaptive online tool for assessing Polish vocabulary size in both native and non-native speakers, validated through a pilot study.


<details>
  <summary>Details</summary>
Motivation: To create an accurate, efficient method for measuring Polish vocabulary size adaptable to different proficiency levels.

Method: Utilized Item Response Theory and Computerized Adaptive Testing in developing the PVST and validated it with a pilot study involving 1,475 participants.

Result: Native speakers have larger vocabularies than non-native speakers, with vocabulary size increasing with age in natives.

Conclusion: PVST is an effective online assessment tool for Polish vocabulary.

Abstract: We present the Polish Vocabulary Size Test (PVST), a novel tool for assessing
the receptive vocabulary size of both native and non-native Polish speakers.
Based on Item Response Theory and Computerized Adaptive Testing, PVST
dynamically adjusts to each test-taker's proficiency level, ensuring high
accuracy while keeping the test duration short. To validate the test, a pilot
study was conducted with 1.475 participants. Native Polish speakers
demonstrated significantly larger vocabularies compared to non-native speakers.
For native speakers, vocabulary size showed a strong positive correlation with
age. The PVST is available online at myvocab.info/pl.

</details>


### [20] [Zero-shot Performance of Generative AI in Brazilian Portuguese Medical Exam](https://arxiv.org/abs/2507.19885)
*Cesar Augusto Madid Truyts,Amanda Gomes Rabelo,Gabriel Mesquita de Souza,Daniel Scaldaferri Lages,Adriano Jose Pereira,Uri Adrian Prync Flato,Eduardo Pontes dos Reis,Joaquim Edson Vieira,Paulo Sergio Panse Silveira,Edson Amaro Junior*

Main category: cs.CL

TL;DR: The study evaluates the performance of several LLMs and MLLMs in understanding Brazilian Portuguese medical exam questions, highlighting the need for more diverse language training to ensure fair healthcare AI applications.


<details>
  <summary>Details</summary>
Motivation: To address the gap in evaluating AI models in non-English languages within healthcare, specifically Brazilian Portuguese.

Method: Benchmarking six LLMs and four MLLMs against human candidates in answering medical exam questions, analyzing accuracy, processing time, and coherence.

Result: Some models, especially Claude-3.5-Sonnet and Claude-3-Opus, performed comparably to humans, but with notable gaps in multimodal question interpretation; language disparities were also observed.

Conclusion: Further fine-tuning and dataset expansion are necessary for non-English medical AI, with ongoing evaluation across languages to ensure fair and effective healthcare deployment.

Abstract: Artificial intelligence (AI) has shown the potential to revolutionize
healthcare by improving diagnostic accuracy, optimizing workflows, and
personalizing treatment plans. Large Language Models (LLMs) and Multimodal
Large Language Models (MLLMs) have achieved notable advancements in natural
language processing and medical applications. However, the evaluation of these
models has focused predominantly on the English language, leading to potential
biases in their performance across different languages.
  This study investigates the capability of six LLMs (GPT-4.0 Turbo,
LLaMA-3-8B, LLaMA-3-70B, Mixtral 8x7B Instruct, Titan Text G1-Express, and
Command R+) and four MLLMs (Claude-3.5-Sonnet, Claude-3-Opus, Claude-3-Sonnet,
and Claude-3-Haiku) to answer questions written in Brazilian spoken portuguese
from the medical residency entrance exam of the Hospital das Cl\'inicas da
Faculdade de Medicina da Universidade de S\~ao Paulo (HCFMUSP) - the largest
health complex in South America. The performance of the models was benchmarked
against human candidates, analyzing accuracy, processing time, and coherence of
the generated explanations.
  The results show that while some models, particularly Claude-3.5-Sonnet and
Claude-3-Opus, achieved accuracy levels comparable to human candidates,
performance gaps persist, particularly in multimodal questions requiring image
interpretation. Furthermore, the study highlights language disparities,
emphasizing the need for further fine-tuning and data set augmentation for
non-English medical AI applications.
  Our findings reinforce the importance of evaluating generative AI in various
linguistic and clinical settings to ensure a fair and reliable deployment in
healthcare. Future research should explore improved training methodologies,
improved multimodal reasoning, and real-world clinical integration of AI-driven
medical assistance.

</details>


### [21] [A Gold Standard Dataset and Evaluation Framework for Depression Detection and Explanation in Social Media using LLMs](https://arxiv.org/abs/2507.19899)
*Prajval Bolegave,Pushpak Bhattacharya*

Main category: cs.CL

TL;DR: The paper introduces a detailed, expert-annotated dataset of social media posts for depression detection, and evaluates large language models' ability to generate clinically meaningful explanations, highlighting the importance of expert guidance for AI safety in mental health.


<details>
  <summary>Details</summary>
Motivation: To improve early depression detection from social media and evaluate the quality of AI explanations in clinical contexts.

Method: Creating a fine-grained, expert-annotated dataset and testing various prompting strategies on top LLMs to generate and assess explanations.

Result: Different LLMs show varying performance on clinical explanation tasks; prompting strategies significantly influence outcomes, emphasizing human expertise's importance.

Conclusion: Expert-guided prompting enhances AI safety and transparency, advancing mental health applications of LLMs.

Abstract: Early detection of depression from online social media posts holds promise
for providing timely mental health interventions. In this work, we present a
high-quality, expert-annotated dataset of 1,017 social media posts labeled with
depressive spans and mapped to 12 depression symptom categories. Unlike prior
datasets that primarily offer coarse post-level labels
\cite{cohan-etal-2018-smhd}, our dataset enables fine-grained evaluation of
both model predictions and generated explanations.
  We develop an evaluation framework that leverages this clinically grounded
dataset to assess the faithfulness and quality of natural language explanations
generated by large language models (LLMs). Through carefully designed prompting
strategies, including zero-shot and few-shot approaches with domain-adapted
examples, we evaluate state-of-the-art proprietary LLMs including GPT-4.1,
Gemini 2.5 Pro, and Claude 3.7 Sonnet.
  Our comprehensive empirical analysis reveals significant differences in how
these models perform on clinical explanation tasks, with zero-shot and few-shot
prompting. Our findings underscore the value of human expertise in guiding LLM
behavior and offer a step toward safer, more transparent AI systems for
psychological well-being.

</details>


### [22] [CaliDrop: KV Cache Compression with Calibration](https://arxiv.org/abs/2507.19906)
*Yi Su,Quantong Qiu,Yuechi Zhou,Juntao Li,Qingrong Xia,Ping Li,Xinyu Duan,Zhefeng Wang,Min Zhang*

Main category: cs.CL

TL;DR: CaliDrop improves token eviction in LLMs by using speculative calibration to reduce accuracy loss, enabling more efficient memory usage.


<details>
  <summary>Details</summary>
Motivation: The need to reduce memory bottlenecks caused by large KV caches in LLMs during long-context generation.

Method: Introducing CaliDrop, which performs speculative calibration on evicted tokens based on observed high similarity among neighboring query positions.

Result: CaliDrop significantly enhances the accuracy of existing token eviction techniques, maintaining model performance while reducing memory footprint.

Conclusion: CaliDrop offers a promising approach for memory-efficient LLMs by balancing cache compression and accuracy.

Abstract: Large Language Models (LLMs) require substantial computational resources
during generation. While the Key-Value (KV) cache significantly accelerates
this process by storing attention intermediates, its memory footprint grows
linearly with sequence length, batch size, and model size, creating a
bottleneck in long-context scenarios. Various KV cache compression techniques,
including token eviction, quantization, and low-rank projection, have been
proposed to mitigate this bottleneck, often complementing each other. This
paper focuses on enhancing token eviction strategies. Token eviction leverages
the observation that the attention patterns are often sparse, allowing for the
removal of less critical KV entries to save memory. However, this reduction
usually comes at the cost of notable accuracy degradation, particularly under
high compression ratios. To address this issue, we propose \textbf{CaliDrop}, a
novel strategy that enhances token eviction through calibration. Our
preliminary experiments show that queries at nearby positions exhibit high
similarity. Building on this observation, CaliDrop performs speculative
calibration on the discarded tokens to mitigate the accuracy loss caused by
token eviction. Extensive experiments demonstrate that CaliDrop significantly
improves the accuracy of existing token eviction methods.

</details>


### [23] [KLAAD: Refining Attention Mechanisms to Reduce Societal Bias in Generative Language Models](https://arxiv.org/abs/2507.19962)
*Seorin Kim,Dongyoung Lee,Jaejin Lee*

Main category: cs.CL

TL;DR: KLAAD is an attention-based debiasing method that aligns attention distributions between stereotypical and anti-stereotypical sentences to reduce societal biases in language models.


<details>
  <summary>Details</summary>
Motivation: Address societal biases in large language models, which pose ethical concerns of fairness and harm.

Method: Use a composite training objective combining Cross-Entropy, KL divergence, and Triplet losses to align attention across biased and unbiased contexts without modifying model weights.

Result: Improved bias mitigation on BBQ and BOLD benchmarks with minimal impact on language quality.

Conclusion: Attention-level alignment is an effective approach for bias mitigation in language models.

Abstract: Large language models (LLMs) often exhibit societal biases in their outputs,
prompting ethical concerns regarding fairness and harm. In this work, we
propose KLAAD (KL-Attention Alignment Debiasing), an attention-based debiasing
framework that implicitly aligns attention distributions between stereotypical
and anti-stereotypical sentence pairs without directly modifying model weights.
KLAAD introduces a composite training objective combining Cross-Entropy, KL
divergence, and Triplet losses, guiding the model to consistently attend across
biased and unbiased contexts while preserving fluency and coherence.
Experimental evaluation of KLAAD demonstrates improved bias mitigation on both
the BBQ and BOLD benchmarks, with minimal impact on language modeling quality.
The results indicate that attention-level alignment offers a principled
solution for mitigating bias in generative language models.

</details>


### [24] [Text2Vis: A Challenging and Diverse Benchmark for Generating Multimodal Visualizations from Text](https://arxiv.org/abs/2507.19969)
*Mizanur Rahman,Md Tahmid Rahman Laskar,Shafiq Joty,Enamul Hoque*

Main category: cs.CL

TL;DR: Introduction of Text2Vis, a benchmark for evaluating text-to-visualization models across diverse chart types and queries.


<details>
  <summary>Details</summary>
Motivation: To enable rigorous assessment of LLMs in generating visualizations and address performance gaps.

Method: Creating a comprehensive benchmark with diverse data science queries, and proposing a cross-modal actor-critic framework along with an automated evaluation system.

Result: Identified performance gaps among models; improved GPT-4o's success rate from 26% to 42% using the proposed framework; facilitating scalable evaluation.

Conclusion: Text2Vis advances the evaluation of AI models in data visualization, providing tools and benchmarks to foster future improvements.

Abstract: Automated data visualization plays a crucial role in simplifying data
interpretation, enhancing decision-making, and improving efficiency. While
large language models (LLMs) have shown promise in generating visualizations
from natural language, the absence of comprehensive benchmarks limits the
rigorous evaluation of their capabilities. We introduce Text2Vis, a benchmark
designed to assess text-to-visualization models, covering 20+ chart types and
diverse data science queries, including trend analysis, correlation, outlier
detection, and predictive analytics. It comprises 1,985 samples, each with a
data table, natural language query, short answer, visualization code, and
annotated charts. The queries involve complex reasoning, conversational turns,
and dynamic data retrieval. We benchmark 11 open-source and closed-source
models, revealing significant performance gaps, highlighting key challenges,
and offering insights for future advancements. To close this gap, we propose
the first cross-modal actor-critic agentic framework that jointly refines the
textual answer and visualization code, increasing GPT-4o`s pass rate from 26%
to 42% over the direct approach and improving chart quality. We also introduce
an automated LLM-based evaluation framework that enables scalable assessment
across thousands of samples without human annotation, measuring answer
correctness, code execution success, visualization readability, and chart
accuracy. We release Text2Vis at https://github.com/vis-nlp/Text2Vis.

</details>


### [25] [Exploring LLM Autoscoring Reliability in Large-Scale Writing Assessments Using Generalizability Theory](https://arxiv.org/abs/2507.19980)
*Dan Song,Won-Chan Lee,Hong Jiao*

Main category: cs.CL

TL;DR: The study evaluates the reliability of AI scoring in AP Chinese writing tasks, finding that while humans are more reliable overall, AI shows promise, especially in story narration, and hybrid scoring can enhance reliability.


<details>
  <summary>Details</summary>
Motivation: To explore the potential of AI in scoring writing assessments and compare its reliability to human raters.

Method: Using generalizability theory to assess score consistency between human and AI raters on different writing tasks.

Result: AI raters are reasonably consistent in some contexts, and hybrid scoring improves reliability.

Conclusion: Hybrid scoring models combining human and AI raters may enhance large-scale assessment reliability.

Abstract: This study investigates the estimation of reliability for large language
models (LLMs) in scoring writing tasks from the AP Chinese Language and Culture
Exam. Using generalizability theory, the research evaluates and compares score
consistency between human and AI raters across two types of AP Chinese
free-response writing tasks: story narration and email response. These essays
were independently scored by two trained human raters and seven AI raters. Each
essay received four scores: one holistic score and three analytic scores
corresponding to the domains of task completion, delivery, and language use.
Results indicate that although human raters produced more reliable scores
overall, LLMs demonstrated reasonable consistency under certain conditions,
particularly for story narration tasks. Composite scoring that incorporates
both human and AI raters improved reliability, which supports that hybrid
scoring models may offer benefits for large-scale writing assessments.

</details>


### [26] [VLQA: The First Comprehensive, Large, and High-Quality Vietnamese Dataset for Legal Question Answering](https://arxiv.org/abs/2507.19995)
*Tan-Minh Nguyen,Hoang-Trung Nguyen,Trong-Khoi Dao,Xuan-Hieu Phan,Ha-Thanh Nguyen,Thi-Hai-Yen Vuong*

Main category: cs.CL

TL;DR: Introduction of VLQA, a Vietnamese legal dataset, and evaluation of its usefulness for legal NLP tasks.


<details>
  <summary>Details</summary>
Motivation: Address the scarcity of legal NLP resources in low-resource languages like Vietnamese, and improve legal text processing.

Method: Creating a high-quality legal dataset for Vietnamese, analyzing it statistically, and testing with state-of-the-art models on retrieval and question-answering tasks.

Result: The dataset was shown to be effective for legal NLP tasks, demonstrating its potential for advancing legal AI in Vietnamese.

Conclusion: VLQA is a valuable resource that can facilitate future research and development in Vietnamese legal NLP applications.

Abstract: The advent of large language models (LLMs) has led to significant
achievements in various domains, including legal text processing. Leveraging
LLMs for legal tasks is a natural evolution and an increasingly compelling
choice. However, their capabilities are often portrayed as greater than they
truly are. Despite the progress, we are still far from the ultimate goal of
fully automating legal tasks using artificial intelligence (AI) and natural
language processing (NLP). Moreover, legal systems are deeply domain-specific
and exhibit substantial variation across different countries and languages. The
need for building legal text processing applications for different natural
languages is, therefore, large and urgent. However, there is a big challenge
for legal NLP in low-resource languages such as Vietnamese due to the scarcity
of resources and annotated data. The need for labeled legal corpora for
supervised training, validation, and supervised fine-tuning is critical. In
this paper, we introduce the VLQA dataset, a comprehensive and high-quality
resource tailored for the Vietnamese legal domain. We also conduct a
comprehensive statistical analysis of the dataset and evaluate its
effectiveness through experiments with state-of-the-art models on legal
information retrieval and question-answering tasks.

</details>


### [27] [Anomaly Detection in Human Language via Meta-Learning: A Few-Shot Approach](https://arxiv.org/abs/2507.20019)
*Saurav Singla,Aarav Singla,Advik Gupta,Parnika Gupta*

Main category: cs.CL

TL;DR: A meta-learning framework for few-shot anomaly detection in human language across various domains.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of detecting diverse and sparse language anomalies like spam, fake news, and hate speech with limited labeled data.

Method: Combining episodic training, prototypical networks, and domain resampling within a meta-learning approach for quick adaptation to new tasks.

Result: Outperforms several strong baselines in F1 and AUC metrics, demonstrating effective generalization to unseen tasks.

Conclusion: The approach enhances few-shot language anomaly detection and provides resources for further research.

Abstract: We propose a meta learning framework for detecting anomalies in human
language across diverse domains with limited labeled data. Anomalies in
language ranging from spam and fake news to hate speech pose a major challenge
due to their sparsity and variability. We treat anomaly detection as a few shot
binary classification problem and leverage meta-learning to train models that
generalize across tasks. Using datasets from domains such as SMS spam, COVID-19
fake news, and hate speech, we evaluate model generalization on unseen tasks
with minimal labeled anomalies. Our method combines episodic training with
prototypical networks and domain resampling to adapt quickly to new anomaly
detection tasks. Empirical results show that our method outperforms strong
baselines in F1 and AUC scores. We also release the code and benchmarks to
facilitate further research in few-shot text anomaly detection.

</details>


### [28] [FAEDKV: Infinite-Window Fourier Transform for Unbiased KV Cache Compression](https://arxiv.org/abs/2507.20030)
*Runchao Li,Yao Fu,Mu Sheng,Xianxuan Long,Haotian Yu,Pan Li*

Main category: cs.CL

TL;DR: FAEDKV is a training-free KV cache compression method using Fourier Transform to preserve unbiased long-term information, outperforming existing techniques.


<details>
  <summary>Details</summary>
Motivation: To address the biased information retention and high resource demands of current KV cache compression methods in LLMs.

Method: Transforming KV cache into the frequency domain with Infinite-Window Fourier Transform to ensure equalized contribution of all tokens.

Result: FAEDKV outperforms existing methods by up to 22% on LongBench and exhibits better position-agnostic retrieval accuracy on Needle-In-A-Haystack.

Conclusion: FAEDKV effectively maintains unbiased long-term context information in KV caches without retraining, improving long-context task performance.

Abstract: The efficacy of Large Language Models (LLMs) in long-context tasks is often
hampered by the substantial memory footprint and computational demands of the
Key-Value (KV) cache. Current compression strategies, including token eviction
and learned projections, frequently lead to biased representations -- either by
overemphasizing recent/high-attention tokens or by repeatedly degrading
information from earlier context -- and may require costly model retraining. We
present FAEDKV (Frequency-Adaptive Infinite-Window for KV cache), a novel,
training-free KV cache compression framework that ensures unbiased information
retention. FAEDKV operates by transforming the KV cache into the frequency
domain using a proposed Infinite-Window Fourier Transform (IWDFT). This
approach allows for the equalized contribution of all tokens to the compressed
representation, effectively preserving both early and recent contextual
information. A preliminary frequency ablation study identifies critical
spectral components for layer-wise, targeted compression. Experiments on
LongBench benchmark demonstrate FAEDKV's superiority over existing methods by
up to 22\%. In addition, our method shows superior, position-agnostic retrieval
accuracy on the Needle-In-A-Haystack task compared to compression based
approaches.

</details>


### [29] [Infogen: Generating Complex Statistical Infographics from Documents](https://arxiv.org/abs/2507.20046)
*Akash Ghosh,Aparna Garimella,Pritika Ramu,Sambaran Bandyopadhyay,Sriparna Saha*

Main category: cs.CL

TL;DR: The paper introduces a new task and dataset for generating complex statistical infographics from text-heavy documents, and proposes a two-stage framework called Infogen that outperforms existing LLMs.


<details>
  <summary>Details</summary>
Motivation: To address the limited capabilities of current AI systems in generating complex, multi-chart infographics from detailed textual content.

Method: Development of a new dataset (Infodat) and a two-stage framework (Infogen) involving fine-tuned LLMs for metadata generation and infographic coding.

Result: Infogen achieves state-of-the-art performance on the Infodat dataset, surpassing existing models in text-to-infographic generation.

Conclusion: This work advances the field by enabling the creation of informative, complex infographics from textual data, filling a significant research gap.

Abstract: Statistical infographics are powerful tools that simplify complex data into
visually engaging and easy-to-understand formats. Despite advancements in AI,
particularly with LLMs, existing efforts have been limited to generating simple
charts, with no prior work addressing the creation of complex infographics from
text-heavy documents that demand a deep understanding of the content. We
address this gap by introducing the task of generating statistical infographics
composed of multiple sub-charts (e.g., line, bar, pie) that are contextually
accurate, insightful, and visually aligned. To achieve this, we define
infographic metadata that includes its title and textual insights, along with
sub-chart-specific details such as their corresponding data and alignment. We
also present Infodat, the first benchmark dataset for text-to-infographic
metadata generation, where each sample links a document to its metadata. We
propose Infogen, a two-stage framework where fine-tuned LLMs first generate
metadata, which is then converted into infographic code. Extensive evaluations
on Infodat demonstrate that Infogen achieves state-of-the-art performance,
outperforming both closed and open-source LLMs in text-to-statistical
infographic generation.

</details>


### [30] [A Tensor-Based Compiler and a Runtime for Neuron-Level DNN Certifier Specifications](https://arxiv.org/abs/2507.20055)
*Avaljot Singh,Yamin Chandini Sarita,Aditya Mishra,Ishaan Goyal,Gagandeep Singh,Charith Mendis*

Main category: cs.CL

TL;DR: A compiler framework translates neuron-level DNN certifiers into efficient tensor-based implementations, facilitating certifier development and analysis.


<details>
  <summary>Details</summary>
Motivation: Address the difficulty in developing and modifying DNN certifiers due to the semantic gap between their design and implementation.

Method: Introducing a compiler with a novel IR and shape analysis to automate translation from neuron-level specs to tensor operations, coupled with a new sparse tensor format (g-BCSR).

Result: The framework enables easy development of certifiers with performance comparable to manual implementations and handles diverse DNN architectures.

Conclusion: The proposed compiler and representation bridge the gap between design and implementation, advancing practical DNN certification.

Abstract: The uninterpretability of DNNs has led to the adoption of abstract
interpretation-based certification as a practical means to establish trust in
real-world systems that rely on DNNs. However, the current landscape supports
only a limited set of certifiers, and developing new ones or modifying existing
ones for different applications remains difficult. This is because the
mathematical design of certifiers is expressed at the neuron level, while their
implementations are optimized and executed at the tensor level. This mismatch
creates a semantic gap between design and implementation, making manual
bridging both complex and expertise-intensive -- requiring deep knowledge in
formal methods, high-performance computing, etc.
  We propose a compiler framework that automatically translates neuron-level
specifications of DNN certifiers into tensor-based, layer-level
implementations. This is enabled by two key innovations: a novel stack-based
intermediate representation (IR) and a shape analysis that infers the implicit
tensor operations needed to simulate the neuron-level semantics. During
lifting, the shape analysis creates tensors in the minimal shape required to
perform the corresponding operations. The IR also enables domain-specific
optimizations as rewrites. At runtime, the resulting tensor computations
exhibit sparsity tied to the DNN architecture. This sparsity does not align
well with existing formats. To address this, we introduce g-BCSR, a
double-compression format that represents tensors as collections of blocks of
varying sizes, each possibly internally sparse.
  Using our compiler and g-BCSR, we make it easy to develop new certifiers and
analyze their utility across diverse DNNs. Despite its flexibility, the
compiler achieves performance comparable to hand-optimized implementations.

</details>


### [31] [RAG in the Wild: On the (In)effectiveness of LLMs with Mixture-of-Knowledge Retrieval Augmentation](https://arxiv.org/abs/2507.20059)
*Ran Xu,Yuchen Zhuang,Yue Yu,Haoyu Wang,Wenqi Shi,Carl Yang*

Main category: cs.CL

TL;DR: Retrieval-augmented generation (RAG) improves language models by using external knowledge but faces challenges in diverse, real-world scenarios, especially with larger and heterogeneous knowledge sources.


<details>
  <summary>Details</summary>
Motivation: To evaluate RAG systems beyond benchmark datasets and understand their limitations in realistic, diverse retrieval environments.

Method: Using MassiveDS, a large-scale, diverse datastore, to test RAG performance and analyze factors such as model size, reranking, and knowledge source diversity.

Result: Findings indicate RAG mainly benefits smaller models, rerankers add little value, no single source is superior, and models struggle to navigate multiple knowledge sources effectively.

Conclusion: Effective deployment of RAG requires adaptive retrieval strategies, emphasizing the need for further research into managing heterogeneous knowledge sources in real-world applications.

Abstract: Retrieval-augmented generation (RAG) enhances large language models (LLMs) by
integrating external knowledge retrieved at inference time. While RAG
demonstrates strong performance on benchmarks largely derived from
general-domain corpora like Wikipedia, its effectiveness under realistic,
diverse retrieval scenarios remains underexplored. We evaluated RAG systems
using MassiveDS, a large-scale datastore with mixture of knowledge, and
identified critical limitations: retrieval mainly benefits smaller models,
rerankers add minimal value, and no single retrieval source consistently
excels. Moreover, current LLMs struggle to route queries across heterogeneous
knowledge sources. These findings highlight the need for adaptive retrieval
strategies before deploying RAG in real-world settings. Our code and data can
be found at https://github.com/ritaranx/RAG_in_the_Wild.

</details>


### [32] [ProsodyLM: Uncovering the Emerging Prosody Processing Capabilities in Speech Language Models](https://arxiv.org/abs/2507.20091)
*Kaizhi Qian,Xulin Fan,Junrui Ni,Slava Shechtman,Mark Hasegawa-Johnson,Chuang Gan,Yang Zhang*

Main category: cs.CL

TL;DR: ProsodyLM introduces a new tokenization scheme that enhances prosody learning in speech language models, enabling better processing of emotional and stress-related prosody in speech.


<details>
  <summary>Details</summary>
Motivation: Existing speech language models struggle to capture prosody due to tokenization limitations, hindering their ability to process nuanced speech features.

Method: Proposes ProsodyLM with a new prosody-preserving tokenization scheme that transcribes speech to text followed by prosody tokens, making prosody information more accessible to LLMs.

Result: ProsodyLM demonstrates new capabilities in prosody processing, including emotion recognition, stress, contrastive focus, and prosody consistency in long contexts, via pre-training alone.

Conclusion: The novel tokenization scheme in ProsodyLM significantly improves prosody understanding in speech language models, broadening their applications in speech processing.

Abstract: Speech language models refer to language models with speech processing and
understanding capabilities. One key desirable capability for speech language
models is the ability to capture the intricate interdependency between content
and prosody. The existing mainstream paradigm of training speech language
models, which converts speech into discrete tokens before feeding them into
LLMs, is sub-optimal in learning prosody information -- we find that the
resulting LLMs do not exhibit obvious emerging prosody processing capabilities
via pre-training alone. To overcome this, we propose ProsodyLM, which
introduces a simple tokenization scheme amenable to learning prosody. Each
speech utterance is first transcribed into text, followed by a sequence of
word-level prosody tokens. Compared with conventional speech tokenization
schemes, the proposed tokenization scheme retains more complete prosody
information, and is more understandable to text-based LLMs. We find that
ProsodyLM can learn surprisingly diverse emerging prosody processing
capabilities through pre-training alone, ranging from harnessing the prosody
nuances in generated speech, such as contrastive focus, understanding emotion
and stress in an utterance, to maintaining prosody consistency in long
contexts.

</details>


### [33] [AI-Driven Generation of Old English: A Framework for Low-Resource Languages](https://arxiv.org/abs/2507.20111)
*Rodrigo Gabriel Salazar Alva,Mat√≠as Nu√±ez,Cristian L√≥pez,Javier Mart√≠n Arista*

Main category: cs.CL

TL;DR: The paper introduces a scalable framework using large language models with fine-tuning, data augmentation, and dual-agent pipeline to generate high-quality Old English texts, improving translation metrics and aiding language preservation.


<details>
  <summary>Details</summary>
Motivation: To address the scarcity of resources for Old English, enabling better NLP applications and supporting cultural heritage preservation.

Method: Combines parameter-efficient fine-tuning (LoRA), backtranslation data augmentation, and a dual-agent pipeline separating content generation and translation.

Result: Achieved significant improvements in translation quality metrics (BLEU over 65, up from 26), with high grammatical and stylistic fidelity confirmed by human experts.

Conclusion: The approach effectively enhances Old English corpus and offers a scalable template for revitalizing other endangered languages through AI.

Abstract: Preserving ancient languages is essential for understanding humanity's
cultural and linguistic heritage, yet Old English remains critically
under-resourced, limiting its accessibility to modern natural language
processing (NLP) techniques. We present a scalable framework that uses advanced
large language models (LLMs) to generate high-quality Old English texts,
addressing this gap. Our approach combines parameter-efficient fine-tuning
(Low-Rank Adaptation, LoRA), data augmentation via backtranslation, and a
dual-agent pipeline that separates the tasks of content generation (in English)
and translation (into Old English). Evaluation with automated metrics (BLEU,
METEOR, and CHRF) shows significant improvements over baseline models, with
BLEU scores increasing from 26 to over 65 for English-to-Old English
translation. Expert human assessment also confirms high grammatical accuracy
and stylistic fidelity in the generated texts. Beyond expanding the Old English
corpus, our method offers a practical blueprint for revitalizing other
endangered languages, effectively uniting AI innovation with the goals of
cultural preservation.

</details>


### [34] [Sem-DPO: Mitigating Semantic Inconsistency in Preference Optimization for Prompt Engineering](https://arxiv.org/abs/2507.20133)
*Anas Mohamed,Azal Ahmad Khan,Xinran Wang,Ahmad Faraz Khan,Shuwen Ge,Saman Bahzad Khan,Ayaan Ahmad,Ali Anwar*

Main category: cs.CL

TL;DR: Sem-DPO enhances preference optimization for prompts by incorporating semantic consistency, leading to improved image generation quality and human preferences.


<details>
  <summary>Details</summary>
Motivation: Improve prompt optimization in generative AI by addressing semantic drift in preference-based methods.

Method: Scaling DPO loss with an exponential weight based on cosine distance in embedding space to preserve semantic similarity.

Result: Achieved 8-12% higher CLIP similarity and 5-9% higher human preferences, outperforming DPO and other baselines.

Conclusion: Sem-DPO maintains prompts within a bounded semantic neighborhood, offering a robust and effective enhancement for prompt optimization.

Abstract: Generative AI can now synthesize strikingly realistic images from text, yet
output quality remains highly sensitive to how prompts are phrased. Direct
Preference Optimization (DPO) offers a lightweight, off-policy alternative to
RL for automatic prompt engineering, but its token-level regularization leaves
semantic inconsistency unchecked as prompts that win higher preference scores
can still drift away from the user's intended meaning.
  We introduce Sem-DPO, a variant of DPO that preserves semantic consistency
yet retains its simplicity and efficiency. Sem-DPO scales the DPO loss by an
exponential weight proportional to the cosine distance between the original
prompt and winning candidate in embedding space, softly down-weighting training
signals that would otherwise reward semantically mismatched prompts. We provide
the first analytical bound on semantic drift for preference-tuned prompt
generators, showing that Sem-DPO keeps learned prompts within a provably
bounded neighborhood of the original text. On three standard text-to-image
prompt-optimization benchmarks and two language models, Sem-DPO achieves 8-12%
higher CLIP similarity and 5-9% higher human-preference scores (HPSv2.1,
PickScore) than DPO, while also outperforming state-of-the-art baselines. These
findings suggest that strong flat baselines augmented with semantic weighting
should become the new standard for prompt-optimization studies and lay the
groundwork for broader, semantics-aware preference optimization in language
models.

</details>


### [35] [Multi-Stage Verification-Centric Framework for Mitigating Hallucination in Multi-Modal RAG](https://arxiv.org/abs/2507.20136)
*Baiyu Chen,Wilson Wongso,Xiaoqian Hu,Yue Tan,Flora Salim*

Main category: cs.CL

TL;DR: The paper presents a multi-stage framework to improve factual accuracy and reduce hallucinations in multi-modal, multi-turn question-answering systems, achieving 3rd place in a competitive benchmark.


<details>
  <summary>Details</summary>
Motivation: Address the hallucination problem in Vision Language Models, especially with egocentric imagery and complex questions, crucial for real-world fact-seeking applications.

Method: A multi-stage approach including a query router, retrieval and summarization pipeline, dual-path generation, and post-hoc verification to ensure factual accuracy and reduce hallucinations.

Result: Achieved 3rd place in the KDD Cup 2025 CRAG-MM challenge, demonstrating improved answer reliability.

Conclusion: Prioritizing answer reliability over completeness effectively reduces hallucinations in multi-modal RAG systems, with strong competitive results.

Abstract: This paper presents the technical solution developed by team CRUISE for the
KDD Cup 2025 Meta Comprehensive RAG Benchmark for Multi-modal, Multi-turn
(CRAG-MM) challenge. The challenge aims to address a critical limitation of
modern Vision Language Models (VLMs): their propensity to hallucinate,
especially when faced with egocentric imagery, long-tail entities, and complex,
multi-hop questions. This issue is particularly problematic in real-world
applications where users pose fact-seeking queries that demand high factual
accuracy across diverse modalities. To tackle this, we propose a robust,
multi-stage framework that prioritizes factual accuracy and truthfulness over
completeness. Our solution integrates a lightweight query router for
efficiency, a query-aware retrieval and summarization pipeline, a dual-pathways
generation and a post-hoc verification. This conservative strategy is designed
to minimize hallucinations, which incur a severe penalty in the competition's
scoring metric. Our approach achieved 3rd place in Task 1, demonstrating the
effectiveness of prioritizing answer reliability in complex multi-modal RAG
systems. Our implementation is available at
https://github.com/Breezelled/KDD-Cup-2025-Meta-CRAG-MM .

</details>


### [36] [Multi-Agent Interactive Question Generation Framework for Long Document Understanding](https://arxiv.org/abs/2507.20145)
*Kesen Wang,Daulet Toibazar,Abdulrahman Alfulayt,Abdulaziz S. Albadawi,Ranya A. Alkahtani,Asma A. Ibrahim,Haneen A. Alhomoud,Sherif Mohamed,Pedro J. Moreno*

Main category: cs.CL

TL;DR: This paper introduces an automated multi-agent framework for generating long-context questions in English and Arabic to improve vision-language models' understanding of extensive documents, addressing data scarcity issues.


<details>
  <summary>Details</summary>
Motivation: To enhance the long-context understanding capabilities of vision-language models, especially for low-resource languages like Arabic, by generating high-quality training data without costly human annotation.

Method: An automated multi-agent interactive system that generates extensive single- and multi-page questions across diverse domains in English and Arabic.

Result: The generated questions, encapsulated in the AraEngLongBench, challenge existing LVLMs, demonstrating the effectiveness of the data in fostering improved long-document comprehension.

Conclusion: The proposed framework provides a scalable, efficient way to produce challenging long-context questions, aiding the development of more capable LVLMs.

Abstract: Document Understanding (DU) in long-contextual scenarios with complex layouts
remains a significant challenge in vision-language research. Although Large
Vision-Language Models (LVLMs) excel at short-context DU tasks, their
performance declines in long-context settings. A key limitation is the scarcity
of fine-grained training data, particularly for low-resource languages such as
Arabic. Existing state-of-the-art techniques rely heavily on human annotation,
which is costly and inefficient. We propose a fully automated, multi-agent
interactive framework to generate long-context questions efficiently. Our
approach efficiently generates high-quality single- and multi-page questions
for extensive English and Arabic documents, covering hundreds of pages across
diverse domains. This facilitates the development of LVLMs with enhanced
long-context understanding ability. Experimental results in this work have
shown that our generated English and Arabic questions
(\textbf{AraEngLongBench}) are quite challenging to major open- and
close-source LVLMs. The code and data proposed in this work can be found in
https://github.com/wangk0b/Multi_Agentic_QA_Long_Doc.git. Sample Question and
Answer (QA) pairs and structured system prompts can be found in the Appendix.

</details>


### [37] [Goal Alignment in LLM-Based User Simulators for Conversational AI](https://arxiv.org/abs/2507.20152)
*Shuhaib Mehri,Xiaocheng Yang,Takyoung Kim,Gokhan Tur,Shikib Mehri,Dilek Hakkani-T√ºr*

Main category: cs.CL

TL;DR: The paper introduces UGST, a framework for improving user simulators in conversational AI by tracking user goal progression to ensure goal-aligned interactions, demonstrating significant improvements on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current LLM-based user simulators struggle with maintaining goal-oriented behavior over multiple turns, limiting their reliability.

Method: The authors propose UGST, a framework with a three-stage development process and new metrics for evaluating goal alignment.

Result: Their approach significantly enhances simulator performance on MultiWOZ 2.4 and 4-T Benchmark.

Conclusion: UGST is an essential framework that addresses a major gap in creating reliable goal-oriented user simulators.

Abstract: User simulators are essential to conversational AI, enabling scalable agent
development and evaluation through simulated interactions. While current Large
Language Models (LLMs) have advanced user simulation capabilities, we reveal
that they struggle to consistently demonstrate goal-oriented behavior across
multi-turn conversations--a critical limitation that compromises their
reliability in downstream applications. We introduce User Goal State Tracking
(UGST), a novel framework that tracks user goal progression throughout
conversations. Leveraging UGST, we present a three-stage methodology for
developing user simulators that can autonomously track goal progression and
reason to generate goal-aligned responses. Moreover, we establish comprehensive
evaluation metrics for measuring goal alignment in user simulators, and
demonstrate that our approach yields substantial improvements across two
benchmarks (MultiWOZ 2.4 and {\tau}-Bench). Our contributions address a
critical gap in conversational AI and establish UGST as an essential framework
for developing goal-aligned user simulators.

</details>


### [38] [SGPO: Self-Generated Preference Optimization based on Self-Improver](https://arxiv.org/abs/2507.20181)
*Hyeonji Lee,Daejin Jo,Seohwan Yun,Sungwoong Kim*

Main category: cs.CL

TL;DR: SGPO leverages on-policy self-improvement to align large language models with human preferences, surpassing traditional methods without external data.


<details>
  <summary>Details</summary>
Motivation: To develop an effective alignment method for LLMs that overcomes limitations of off-policy and data-dependent approaches.

Method: Self-Generated Preference Optimization based on a self-improving mechanism where the model refines responses to generate preference data for on-policy training.

Result: SGPO significantly outperforms DPO and other baseline self-improvement methods on benchmarks without external preference datasets.

Conclusion: The proposed SGPO framework offers a promising alternative for aligning LLMs, emphasizing on-policy self-improvement over reliance on external annotations.

Abstract: Large language models (LLMs), despite their extensive pretraining on diverse
datasets, require effective alignment to human preferences for practical and
reliable deployment. Conventional alignment methods typically employ off-policy
learning and depend on human-annotated datasets, which limits their broad
applicability and introduces distribution shift issues during training. To
address these challenges, we propose Self-Generated Preference Optimization
based on Self-Improver (SGPO), an innovative alignment framework that leverages
an on-policy self-improving mechanism. Specifically, the improver refines
responses from a policy model to self-generate preference data for direct
preference optimization (DPO) of the policy model. Here, the improver and
policy are unified into a single model, and in order to generate higher-quality
preference data, this self-improver learns to make incremental yet discernible
improvements to the current responses by referencing supervised fine-tuning
outputs. Experimental results on AlpacaEval 2.0 and Arena-Hard show that the
proposed SGPO significantly improves performance over DPO and baseline
self-improving methods without using external preference data.

</details>


### [39] [SessionIntentBench: A Multi-task Inter-session Intention-shift Modeling Benchmark for E-commerce Customer Behavior Understanding](https://arxiv.org/abs/2507.20185)
*Yuqi Yang,Weiqi Wang,Baixuan Xu,Wei Fan,Qing Zong,Chunkit Chan,Zheye Deng,Xin Liu,Yifan Gao,Changlong Yu,Chen Luo,Yang Li,Zheng Li,Qingyu Yin,Bing Yin,Yangqiu Song*

Main category: cs.CL

TL;DR: The paper introduces SessionIntentBench, a dataset for understanding customer intentions in e-commerce sessions, revealing current models' limitations and the benefits of intention injection.


<details>
  <summary>Details</summary>
Motivation: To improve modeling of customer intentions in e-commerce sessions and address the lack of suitable datasets and benchmarks.

Method: Constructed a multimodal dataset with intention trees, annotated data, and defined four subtasks for evaluation, along with human annotations for ground truth.

Result: Current LLMs struggle to grasp intentions across complex sessions, but injecting intention data improves their performance.

Conclusion: Enhancing LLMs with intention information can significantly improve understanding in e-commerce session analysis.

Abstract: Session history is a common way of recording user interacting behaviors
throughout a browsing activity with multiple products. For example, if an user
clicks a product webpage and then leaves, it might because there are certain
features that don't satisfy the user, which serve as an important indicator of
on-the-spot user preferences. However, all prior works fail to capture and
model customer intention effectively because insufficient information
exploitation and only apparent information like descriptions and titles are
used. There is also a lack of data and corresponding benchmark for explicitly
modeling intention in E-commerce product purchase sessions. To address these
issues, we introduce the concept of an intention tree and propose a dataset
curation pipeline. Together, we construct a sibling multimodal benchmark,
SessionIntentBench, that evaluates L(V)LMs' capability on understanding
inter-session intention shift with four subtasks. With 1,952,177 intention
entries, 1,132,145 session intention trajectories, and 13,003,664 available
tasks mined using 10,905 sessions, we provide a scalable way to exploit the
existing session data for customer intention understanding. We conduct human
annotations to collect ground-truth label for a subset of collected data to
form an evaluation gold set. Extensive experiments on the annotated data
further confirm that current L(V)LMs fail to capture and utilize the intention
across the complex session setting. Further analysis show injecting intention
enhances LLMs' performances.

</details>


### [40] [Diversity-Enhanced Reasoning for Subjective Questions](https://arxiv.org/abs/2507.20187)
*Yumeng Wang,Zhiyuan Fan,Jiayu Liu,Yi R. Fung*

Main category: cs.CL

TL;DR: MultiRole-R1 enhances reasoning diversity in large reasoning models, improving accuracy and diversity in subjective and objective tasks.


<details>
  <summary>Details</summary>
Motivation: Limitations in subjective reasoning due to homogeneous perspectives in current LRMs.

Method: Unsupervised data construction for diverse role perspectives and reinforcement learning with diversity rewards.

Result: Improved reasoning diversity and accuracy across benchmarks, demonstrating effectiveness and generalizability.

Conclusion: Diversity-enhanced training promotes better reasoning performance in LRMs, especially for subjective questions.

Abstract: Large reasoning models (LRM) with long chain-of-thought (CoT) capabilities
have shown strong performance on objective tasks, such as math reasoning and
coding. However, their effectiveness on subjective questions that may have
different responses from different perspectives is still limited by a tendency
towards homogeneous reasoning, introduced by the reliance on a single ground
truth in supervised fine-tuning and verifiable reward in reinforcement
learning. Motivated by the finding that increasing role perspectives
consistently improves performance, we propose MultiRole-R1, a
diversity-enhanced framework with multiple role perspectives, to improve the
accuracy and diversity in subjective reasoning tasks. MultiRole-R1 features an
unsupervised data construction pipeline that generates reasoning chains that
incorporate diverse role perspectives. We further employ reinforcement learning
via Group Relative Policy Optimization (GRPO) with reward shaping, by taking
diversity as a reward signal in addition to the verifiable reward. With
specially designed reward functions, we successfully promote perspective
diversity and lexical diversity, uncovering a positive relation between
reasoning diversity and accuracy. Our experiment on six benchmarks demonstrates
MultiRole-R1's effectiveness and generalizability in enhancing both subjective
and objective reasoning, showcasing the potential of diversity-enhanced
training in LRMs.

</details>


### [41] [IQ Test for LLMs: An Evaluation Framework for Uncovering Core Skills in LLMs](https://arxiv.org/abs/2507.20208)
*Aviya Maimon,Amir DN Cohen,Gal Vishne,Shauli Ravfogel,Reut Tsarfaty*

Main category: cs.CL

TL;DR: Proposes a factor analysis approach to understand underlying skills in LLM evaluation benchmarks, revealing latent skills and reducing redundancy.


<details>
  <summary>Details</summary>
Motivation: Current LLM evaluations rely on scores that don't clarify the relationships or overlaps between tasks, limiting understanding of model capabilities.

Method: Uses factor analysis on performance data from 44 tasks across 60 LLMs to identify latent skills impacting performance.

Result: Identifies a small number of latent skills that explain most performance variance, enabling better task redundancy detection, model selection, and profiling.

Conclusion: This new evaluation paradigm enhances understanding of LLM capabilities and streamlines assessment by revealing core skills.

Abstract: Current evaluations of large language models (LLMs) rely on benchmark scores,
but it is difficult to interpret what these individual scores reveal about a
model's overall skills. Specifically, as a community we lack understanding of
how tasks relate to one another, what they measure in common, how they differ,
or which ones are redundant. As a result, models are often assessed via a
single score averaged across benchmarks, an approach that fails to capture the
models' wholistic strengths and limitations. Here, we propose a new evaluation
paradigm that uses factor analysis to identify latent skills driving
performance across benchmarks. We apply this method to a comprehensive new
leaderboard showcasing the performance of 60 LLMs on 44 tasks, and identify a
small set of latent skills that largely explain performance. Finally, we turn
these insights into practical tools that identify redundant tasks, aid in model
selection, and profile models along each latent skill.

</details>


### [42] [Co-NAML-LSTUR: A Combined Model with Attentive Multi-View Learning and Long- and Short-term User Representations for News Recommendation](https://arxiv.org/abs/2507.20210)
*Minh Hoang Nguyen,Thuat Thien Nguyen,Minh Nhat Ta*

Main category: cs.CL

TL;DR: A hybrid news recommendation framework, Co-NAML-LSTUR, combines multi-view news modeling and dual-scale user interests to improve personalized news delivery, outperforming state-of-the-art baselines.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of effectively modeling multi-view news features and dynamic user interests spanning different time scales in news recommendation systems.

Method: Integrates NAML for multi-view news modeling, LSTUR for capturing both long- and short-term user interests, and uses BERT embeddings for semantic enhancement.

Result: Achieves substantial improvements over existing baselines on MIND-small and MIND-large datasets, demonstrating the effectiveness of their combined approach.

Conclusion: Co-NAML-LSTUR effectively combines multi-view news representations with dual-scale user modeling, significantly advancing personalized news recommendation performance.

Abstract: News recommendation systems play a vital role in mitigating information
overload by delivering personalized news content. A central challenge is to
effectively model both multi-view news representations and the dynamic nature
of user interests, which often span both short- and long-term preferences.
Existing methods typically rely on single-view features of news articles (e.g.,
titles or categories) or fail to comprehensively capture user preferences
across time scales. In this work, we propose Co-NAML-LSTUR, a hybrid news
recommendation framework that integrates NAML for attentive multi-view news
modeling and LSTUR for capturing both long- and short-term user
representations. Our model also incorporates BERT-based word embeddings to
enhance semantic feature extraction. We evaluate Co-NAML-LSTUR on two widely
used benchmarks, MIND-small and MIND-large. Experimental results show that
Co-NAML-LSTUR achieves substantial improvements over most state-of-the-art
baselines on MIND-small and MIND-large, respectively. These results demonstrate
the effectiveness of combining multi-view news representations with dual-scale
user modeling. The implementation of our model is publicly available at
https://github.com/MinhNguyenDS/Co-NAML-LSTUR.

</details>


### [43] [Reframe Your Life Story: Interactive Narrative Therapist and Innovative Moment Assessment with Large Language Models](https://arxiv.org/abs/2507.20241)
*Yi Feng,Jiaqi Wang,Wenxuan Zhang,Zhuang Chen,Yutong Shen,Xiyao Xiao,Minlie Huang,Liping Jing,Jian Yu*

Main category: cs.CL

TL;DR: The paper presents a framework combining an interactive narrative therapist (INT) and an evaluation method (IMA) to enhance mental health support through realistic, progressive narrative therapy simulation.


<details>
  <summary>Details</summary>
Motivation: Current LLM-based approaches lack realism in therapy simulation and fail to capture therapeutic progression, limiting mental health support.

Method: The framework integrates INT for simulating expert therapy sessions and IMA for evaluating therapy effectiveness via innovative moments.

Result: INT outperforms standard models in therapeutic quality across simulated and human trials, and effectively supports social applications.

Conclusion: The proposed framework improves realism and progress tracking in narrative therapy, advancing AI-assisted mental health support.

Abstract: Recent progress in large language models (LLMs) has opened new possibilities
for mental health support, yet current approaches lack realism in simulating
specialized psychotherapy and fail to capture therapeutic progression over
time. Narrative therapy, which helps individuals transform problematic life
stories into empowering alternatives, remains underutilized due to limited
access and social stigma. We address these limitations through a comprehensive
framework with two core components. First, INT (Interactive Narrative
Therapist) simulates expert narrative therapists by planning therapeutic
stages, guiding reflection levels, and generating contextually appropriate
expert-like responses. Second, IMA (Innovative Moment Assessment) provides a
therapy-centric evaluation method that quantifies effectiveness by tracking
"Innovative Moments" (IMs), critical narrative shifts in client speech
signaling therapy progress. Experimental results on 260 simulated clients and
230 human participants reveal that INT consistently outperforms standard LLMs
in therapeutic quality and depth. We further demonstrate the effectiveness of
INT in synthesizing high-quality support conversations to facilitate social
applications.

</details>


### [44] [Modeling Professionalism in Expert Questioning through Linguistic Differentiation](https://arxiv.org/abs/2507.20249)
*Giulia D'Agostino,Chung-Chi Chen*

Main category: cs.CL

TL;DR: This paper explores how linguistic features can model and evaluate professionalism in expert questions, using novel annotations and classifiers that outperform baselines.


<details>
  <summary>Details</summary>
Motivation: To address the underexplored aspect of professionalism in expert communication, especially in high-stakes domains like finance.

Method: Introducing a new annotation framework for linguistic features, constructing datasets with human and LLM questions, and training classifiers on interpretable features.

Result: Linguistic features correlate with professionalism and question origin; classifiers outperform baselines in distinguishing expert questions.

Conclusion: Professionalism is a learnable, domain-general construct that can be effectively modeled through linguistic analysis.

Abstract: Professionalism is a crucial yet underexplored dimension of expert
communication, particularly in high-stakes domains like finance. This paper
investigates how linguistic features can be leveraged to model and evaluate
professionalism in expert questioning. We introduce a novel annotation
framework to quantify structural and pragmatic elements in financial analyst
questions, such as discourse regulators, prefaces, and request types. Using
both human-authored and large language model (LLM)-generated questions, we
construct two datasets: one annotated for perceived professionalism and one
labeled by question origin. We show that the same linguistic features correlate
strongly with both human judgments and authorship origin, suggesting a shared
stylistic foundation. Furthermore, a classifier trained solely on these
interpretable features outperforms gemini-2.0 and SVM baselines in
distinguishing expert-authored questions. Our findings demonstrate that
professionalism is a learnable, domain-general construct that can be captured
through linguistically grounded modeling.

</details>


### [45] [Post-Completion Learning for Language Models](https://arxiv.org/abs/2507.20252)
*Xiang Fei,Siqi Wang,Shu Wei,Yuxiang Nie,Wei Shi,Hao Feng,Can Huang*

Main category: cs.CL

TL;DR: Post-Completion Learning (PCL) utilizes the post-sequence space in language model training to improve reasoning and self-evaluation without compromising efficiency.


<details>
  <summary>Details</summary>
Motivation: To leverage the post-completion space in language models for enhanced reasoning and self-assessment capabilities.

Method: Introducing PCL with a white-box reinforcement learning approach, dual-track supervised fine-tuning (SFT), and hybrid optimization combining RL.

Result: Consistent performance improvements across datasets and models compared to traditional training methods.

Conclusion: PCL offers a promising new training framework that boosts output quality while maintaining deployment efficiency.

Abstract: Current language model training paradigms typically terminate learning upon
reaching the end-of-sequence (<eos>}) token, overlooking the potential learning
opportunities in the post-completion space. We propose Post-Completion Learning
(PCL), a novel training framework that systematically utilizes the sequence
space after model output completion, to enhance both the reasoning and
self-evaluation abilities. PCL enables models to continue generating
self-assessments and reward predictions during training, while maintaining
efficient inference by stopping at the completion point.
  To fully utilize this post-completion space, we design a white-box
reinforcement learning method: let the model evaluate the output content
according to the reward rules, then calculate and align the score with the
reward functions for supervision. We implement dual-track SFT to optimize both
reasoning and evaluation capabilities, and mixed it with RL training to achieve
multi-objective hybrid optimization.
  Experimental results on different datasets and models demonstrate consistent
improvements over traditional SFT and RL methods. Our method provides a new
technical path for language model training that enhances output quality while
preserving deployment efficiency.

</details>


### [46] [EMBRACE: Shaping Inclusive Opinion Representation by Aligning Implicit Conversations with Social Norms](https://arxiv.org/abs/2507.20264)
*Abeer Aldayel,Areej Alokaili*

Main category: cs.CL

TL;DR: The paper introduces an evaluation framework for NLP models that accounts for implicit opinions expressed in conversations to promote inclusive and aligned representations.


<details>
  <summary>Details</summary>
Motivation: Existing methods often rely on overt demographic cues, missing nuanced, implicit opinions that shape fair and inclusive model outputs.

Method: The framework models the stance of responses as a proxy for opinions and assesses alignment using positive-unlabeled learning and instruction-tuned models.

Result: The approach reveals how implicit opinions are represented and misrepresented, guiding towards more inclusive model behavior.

Conclusion: Accounting for implicit opinions is crucial for developing fair, reflective NLP models, and the proposed framework offers a pathway for improvement.

Abstract: Shaping inclusive representations that embrace diversity and ensure fair
participation and reflections of values is at the core of many
conversation-based models. However, many existing methods rely on surface
inclusion using mention of user demographics or behavioral attributes of social
groups. Such methods overlook the nuanced, implicit expression of opinion
embedded in conversations. Furthermore, the over-reliance on overt cues can
exacerbate misalignment and reinforce harmful or stereotypical representations
in model outputs. Thus, we took a step back and recognized that equitable
inclusion needs to account for the implicit expression of opinion and use the
stance of responses to validate the normative alignment. This study aims to
evaluate how opinions are represented in NLP or computational models by
introducing an alignment evaluation framework that foregrounds implicit, often
overlooked conversations and evaluates the normative social views and
discourse. Our approach models the stance of responses as a proxy for the
underlying opinion, enabling a considerate and reflective representation of
diverse social viewpoints. We evaluate the framework using both (i)
positive-unlabeled (PU) online learning with base classifiers, and (ii)
instruction-tuned language models to assess post-training alignment. Through
this, we provide a lens on how implicit opinions are (mis)represented and offer
a pathway toward more inclusive model behavior.

</details>


### [47] [MoL-RL: Distilling Multi-Step Environmental Feedback into LLMs for Feedback-Independent Reasoning](https://arxiv.org/abs/2507.20278)
*Kang Yang,Jingxue Chen,Qingkun Tang,Tianxiang Zhang,Qianchun Lu*

Main category: cs.CL

TL;DR: MoL-RL is a training paradigm for LLMs that effectively integrates multi-step environmental feedback into reasoning, achieving state-of-the-art performance without external feedback loops.


<details>
  <summary>Details</summary>
Motivation: To improve the utilization of sequential environmental feedback signals in chain-of-thought reasoning for LLMs.

Method: Combining Mixture-of-Losses continual training with GRPO-based post-training to incorporate feedback signals while preserving language capabilities.

Result: Achieved state-of-the-art results on reasoning and code generation benchmarks with models like Qwen3-8B and strong generalization across scales.

Conclusion: MoL-RL offers a promising approach for leveraging textual feedback to enhance LLM reasoning in diverse domains.

Abstract: Large language models (LLMs) face significant challenges in effectively
leveraging sequential environmental feedback (EF) signals, such as natural
language evaluations, for feedback-independent chain-of-thought (CoT)
reasoning. Existing approaches either convert EF into scalar rewards, losing
rich contextual information, or employ refinement datasets, failing to exploit
the multi-step and discrete nature of EF interactions. To address these
limitations, we propose MoL-RL, a novel training paradigm that integrates
multi-step EF signals into LLMs through a dual-objective optimization
framework. Our method combines MoL (Mixture-of-Losses) continual training,
which decouples domain-specific EF signals (optimized via cross-entropy loss)
and general language capabilities (preserved via Kullback-Leibler divergence),
with GRPO-based post-training to distill sequential EF interactions into
single-step inferences. This synergy enables robust feedback-independent
reasoning without relying on external feedback loops. Experimental results on
mathematical reasoning (MATH-500, AIME24/AIME25) and code generation
(CodeAgent-Test) benchmarks demonstrate that MoL-RL achieves state-of-the-art
performance with the Qwen3-8B model, while maintaining strong generalization
across model scales (Qwen3-4B). This work provides a promising approach for
leveraging multi-step textual feedback to enhance LLMs' reasoning capabilities
in diverse domains.

</details>


### [48] [What Language(s) Does Aya-23 Think In? How Multilinguality Affects Internal Language Representations](https://arxiv.org/abs/2507.20279)
*Katharina Trinley,Toshiki Nakai,Tatiana Anikina,Tanja Baeumel*

Main category: cs.CL

TL;DR: Aya-23-8B, a multilingual decoder-only LLM, shows distinct internal language representations during translation and code-mixing, influenced by typological relationships, unlike monolingual models.


<details>
  <summary>Details</summary>
Motivation: To understand how multilingual training influences LLM internal processing, particularly for code-mixed and translation tasks.

Method: Analyzed Aya-23-8B using logit lens and neuron specialization, comparing it with monolingual models like Llama 3 and Chinese-LLaMA-2, focusing on internal neuron activation and language representation patterns.

Result: Aya-23 activates typologically related languages differently during translation, with code-mixed neurons influenced more by base language and concentrated in later layers. Script similarity and typological links affect internal processing across models.

Conclusion: Multilingual training shapes internal representations significantly, influencing how LLMs process mixed-language inputs, which informs future cross-lingual research.

Abstract: Large language models (LLMs) excel at multilingual tasks, yet their internal
language processing remains poorly understood. We analyze how Aya-23-8B, a
decoder-only LLM trained on balanced multilingual data, handles code-mixed,
cloze, and translation tasks compared to predominantly monolingual models like
Llama 3 and Chinese-LLaMA-2. Using logit lens and neuron specialization
analyses, we find: (1) Aya-23 activates typologically related language
representations during translation, unlike English-centric models that rely on
a single pivot language; (2) code-mixed neuron activation patterns vary with
mixing rates and are shaped more by the base language than the mixed-in one;
and (3) Aya-23's languagespecific neurons for code-mixed inputs concentrate in
final layers, diverging from prior findings on decoder-only models. Neuron
overlap analysis further shows that script similarity and typological relations
impact processing across model types. These findings reveal how multilingual
training shapes LLM internals and inform future cross-lingual transfer
research.

</details>


### [49] [Advancing Dialectal Arabic to Modern Standard Arabic Machine Translation](https://arxiv.org/abs/2507.20301)
*Abdullah Alabdullah,Lifeng Han,Chenghua Lin*

Main category: cs.CL

TL;DR: The paper advances dialectal Arabic to Modern Standard Arabic translation using prompting techniques and resource-efficient fine-tuning, demonstrating significant improvements even in low-resource settings.


<details>
  <summary>Details</summary>
Motivation: To address the linguistic divide between dialectal Arabic and Modern Standard Arabic, which limits digital accessibility and progress in NLP applications.

Method: Evaluation of various prompting strategies across large language models, and development of a resource-efficient fine-tuning pipeline with quantization and joint multi-dialect training.

Result: Few-shot prompting outperforms other prompting methods; GPT-4o performs best among prompts; a quantized Gemma2-9B model outperforms zero-shot GPT-4o; multi-dialect models outperform single-dialect models, with quantization reducing memory usage substantially.

Conclusion: High-quality dialectal to standard Arabic translation is achievable with limited resources, promoting more inclusive NLP solutions.

Abstract: Dialectal Arabic (DA) poses a persistent challenge for natural language
processing (NLP), as most everyday communication in the Arab world occurs in
dialects that diverge significantly from Modern Standard Arabic (MSA). This
linguistic divide limits access to digital services and educational resources
and impedes progress in Arabic machine translation. This paper presents two
core contributions to advancing DA-MSA translation for the Levantine, Egyptian,
and Gulf dialects, particularly in low-resource and computationally constrained
settings: a comprehensive evaluation of training-free prompting techniques, and
the development of a resource-efficient fine-tuning pipeline. Our evaluation of
prompting strategies across six large language models (LLMs) found that
few-shot prompting consistently outperformed zero-shot, chain-of-thought, and
our proposed Ara-TEaR method. GPT-4o achieved the highest performance across
all prompting settings. For fine-tuning, a quantized Gemma2-9B model achieved a
CHrF++ score of 49.88, outperforming zero-shot GPT-4o (44.58). Joint
multi-dialect trained models outperformed single-dialect counterparts by over
10% CHrF++, and 4-bit quantization reduced memory usage by 60% with less than
1% performance loss. The results and insights of our experiments offer a
practical blueprint for improving dialectal inclusion in Arabic NLP, showing
that high-quality DA-MSA machine translation is achievable even with limited
resources and paving the way for more inclusive language technologies.

</details>


### [50] [DYNARTmo: A Dynamic Articulatory Model for Visualization of Speech Movement Patterns](https://arxiv.org/abs/2507.20343)
*Bernd J. Kr√∂ger*

Main category: cs.CL

TL;DR: DYNARTmo is a 2D articulatory model that visualizes speech production, based on UK-DYNAMO, supporting phonetics education and speech therapy.


<details>
  <summary>Details</summary>
Motivation: To develop a visual and interactive model of speech articulation for educational and therapeutic purposes.

Method: Built upon the UK-DYNAMO framework, incorporating articulatory underspecification, segmental and gestural control, and coarticulation, and implemented in a web-based application.

Result: A functional web tool that visualizes key articulators with multiple views, suitable for phonetics and speech therapy, focusing on static modeling.

Conclusion: The model successfully visualizes speech articulatory configurations; future work will include dynamic movement and acoustic integration.

Abstract: We present DYNARTmo, a dynamic articulatory model designed to visualize
speech articulation processes in a two-dimensional midsagittal plane. The model
builds upon the UK-DYNAMO framework and integrates principles of articulatory
underspecification, segmental and gestural control, and coarticulation.
DYNARTmo simulates six key articulators based on ten continuous and six
discrete control parameters, allowing for the generation of both vocalic and
consonantal articulatory configurations. The current implementation is embedded
in a web-based application (SpeechArticulationTrainer) that includes sagittal,
glottal, and palatal views, making it suitable for use in phonetics education
and speech therapy. While this paper focuses on the static modeling aspects,
future work will address dynamic movement generation and integration with
articulatory-acoustic modules.

</details>


### [51] [RMTBench: Benchmarking LLMs Through Multi-Turn User-Centric Role-Playing](https://arxiv.org/abs/2507.20352)
*Hao Xiang,Tianyi Tang,Yang Su,Bowen Yu,An Yang,Fei Huang,Yichang Zhang,Yaojie Lu,Hongyu Lin,Xianpei Han,Jingren Zhou,Junyang Lin,Le Sun*

Main category: cs.CL

TL;DR: RMTBench is a new user-centric, bilingual role-playing benchmark for evaluating Large Language Models, emphasizing realistic user motivations and complex dialogues to better reflect real-world applications.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve the evaluation of LLMs' role-playing capabilities by addressing the limitations of current character-centric benchmarks, which oversimplify interactions.

Method: The paper introduces RMTBench, a comprehensive benchmark with diverse characters and dialogue constructed based on explicit user motivations, using an authentic multi-turn dialogue simulation mechanism and LLM-based scoring.

Result: RMTBench provides a more realistic and effective framework for evaluating LLMs' role-playing, bridging the gap between academic benchmarks and practical deployment.

Conclusion: RMTBench advances role-playing assessment by focusing on user intentions and complex interactions, making it a valuable tool for real-world LLM applications.

Abstract: Recent advancements in Large Language Models (LLMs) have shown outstanding
potential for role-playing applications. Evaluating these capabilities is
becoming crucial yet remains challenging. Existing benchmarks mostly adopt a
\textbf{character-centric} approach, simplify user-character interactions to
isolated Q&A tasks, and fail to reflect real-world applications. To address
this limitation, we introduce RMTBench, a comprehensive \textbf{user-centric}
bilingual role-playing benchmark featuring 80 diverse characters and over 8,000
dialogue rounds. RMTBench includes custom characters with detailed backgrounds
and abstract characters defined by simple traits, enabling evaluation across
various user scenarios. Our benchmark constructs dialogues based on explicit
user motivations rather than character descriptions, ensuring alignment with
practical user applications. Furthermore, we construct an authentic multi-turn
dialogue simulation mechanism. With carefully selected evaluation dimensions
and LLM-based scoring, this mechanism captures the complex intention of
conversations between the user and the character. By shifting focus from
character background to user intention fulfillment, RMTBench bridges the gap
between academic evaluation and practical deployment requirements, offering a
more effective framework for assessing role-playing capabilities in LLMs. All
code and datasets will be released soon.

</details>


### [52] [Length Representations in Large Language Models](https://arxiv.org/abs/2507.20398)
*Sangjun Moon,Dasom Choi,Jingun Kwon,Hidetaka Kamigaito,Manabu Okumura*

Main category: cs.CL

TL;DR: The study uncovers how large language models internally encode and control output length, primarily through multi-head attention and specific hidden units, enabling length management without compromising semantic quality.


<details>
  <summary>Details</summary>
Motivation: Understanding how LLMs control output length internally is crucial for improving their interpretability and reliability in instruction-based tasks.

Method: The researchers empirically analyze the internal representations of LLMs, focusing on attention mechanisms and hidden unit scaling to determine their roles in length control.

Result: Multi-head attention mechanisms are key in length encoding, and manipulating certain hidden units allows control of output length while maintaining text informativeness; length-related units become more active with length-specific prompts.

Conclusion: LLMs possess robust internal mechanisms for output length control that are disentangled from semantic content, indicating a learned and adaptable internal awareness of sequence length.

Abstract: Large language models (LLMs) have shown remarkable capabilities across
various tasks, that are learned from massive amounts of text-based data.
Although LLMs can control output sequence length, particularly in
instruction-based settings, the internal mechanisms behind this control have
been unexplored yet. In this study, we provide empirical evidence on how output
sequence length information is encoded within the internal representations in
LLMs. In particular, our findings show that multi-head attention mechanisms are
critical in determining output sequence length, which can be adjusted in a
disentangled manner. By scaling specific hidden units within the model, we can
control the output sequence length without losing the informativeness of the
generated text, thereby indicating that length information is partially
disentangled from semantic information. Moreover, some hidden units become
increasingly active as prompts become more length-specific, thus reflecting the
model's internal awareness of this attribute. Our findings suggest that LLMs
have learned robust and adaptable internal mechanisms for controlling output
length without any external control.

</details>


### [53] [Cognitive Chain-of-Thought: Structured Multimodal Reasoning about Social Situations](https://arxiv.org/abs/2507.20409)
*Eunkyu Park,Wesley Hanwen Deng,Gunhee Kim,Motahhare Eslami,Maarten Sap*

Main category: cs.CL

TL;DR: CoCoT enhances Vision-Language Models by introducing cognitively inspired reasoning stages, improving performance and interpretability in social context tasks.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of flat CoT prompting in complex social-contextual visual tasks.

Method: The paper proposes Cognitive Chain-of-Thought (CoCoT) prompting with three reasoning stages: perception, situation, and norm, tested on various benchmarks.

Result: CoCoT outperforms traditional CoT and direct prompting by about 8%, enhancing interpretability and social awareness.

Conclusion: Cognitively grounded reasoning stages significantly improve multimodal model capabilities in social contexts, leading to safer, more reliable AI.

Abstract: Chain-of-Thought (CoT) prompting helps models think step by step. But what
happens when they must see, understand, and judge-all at once? In visual tasks
grounded in social context, where bridging perception with norm-grounded
judgments is essential, flat CoT often breaks down. We introduce Cognitive
Chain-of-Thought (CoCoT), a prompting strategy that scaffolds VLM reasoning
through three cognitively inspired stages: perception, situation, and norm. Our
experiments show that, across multiple multimodal benchmarks (including intent
disambiguation, commonsense reasoning, and safety), CoCoT consistently
outperforms CoT and direct prompting (+8\% on average). Our findings
demonstrate that cognitively grounded reasoning stages enhance interpretability
and social awareness in VLMs, paving the way for safer and more reliable
multimodal systems.

</details>


### [54] [CONCAP: Seeing Beyond English with Concepts Retrieval-Augmented Captioning](https://arxiv.org/abs/2507.20411)
*George Ibrahim,Rita Ramos,Yova Kementchedjhieva*

Main category: cs.CL

TL;DR: CONCAP improves multilingual image captioning by integrating retrieved captions with image concepts, reducing data needs and enhancing performance especially in low-resource languages.


<details>
  <summary>Details</summary>
Motivation: To address the lag of multilingual vision-language models behind English counterparts due to limited data and high costs, leveraging retrieval-augmented generation with best context grounding.

Method: Introducing CONCAP, combining retrieved captions with image-specific concepts to improve captioning across languages.

Result: CONCAP achieves strong performance on low- and mid-resource languages in the XM3600 dataset, with reduced data requirements.

Conclusion: Concept-aware retrieval augmentation effectively bridges performance gaps in multilingual image captioning.

Abstract: Multilingual vision-language models have made significant strides in image
captioning, yet they still lag behind their English counterparts due to limited
multilingual training data and costly large-scale model parameterization.
Retrieval-augmented generation (RAG) offers a promising alternative by
conditioning caption generation on retrieved examples in the target language,
reducing the need for extensive multilingual training. However, multilingual
RAG captioning models often depend on retrieved captions translated from
English, which can introduce mismatches and linguistic biases relative to the
source language. We introduce CONCAP, a multilingual image captioning model
that integrates retrieved captions with image-specific concepts, enhancing the
contextualization of the input image and grounding the captioning process
across different languages. Experiments on the XM3600 dataset indicate that
CONCAP enables strong performance on low- and mid-resource languages, with
highly reduced data requirements. Our findings highlight the effectiveness of
concept-aware retrieval augmentation in bridging multilingual performance gaps.

</details>


### [55] [Survey of NLU Benchmarks Diagnosing Linguistic Phenomena: Why not Standardize Diagnostics Benchmarks?](https://arxiv.org/abs/2507.20419)
*Khloud AL Jallad,Nada Ghneim,Ghaida Rebdawi*

Main category: cs.CL

TL;DR: The paper reviews NLU benchmarks focusing on diagnostics datasets and linguistic phenomena, highlighting gaps such as lack of standardization and advocating for evaluation metrics.


<details>
  <summary>Details</summary>
Motivation: To analyze existing NLU benchmarks and improve their evaluation methods by establishing standards.

Method: Survey and comparative analysis of NLU benchmarks and diagnostics datasets, with proposing a research question about evaluation metrics.

Result: Identified gaps in standardization of linguistic phenomena and evaluation metrics; suggested creating a hierarchy of linguistic phenomena for better evaluation.

Conclusion: Development of evaluation metrics for diagnostics could improve insights and consistency in NLU model assessment.

Abstract: Natural Language Understanding (NLU) is a basic task in Natural Language
Processing (NLP). The evaluation of NLU capabilities has become a trending
research topic that attracts researchers in the last few years, resulting in
the development of numerous benchmarks. These benchmarks include various tasks
and datasets in order to evaluate the results of pretrained models via public
leaderboards. Notably, several benchmarks contain diagnostics datasets designed
for investigation and fine-grained error analysis across a wide range of
linguistic phenomena. This survey provides a comprehensive review of available
English, Arabic, and Multilingual NLU benchmarks, with a particular emphasis on
their diagnostics datasets and the linguistic phenomena they covered. We
present a detailed comparison and analysis of these benchmarks, highlighting
their strengths and limitations in evaluating NLU tasks and providing in-depth
error analysis. When highlighting the gaps in the state-of-the-art, we noted
that there is no naming convention for macro and micro categories or even a
standard set of linguistic phenomena that should be covered. Consequently, we
formulated a research question regarding the evaluation metrics of the
evaluation diagnostics benchmarks: "Why do not we have an evaluation standard
for the NLU evaluation diagnostics benchmarks?" similar to ISO standard in
industry. We conducted a deep analysis and comparisons of the covered
linguistic phenomena in order to support experts in building a global hierarchy
for linguistic phenomena in future. We think that having evaluation metrics for
diagnostics evaluation could be valuable to gain more insights when comparing
the results of the studied models on different diagnostics benchmarks.

</details>


### [56] [CodeNER: Code Prompting for Named Entity Recognition](https://arxiv.org/abs/2507.20423)
*Sungwoo Han,Hyeyeon Kim,Jingun Kwon,Hidetaka Kamigaito,Manabu Okumura*

Main category: cs.CL

TL;DR: The paper introduces a code-based prompting method for better NER performance using large language models, outperforming traditional text prompts across multiple languages, especially when combined with chain-of-thought prompting.


<details>
  <summary>Details</summary>
Motivation: To improve LLM-based NER by addressing the limitations of input context-only methods, leveraging detailed instructions through code-based prompts.

Method: Embedding detailed BIO labeling instructions in code prompts to enhance LLM understanding, combined with chain-of-thought prompting for further performance gains.

Result: The proposed method significantly outperforms conventional approaches on ten benchmarks across multiple languages, with additional gains when combined with chain-of-thought prompting.

Conclusion: Explicitly structuring NER instructions via code-based prompts enhances LLM effectiveness, offering a promising direction for multilingual NER tasks.

Abstract: Recent studies have explored various approaches for treating candidate named
entity spans as both source and target sequences in named entity recognition
(NER) by leveraging large language models (LLMs). Although previous approaches
have successfully generated candidate named entity spans with suitable labels,
they rely solely on input context information when using LLMs, particularly,
ChatGPT. However, NER inherently requires capturing detailed labeling
requirements with input context information. To address this issue, we propose
a novel method that leverages code-based prompting to improve the capabilities
of LLMs in understanding and performing NER. By embedding code within prompts,
we provide detailed BIO schema instructions for labeling, thereby exploiting
the ability of LLMs to comprehend long-range scopes in programming languages.
Experimental results demonstrate that the proposed code-based prompting method
outperforms conventional text-based prompting on ten benchmarks across English,
Arabic, Finnish, Danish, and German datasets, indicating the effectiveness of
explicitly structuring NER instructions. We also verify that combining the
proposed code-based prompting method with the chain-of-thought prompting
further improves performance.

</details>


### [57] [Speaking in Words, Thinking in Logic: A Dual-Process Framework in QA Systems](https://arxiv.org/abs/2507.20491)
*Tuan Bui,Trong Le,Phat Thai,Sang Nguyen,Minh Hua,Ngan Pham,Thang Bui,Tho Quan*

Main category: cs.CL

TL;DR: Text-JEPA offers an efficient, lightweight framework for converting natural language into first-order logic, enhancing explainability and reasoning in closed-domain QA systems.


<details>
  <summary>Details</summary>
Motivation: Address the need for transparent, explainable reasoning in closed-domain question-answering beyond large, inefficient models.

Method: Introduce Text-JEPA, a cognitive-inspired architecture that combines efficient natural language to logic translation with a symbolic reasoner, and develop a comprehensive evaluation framework.

Result: Achieves competitive performance with lower computational costs, demonstrating the effectiveness of structured reasoning for specialized domain QA systems.

Conclusion: Structured, interpretable reasoning frameworks like Text-JEPA can effectively enable explainable and efficient QA in closed domains.

Abstract: Recent advances in large language models (LLMs) have significantly enhanced
question-answering (QA) capabilities, particularly in open-domain contexts.
However, in closed-domain scenarios such as education, healthcare, and law,
users demand not only accurate answers but also transparent reasoning and
explainable decision-making processes. While neural-symbolic (NeSy) frameworks
have emerged as a promising solution, leveraging LLMs for natural language
understanding and symbolic systems for formal reasoning, existing approaches
often rely on large-scale models and exhibit inefficiencies in translating
natural language into formal logic representations.
  To address these limitations, we introduce Text-JEPA (Text-based
Joint-Embedding Predictive Architecture), a lightweight yet effective framework
for converting natural language into first-order logic (NL2FOL). Drawing
inspiration from dual-system cognitive theory, Text-JEPA emulates System 1 by
efficiently generating logic representations, while the Z3 solver operates as
System 2, enabling robust logical inference. To rigorously evaluate the
NL2FOL-to-reasoning pipeline, we propose a comprehensive evaluation framework
comprising three custom metrics: conversion score, reasoning score, and
Spearman rho score, which collectively capture the quality of logical
translation and its downstream impact on reasoning accuracy.
  Empirical results on domain-specific datasets demonstrate that Text-JEPA
achieves competitive performance with significantly lower computational
overhead compared to larger LLM-based systems. Our findings highlight the
potential of structured, interpretable reasoning frameworks for building
efficient and explainable QA systems in specialized domains.

</details>


### [58] [AQUA: A Large Language Model for Aquaculture & Fisheries](https://arxiv.org/abs/2507.20520)
*Praneeth Narisetty,Uday Kumar Reddy Kattamanchi,Lohit Akshant Nimma,Sri Ram Kaushik Karnati,Shiva Nagendra Babu Kore,Mounika Golamari,Tejashree Nageshreddy*

Main category: cs.CL

TL;DR: Introduction of AQUA, a large language model tailored for aquaculture, aiming to improve industry practices through synthetic data generation and expert knowledge integration.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing machine learning methods in handling the domain-specific complexities of aquaculture.

Method: Development of AQUA and AQUADAPT, a framework combining expert knowledge, language models, and evaluation techniques to generate high-quality synthetic data.

Result: AQUA provides a foundation for LLM-driven innovations in aquaculture research and decision-making.

Conclusion: AQUA and AQUADAPT set the stage for advanced AI applications in aquaculture to support industry growth and sustainability.

Abstract: Aquaculture plays a vital role in global food security and coastal economies
by providing sustainable protein sources. As the industry expands to meet
rising demand, it faces growing challenges such as disease outbreaks,
inefficient feeding practices, rising labor costs, logistical inefficiencies,
and critical hatchery issues, including high mortality rates and poor water
quality control. Although artificial intelligence has made significant
progress, existing machine learning methods fall short of addressing the
domain-specific complexities of aquaculture. To bridge this gap, we introduce
AQUA, the first large language model (LLM) tailored for aquaculture, designed
to support farmers, researchers, and industry practitioners. Central to this
effort is AQUADAPT (Data Acquisition, Processing and Tuning), an Agentic
Framework for generating and refining high-quality synthetic data using a
combination of expert knowledge, largescale language models, and automated
evaluation techniques. Our work lays the foundation for LLM-driven innovations
in aquaculture research, advisory systems, and decision-making tools.

</details>


### [59] [SAND-Math: Using LLMs to Generate Novel, Difficult and Useful Mathematics Questions and Answers](https://arxiv.org/abs/2507.20527)
*Chaitanya Manem,Pratik Prabhanjan Brahma,Prakamya Mishra,Zicheng Liu,Emad Barsoum*

Main category: cs.CL

TL;DR: SAND-Math pipeline generates and elevates complex math problems to improve LLM performance, boosting accuracy significantly.


<details>
  <summary>Details</summary>
Motivation: Address the scarcity of difficult and novel math training data for large language models (LLMs).

Method: Generate problems from scratch and systematically increase their difficulty via a Difficulty Hiking step.

Result: Significant performance improvement on the AIME25 benchmark; the Difficulty Hiking step effectively raises problem difficulty and LLM performance.

Conclusion: The pipeline and dataset provide a practical, scalable toolkit for developing more capable mathematical reasoning LLMs.

Abstract: The demand for Large Language Models (LLMs) capable of sophisticated
mathematical reasoning is growing across industries. However, the development
of performant mathematical LLMs is critically bottlenecked by the scarcity of
difficult, novel training data. We introduce \textbf{SAND-Math} (Synthetic
Augmented Novel and Difficult Mathematics problems and solutions), a pipeline
that addresses this by first generating high-quality problems from scratch and
then systematically elevating their complexity via a new \textbf{Difficulty
Hiking} step. We demonstrate the effectiveness of our approach through two key
findings. First, augmenting a strong baseline with SAND-Math data significantly
boosts performance, outperforming the next-best synthetic dataset by
\textbf{$\uparrow$ 17.85 absolute points} on the AIME25 benchmark. Second, in a
dedicated ablation study, we show our Difficulty Hiking process is highly
effective: by increasing average problem difficulty from 5.02 to 5.98, this
step lifts AIME25 performance from 46.38\% to 49.23\%. The full generation
pipeline, final dataset, and a fine-tuned model form a practical and scalable
toolkit for building more capable and efficient mathematical reasoning LLMs.
SAND-Math dataset is released here:
\href{https://huggingface.co/datasets/amd/SAND-MATH}{https://huggingface.co/datasets/amd/SAND-MATH}

</details>


### [60] [Dialogues of Dissent: Thematic and Rhetorical Dimensions of Hate and Counter-Hate Speech in Social Media Conversations](https://arxiv.org/abs/2507.20528)
*Effi Levi,Gal Ron,Odelia Oshri,Shaul R. Shenhav*

Main category: cs.CL

TL;DR: A new multi-label scheme categorizes hate and counter-hate speech on social media by thematic and rhetorical aspects, revealing interaction patterns and insights into online hate dynamics.


<details>
  <summary>Details</summary>
Motivation: To better understand and analyze hate and counter-hate speech on social media.

Method: Developed a multi-labeled annotation scheme and analyzed 720 tweets from 92 conversations, examining thematic and rhetorical dimensions.

Result: Identified interaction patterns and strategies in hate and counter-hate speech, with insights into their dissemination and impact.

Conclusion: The scheme helps understand how hate and counter messages are communicated and their influence on online behavior.

Abstract: We introduce a novel multi-labeled scheme for joint annotation of hate and
counter-hate speech in social media conversations, categorizing hate and
counter-hate messages into thematic and rhetorical dimensions. The thematic
categories outline different discursive aspects of each type of speech, while
the rhetorical dimension captures how hate and counter messages are
communicated, drawing on Aristotle's Logos, Ethos and Pathos. We annotate a
sample of 92 conversations, consisting of 720 tweets, and conduct statistical
analyses, incorporating public metrics, to explore patterns of interaction
between the thematic and rhetorical dimensions within and between hate and
counter-hate speech. Our findings provide insights into the spread of hate
messages on social media, the strategies used to counter them, and their
potential impact on online behavior.

</details>


### [61] [Enhancing Hallucination Detection via Future Context](https://arxiv.org/abs/2507.20546)
*Joosung Lee,Cheonbok Park,Hwiyeol Jo,Jeonghoon Kim,Joonsuk Park,Kang Min Yoo*

Main category: cs.CL

TL;DR: This paper presents a framework for detecting hallucinations in black-box LLM outputs by sampling future contexts, enhancing detection across various methods.


<details>
  <summary>Details</summary>
Motivation: Detecting hallucinations in LLM outputs is crucial as users encounter black-box-generated text without transparency.

Method: Sampling future contexts and integrating them with existing sampling-based detection methods.

Result: The approach significantly improves hallucination detection performance across multiple methods.

Conclusion: Sampling future contexts is an effective strategy for improving hallucination detection in black-box language models.

Abstract: Large Language Models (LLMs) are widely used to generate plausible text on
online platforms, without revealing the generation process. As users
increasingly encounter such black-box outputs, detecting hallucinations has
become a critical challenge. To address this challenge, we focus on developing
a hallucination detection framework for black-box generators. Motivated by the
observation that hallucinations, once introduced, tend to persist, we sample
future contexts. The sampled future contexts provide valuable clues for
hallucination detection and can be effectively integrated with various
sampling-based methods. We extensively demonstrate performance improvements
across multiple methods using our proposed sampling approach.

</details>


### [62] [ZSE-Cap: A Zero-Shot Ensemble for Image Retrieval and Prompt-Guided Captioning](https://arxiv.org/abs/2507.20564)
*Duc-Tai Dinh,Duc Anh Khoa Dinh*

Main category: cs.CL

TL;DR: ZSE-Cap is a zero-shot, ensemble-based system for image captioning and retrieval that leverages foundation models without fine-tuning.


<details>
  <summary>Details</summary>
Motivation: To develop a competitive image captioning and retrieval system without relying on extensive task-specific training.

Method: Ensembling similarity scores from multiple models for retrieval and using engineered prompts to guide the Gemma 3 captioning model.

Result: Achieved top-4 performance in the EVENTA challenge with a score of 0.42002, validating the effectiveness of ensembling and prompting techniques.

Conclusion: Combining foundation models and prompt engineering in a zero-shot setting can yield competitive results in image analysis tasks.

Abstract: We present ZSE-Cap (Zero-Shot Ensemble for Captioning), our 4th place system
in Event-Enriched Image Analysis (EVENTA) shared task on article-grounded image
retrieval and captioning. Our zero-shot approach requires no finetuning on the
competition's data. For retrieval, we ensemble similarity scores from CLIP,
SigLIP, and DINOv2. For captioning, we leverage a carefully engineered prompt
to guide the Gemma 3 model, enabling it to link high-level events from the
article to the visual content in the image. Our system achieved a final score
of 0.42002, securing a top-4 position on the private test set, demonstrating
the effectiveness of combining foundation models through ensembling and
prompting. Our code is available at https://github.com/ductai05/ZSE-Cap.

</details>


### [63] [Before the Outrage: Challenges and Advances in Predicting Online Antisocial Behavior](https://arxiv.org/abs/2507.20614)
*Ana√Øs Ollagnier*

Main category: cs.CL

TL;DR: The paper systematically reviews over 49 studies on antisocial behavior prediction on social media, proposing a taxonomy of five task types and analyzing modeling techniques, dataset challenges, and future research directions.


<details>
  <summary>Details</summary>
Motivation: To address the fragmented state of research on predicting antisocial behaviors on social media and provide a unified framework.

Method: A systematic review of existing studies, categorizing tasks, techniques, and dataset issues.

Result: The review introduces a structured taxonomy, identifies methodological challenges, and suggests future research avenues such as multilingual modeling and human-in-the-loop systems.

Conclusion: Organizing the field around a coherent framework can guide more effective and responsible antisocial behavior prediction.

Abstract: Antisocial behavior (ASB) on social media-including hate speech, harassment,
and trolling-poses growing challenges for platform safety and societal
wellbeing. While prior work has primarily focused on detecting harmful content
after it appears, predictive approaches aim to forecast future harmful
behaviors-such as hate speech propagation, conversation derailment, or user
recidivism-before they fully unfold. Despite increasing interest, the field
remains fragmented, lacking a unified taxonomy or clear synthesis of existing
methods. This paper presents a systematic review of over 49 studies on ASB
prediction, offering a structured taxonomy of five core task types: early harm
detection, harm emergence prediction, harm propagation prediction, behavioral
risk prediction, and proactive moderation support. We analyze how these tasks
differ by temporal framing, prediction granularity, and operational goals. In
addition, we examine trends in modeling techniques-from classical machine
learning to pre-trained language models-and assess the influence of dataset
characteristics on task feasibility and generalization. Our review highlights
methodological challenges, such as dataset scarcity, temporal drift, and
limited benchmarks, while outlining emerging research directions including
multilingual modeling, cross-platform generalization, and human-in-the-loop
systems. By organizing the field around a coherent framework, this survey aims
to guide future work toward more robust and socially responsible ASB
prediction.

</details>


### [64] [Ontology-Enhanced Knowledge Graph Completion using Large Language Models](https://arxiv.org/abs/2507.20643)
*Wenbin Guo,Xin Wang,Jiaoyan Chen,Zhao Li,Zirui Chen*

Main category: cs.CL

TL;DR: A novel ontology-enhanced knowledge graph completion method using LLMs, integrating structural and ontological knowledge, outperforms existing methods.


<details>
  <summary>Details</summary>
Motivation: Enhance LLM-based KGC by addressing implicit knowledge representation issues and improving reasoning capabilities.

Method: Embedding structural information via neural perceptual mechanisms, extracting ontological knowledge, and transforming it into textual format for LLM processing.

Result: OL-KGC significantly outperforms existing methods across multiple benchmarks, achieving state-of-the-art results.

Conclusion: Integrating neural-perceptual structures with ontological knowledge via LLMs improves KGC performance and reasoning.

Abstract: Large Language Models (LLMs) have been extensively adopted in Knowledge Graph
Completion (KGC), showcasing significant research advancements. However, as
black-box models driven by deep neural architectures, current LLM-based KGC
methods rely on implicit knowledge representation with parallel propagation of
erroneous knowledge, thereby hindering their ability to produce conclusive and
decisive reasoning outcomes. We aim to integrate neural-perceptual structural
information with ontological knowledge, leveraging the powerful capabilities of
LLMs to achieve a deeper understanding of the intrinsic logic of the knowledge.
We propose an ontology enhanced KGC method using LLMs -- OL-KGC. It first
leverages neural perceptual mechanisms to effectively embed structural
information into the textual space, and then uses an automated extraction
algorithm to retrieve ontological knowledge from the knowledge graphs (KGs)
that needs to be completed, which is further transformed into a textual format
comprehensible to LLMs for providing logic guidance. We conducted extensive
experiments on three widely-used benchmarks -- FB15K-237, UMLS and WN18RR. The
experimental results demonstrate that OL-KGC significantly outperforms existing
mainstream KGC methods across multiple evaluation metrics, achieving
state-of-the-art performance.

</details>


### [65] [Geometric-Mean Policy Optimization](https://arxiv.org/abs/2507.20673)
*Yuzhong Zhao,Yue Liu,Junpeng Liu,Jingye Chen,Xun Wu,Yaru Hao,Tengchao Lv,Shaohan Huang,Lei Cui,Qixiang Ye,Fang Wan,Furu Wei*

Main category: cs.CL

TL;DR: GMPO improves upon GRPO by using the geometric mean of token rewards, leading to more stable policy updates and better performance on mathematical and reasoning benchmarks.


<details>
  <summary>Details</summary>
Motivation: Enhance the stability and performance of language model optimization by addressing the instability caused by outlier importance weights in GRPO.

Method: Replace the arithmetic mean with the geometric mean of token rewards in the optimization process, providing stability against outliers.

Result: GMPO outperforms GRPO with a 4.1% and 1.4% improvement on specified benchmarks, demonstrating increased stability and effectiveness.

Conclusion: GMPO offers a more stable and effective policy optimization method for language models, especially in mathematical reasoning tasks.

Abstract: Recent advancements, such as Group Relative Policy Optimization (GRPO), have
enhanced the reasoning capabilities of large language models by optimizing the
arithmetic mean of token-level rewards. However, GRPO suffers from unstable
policy updates when processing tokens with outlier importance-weighted rewards,
which manifests as extreme importance sampling ratios during training, i.e.,
the ratio between the sampling probabilities assigned to a token by the current
and old policies. In this work, we propose Geometric-Mean Policy Optimization
(GMPO), a stabilized variant of GRPO. Instead of optimizing the arithmetic
mean, GMPO maximizes the geometric mean of token-level rewards, which is
inherently less sensitive to outliers and maintains a more stable range of
importance sampling ratio. In addition, we provide comprehensive theoretical
and experimental analysis to justify the design and stability benefits of GMPO.
Beyond improved stability, GMPO-7B outperforms GRPO by an average of 4.1% on
multiple mathematical benchmarks and 1.4% on multimodal reasoning benchmark,
including AIME24, AMC, MATH500, OlympiadBench, Minerva, and Geometry3K. Code is
available at https://github.com/callsys/GMPO.

</details>


### [66] [When Scale Meets Diversity: Evaluating Language Models on Fine-Grained Multilingual Claim Verification](https://arxiv.org/abs/2507.20700)
*Hanna Shcharbakova,Tatiana Anikina,Natalia Skachkova,Josef van Genabith*

Main category: cs.CL

TL;DR: Smaller models like XLM-R outperform large language models in multilingual fact verification, highlighting the importance of specialized models for nuanced tasks.


<details>
  <summary>Details</summary>
Motivation: The need for effective automated fact verification systems across multiple languages amid the spread of misinformation.

Method: Comprehensive evaluation of five language models on the X-Fact dataset spanning 25 languages and seven veracity categories, comparing prompting and fine-tuning strategies.

Result: XLM-R significantly outperforms large models like Llama 3.1, with a macro-F1 of 57.7% versus 16.9%, setting new benchmarks. Large models exhibit biases and challenges in evidence use.

Conclusion: Smaller, specialized models are more effective than large general-purpose models for nuanced, multilingual fact verification tasks.

Abstract: The rapid spread of multilingual misinformation requires robust automated
fact verification systems capable of handling fine-grained veracity assessments
across diverse languages. While large language models have shown remarkable
capabilities across many NLP tasks, their effectiveness for multilingual claim
verification with nuanced classification schemes remains understudied. We
conduct a comprehensive evaluation of five state-of-the-art language models on
the X-Fact dataset, which spans 25 languages with seven distinct veracity
categories. Our experiments compare small language models (encoder-based XLM-R
and mT5) with recent decoder-only LLMs (Llama 3.1, Qwen 2.5, Mistral Nemo)
using both prompting and fine-tuning approaches. Surprisingly, we find that
XLM-R (270M parameters) substantially outperforms all tested LLMs (7-12B
parameters), achieving 57.7% macro-F1 compared to the best LLM performance of
16.9%. This represents a 15.8% improvement over the previous state-of-the-art
(41.9%), establishing new performance benchmarks for multilingual fact
verification. Our analysis reveals problematic patterns in LLM behavior,
including systematic difficulties in leveraging evidence and pronounced biases
toward frequent categories in imbalanced data settings. These findings suggest
that for fine-grained multilingual fact verification, smaller specialized
models may be more effective than general-purpose large models, with important
implications for practical deployment of fact-checking systems.

</details>


### [67] [Text2VLM: Adapting Text-Only Datasets to Evaluate Alignment Training in Visual Language Models](https://arxiv.org/abs/2507.20704)
*Gabriel Downer,Sean Craven,Damian Ruck,Jake Thomas*

Main category: cs.CL

TL;DR: Text2VLM is a pipeline that transforms text datasets into multimodal formats for evaluating VLM vulnerabilities to prompt injection attacks, revealing current model weaknesses and aiding in safety improvements.


<details>
  <summary>Details</summary>
Motivation: The growing use of VLMs necessitates robust evaluation methods, especially for multimodal content, as existing datasets mainly focus on text-only prompts.

Method: The pipeline converts text into images with harmful content, creating multimodal prompts to test VLMs, and uses human evaluations to validate effectiveness.

Result: Open-source VLMs show increased vulnerability to prompt injections with visual inputs; models lag behind proprietary counterparts.

Conclusion: Text2VLM is a scalable tool that enhances vulnerability assessment, contributing to safer deployment of VLMs.

Abstract: The increasing integration of Visual Language Models (VLMs) into AI systems
necessitates robust model alignment, especially when handling multimodal
content that combines text and images. Existing evaluation datasets heavily
lean towards text-only prompts, leaving visual vulnerabilities under evaluated.
To address this gap, we propose \textbf{Text2VLM}, a novel multi-stage pipeline
that adapts text-only datasets into multimodal formats, specifically designed
to evaluate the resilience of VLMs against typographic prompt injection
attacks. The Text2VLM pipeline identifies harmful content in the original text
and converts it into a typographic image, creating a multimodal prompt for
VLMs. Also, our evaluation of open-source VLMs highlights their increased
susceptibility to prompt injection when visual inputs are introduced, revealing
critical weaknesses in the current models' alignment. This is in addition to a
significant performance gap compared to closed-source frontier models. We
validate Text2VLM through human evaluations, ensuring the alignment of
extracted salient concepts; text summarization and output classification align
with human expectations. Text2VLM provides a scalable tool for comprehensive
safety assessment, contributing to the development of more robust safety
mechanisms for VLMs. By enhancing the evaluation of multimodal vulnerabilities,
Text2VLM plays a role in advancing the safe deployment of VLMs in diverse,
real-world applications.

</details>


### [68] [Investigating Structural Pruning and Recovery Techniques for Compressing Multimodal Large Language Models: An Empirical Study](https://arxiv.org/abs/2507.20749)
*Yiran Huang,Lukas Thede,Massimiliano Mancini,Wenjia Xu,Zeynep Akata*

Main category: cs.CL

TL;DR: This paper proposes a method for compressing Multimodal Large Language Models (MLLMs) using structural pruning and efficient recovery training, achieving high performance retention with minimal data and resources.


<details>
  <summary>Details</summary>
Motivation: The high computational and memory demands of MLLMs hinder their practical deployment, necessitating effective compression techniques.

Method: The authors explore layerwise and widthwise structural pruning combined with supervised fine-tuning and knowledge distillation, and assess recovery training with limited data.

Result: Widthwise pruning outperforms in low-resource scenarios; effective recovery is possible with only 5% of training data, maintaining over 95% performance; fine-tuning only the multimodal projector is sufficient at high compression levels.

Conclusion: Structural pruning with efficient recovery training enables effective MLLM compression, reducing resource requirements while preserving model performance.

Abstract: While Multimodal Large Language Models (MLLMs) demonstrate impressive
capabilities, their substantial computational and memory requirements pose
significant barriers to practical deployment. Current parameter reduction
techniques primarily involve training MLLMs from Small Language Models (SLMs),
but these methods offer limited flexibility and remain computationally
intensive. To address this gap, we propose to directly compress existing MLLMs
through structural pruning combined with efficient recovery training.
Specifically, we investigate two structural pruning paradigms--layerwise and
widthwise pruning--applied to the language model backbone of MLLMs, alongside
supervised finetuning and knowledge distillation. Additionally, we assess the
feasibility of conducting recovery training with only a small fraction of the
available data. Our results show that widthwise pruning generally maintains
better performance in low-resource scenarios with limited computational
resources or insufficient finetuning data. As for the recovery training,
finetuning only the multimodal projector is sufficient at small compression
levels (< 20%). Furthermore, a combination of supervised finetuning and
hidden-state distillation yields optimal recovery across various pruning
levels. Notably, effective recovery can be achieved with as little as 5% of the
original training data, while retaining over 95% of the original performance.
Through empirical study on two representative MLLMs, i.e., LLaVA-v1.5-7B and
Bunny-v1.0-3B, this study offers actionable insights for practitioners aiming
to compress MLLMs effectively without extensive computation resources or
sufficient data.

</details>


### [69] [Multilingual Self-Taught Faithfulness Evaluators](https://arxiv.org/abs/2507.20752)
*Carlo Alfano,Aymen Al Marjani,Zeno Jonke,Amin Mantrach,Saab Mansour,Marcello Federico*

Main category: cs.CL

TL;DR: The paper introduces a multilingual faithfulness evaluation framework for LLMs that learns from synthetic data and employs cross-lingual transfer, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Address the need for accurate, multilingual evaluators for LLMs to handle information hallucination without extensive labeled data.

Method: Uses synthetic multilingual data and cross-lingual transfer learning to train self-taught evaluators, comparing language-specific and mixed-language fine-tuning.

Result: The framework improves faithfulness evaluation accuracy across languages compared to existing baselines, including state-of-the-art English evaluators.

Conclusion: The proposed approach effectively leverages multilingual training and cross-lingual transfer, enhancing the evaluation of LLMs in diverse languages.

Abstract: The growing use of large language models (LLMs) has increased the need for
automatic evaluation systems, particularly to address the challenge of
information hallucination. Although existing faithfulness evaluation approaches
have shown promise, they are predominantly English-focused and often require
expensive human-labeled training data for fine-tuning specialized models. As
LLMs see increased adoption in multilingual contexts, there is a need for
accurate faithfulness evaluators that can operate across languages without
extensive labeled data. This paper presents Self-Taught Evaluators for
Multilingual Faithfulness, a framework that learns exclusively from synthetic
multilingual summarization data while leveraging cross-lingual transfer
learning. Through experiments comparing language-specific and mixed-language
fine-tuning approaches, we demonstrate a consistent relationship between an
LLM's general language capabilities and its performance in language-specific
evaluation tasks. Our framework shows improvements over existing baselines,
including state-of-the-art English evaluators and machine translation-based
approaches.

</details>


### [70] [On The Role of Pretrained Language Models in General-Purpose Text Embeddings: A Survey](https://arxiv.org/abs/2507.20783)
*Meishan Zhang,Xin Zhang,Xinping Zhao,Shouzheng Huang,Baotian Hu,Min Zhang*

Main category: cs.CL

TL;DR: This survey reviews the development and roles of general-purpose text embeddings (GPTE) driven by pretrained language models (PLMs), covering architecture, advanced applications, and future research directions.


<details>
  <summary>Details</summary>
Motivation: To provide a comprehensive overview of GPTE in the era of PLMs, highlighting their development, applications, and potential future directions.

Method: A systematic review and analysis of GPTE architectures, roles of PLMs, and emerging research topics.

Result: The survey categorizes fundamental and advanced roles of PLMs in GPTE, and discusses future research directions including safety, bias mitigation, and cognitive extensions.

Conclusion: This review offers valuable insights for understanding current GPTE states and exploring future research avenues.

Abstract: Text embeddings have attracted growing interest due to their effectiveness
across a wide range of natural language processing (NLP) tasks, such as
retrieval, classification, clustering, bitext mining, and summarization. With
the emergence of pretrained language models (PLMs), general-purpose text
embeddings (GPTE) have gained significant traction for their ability to produce
rich, transferable representations. The general architecture of GPTE typically
leverages PLMs to derive dense text representations, which are then optimized
through contrastive learning on large-scale pairwise datasets. In this survey,
we provide a comprehensive overview of GPTE in the era of PLMs, focusing on the
roles PLMs play in driving its development. We first examine the fundamental
architecture and describe the basic roles of PLMs in GPTE, i.e., embedding
extraction, expressivity enhancement, training strategies, learning objectives,
and data construction. Then, we describe advanced roles enabled by PLMs, such
as multilingual support, multimodal integration, code understanding, and
scenario-specific adaptation. Finally, we highlight potential future research
directions that move beyond traditional improvement goals, including ranking
integration, safety considerations, bias mitigation, structural information
incorporation, and the cognitive extension of embeddings. This survey aims to
serve as a valuable reference for both newcomers and established researchers
seeking to understand the current state and future potential of GPTE.

</details>


### [71] [Automating Thematic Review of Prevention of Future Deaths Reports: Replicating the ONS Child Suicide Study using Large Language Models](https://arxiv.org/abs/2507.20786)
*Sam Osian,Arpan Dutta,Sahil Bhandari,Iain E. Buchan,Dan W. Joyce*

Main category: cs.CL

TL;DR: An automated language-model pipeline (PFD Toolkit) efficiently and reliably identified child-suicide death reports, nearly doubling the count compared to manual review, with high agreement with clinical adjudication and much faster processing.


<details>
  <summary>Details</summary>
Motivation: To improve the efficiency and reliability of analyzing Prevention of Future Deaths (PFD) reports related to child suicides, which previously required manual effort.

Method: Developed and validated an open-source, automated language-model pipeline (PFD Toolkit) that screens, codes, and analyzes coroners' PFD reports.

Result: Identified 72 child-suicide reports versus 37 manually identified by ONS; showed high agreement with clinicians (Cohen's Œ∫=0.82); significantly reduced analysis time from months to minutes.

Conclusion: Automated LLM analysis reliably and efficiently replicates manual reviews, enabling scalable, timely public health insights, with the toolkit available for future research.

Abstract: Prevention of Future Deaths (PFD) reports, issued by coroners in England and
Wales, flag systemic hazards that may lead to further loss of life. Analysis of
these reports has previously been constrained by the manual effort required to
identify and code relevant cases. In 2025, the Office for National Statistics
(ONS) published a national thematic review of child-suicide PFD reports ($\leq$
18 years), identifying 37 cases from January 2015 to November 2023 - a process
based entirely on manual curation and coding. We evaluated whether a fully
automated, open source "text-to-table" language-model pipeline (PFD Toolkit)
could reproduce the ONS's identification and thematic analysis of child-suicide
PFD reports, and assessed gains in efficiency and reliability. All 4,249 PFD
reports published from July 2013 to November 2023 were processed via PFD
Toolkit's large language model pipelines. Automated screening identified cases
where the coroner attributed death to suicide in individuals aged 18 or
younger, and eligible reports were coded for recipient category and 23 concern
sub-themes, replicating the ONS coding frame. PFD Toolkit identified 72
child-suicide PFD reports - almost twice the ONS count. Three blinded
clinicians adjudicated a stratified sample of 144 reports to validate the
child-suicide screening. Against the post-consensus clinical annotations, the
LLM-based workflow showed substantial to almost-perfect agreement (Cohen's
$\kappa$ = 0.82, 95% CI: 0.66-0.98, raw agreement = 91%). The end-to-end script
runtime was 8m 16s, transforming a process that previously took months into one
that can be completed in minutes. This demonstrates that automated LLM analysis
can reliably and efficiently replicate manual thematic reviews of coronial
data, enabling scalable, reproducible, and timely insights for public health
and safety. The PFD Toolkit is openly available for future research.

</details>


### [72] [Latent Inter-User Difference Modeling for LLM Personalization](https://arxiv.org/abs/2507.20849)
*Yilun Qiu,Tianhao Shi,Xiaoyan Zhao,Fengbin Zhu,Yang Zhang,Fuli Feng*

Main category: cs.CL

TL;DR: DEP is a framework that improves personalized language model outputs by modeling inter-user differences in the latent space rather than using prompts.


<details>
  <summary>Details</summary>
Motivation: Address the limitations of existing personalization methods that rely on user history and language prompts.

Method: Constructs user embeddings contrasted with peer embeddings, filters them with a sparse autoencoder, and injects the distilled features into a frozen LLM.

Result: DEP outperforms baseline methods in personalized review generation tasks.

Conclusion: Modeling inter-user differences in latent space enhances personalization and performance in language models.

Abstract: Large language models (LLMs) are increasingly integrated into users' daily
lives, leading to a growing demand for personalized outputs. Previous work
focuses on leveraging a user's own history, overlooking inter-user differences
that are crucial for effective personalization. While recent work has attempted
to model such differences, the reliance on language-based prompts often hampers
the effective extraction of meaningful distinctions. To address these issues,
we propose Difference-aware Embedding-based Personalization (DEP), a framework
that models inter-user differences in the latent space instead of relying on
language prompts. DEP constructs soft prompts by contrasting a user's embedding
with those of peers who engaged with similar content, highlighting relative
behavioral signals. A sparse autoencoder then filters and compresses both
user-specific and difference-aware embeddings, preserving only task-relevant
features before injecting them into a frozen LLM. Experiments on personalized
review generation show that DEP consistently outperforms baseline methods
across multiple metrics. Our code is available at
https://github.com/SnowCharmQ/DEP.

</details>


### [73] [A survey of diversity quantification in natural language processing: The why, what, where and how](https://arxiv.org/abs/2507.20858)
*Louis Est√®ve,Marie-Catherine de Marneffe,Nurit Melnik,Agata Savary,Olha Kanishcheva*

Main category: cs.CL

TL;DR: The paper surveys the use of diversity in NLP over six years, identifying inconsistent approaches and proposing a unified taxonomy based on ecology and economy theories to improve understanding and comparability.


<details>
  <summary>Details</summary>
Motivation: Enhance the understanding and consistency of measuring diversity in NLP to improve the field's development.

Method: Review of articles from the ACL Anthology, analysis of diversity measurement approaches, and proposing a taxonomy based on ecological and economic frameworks.

Result: Identified diverse and inconsistent measures of diversity, and proposed a unified framework with three dimensions: variety, balance, and disparity.

Conclusion: A systematic approach to measuring diversity can improve the formalization, understanding, and comparability of diversity in NLP.

Abstract: The concept of diversity has received increased consideration in Natural
Language Processing (NLP) in recent years. This is due to various motivations
like promoting and inclusion, approximating human linguistic behavior, and
increasing systems' performance. Diversity has however often been addressed in
an ad hoc manner in NLP, and with few explicit links to other domains where
this notion is better theorized. We survey articles in the ACL Anthology from
the past 6 years, with "diversity" or "diverse" in their title. We find a wide
range of settings in which diversity is quantified, often highly specialized
and using inconsistent terminology. We put forward a unified taxonomy of why,
what on, where, and how diversity is measured in NLP. Diversity measures are
cast upon a unified framework from ecology and economy (Stirling, 2007) with 3
dimensions of diversity: variety, balance and disparity. We discuss the trends
which emerge due to this systematized approach. We believe that this study
paves the way towards a better formalization of diversity in NLP, which should
bring a better understanding of this notion and a better comparability between
various approaches.

</details>


### [74] [Leveraging Open-Source Large Language Models for Clinical Information Extraction in Resource-Constrained Settings](https://arxiv.org/abs/2507.20859)
*Luc Builtjes,Joeran Bosma,Mathias Prokop,Bram van Ginneken,Alessa Hering*

Main category: cs.CL

TL;DR: Open-source LLMs show promise for clinical info extraction, with language and resource considerations.


<details>
  <summary>Details</summary>
Motivation: To evaluate open-source LLMs for clinical information extraction, addressing transparency and privacy issues of proprietary models.

Method: Assessment of nine open-source LLMs on Dutch clinical tasks using the 	exttt{llm	extunderscore extractinator} framework, in a zero-shot setting.

Result: Models like Phi-4-14B, Qwen-2.5-14B, and DeepSeek-R1-14B performed well; Llama-3.3-70B was slightly better but more costly; translation to English reduced performance.

Conclusion: Open-source LLMs are effective, scalable, and privacy-preserving for clinical info extraction, especially in low-resource settings.

Abstract: Medical reports contain rich clinical information but are often unstructured
and written in domain-specific language, posing challenges for information
extraction. While proprietary large language models (LLMs) have shown promise
in clinical natural language processing, their lack of transparency and data
privacy concerns limit their utility in healthcare. This study therefore
evaluates nine open-source generative LLMs on the DRAGON benchmark, which
includes 28 clinical information extraction tasks in Dutch. We developed
\texttt{llm\_extractinator}, a publicly available framework for information
extraction using open-source generative LLMs, and used it to assess model
performance in a zero-shot setting. Several 14 billion parameter models,
Phi-4-14B, Qwen-2.5-14B, and DeepSeek-R1-14B, achieved competitive results,
while the bigger Llama-3.3-70B model achieved slightly higher performance at
greater computational cost. Translation to English prior to inference
consistently degraded performance, highlighting the need of native-language
processing. These findings demonstrate that open-source LLMs, when used with
our framework, offer effective, scalable, and privacy-conscious solutions for
clinical information extraction in low-resource settings.

</details>


### [75] [Soft Injection of Task Embeddings Outperforms Prompt-Based In-Context Learning](https://arxiv.org/abs/2507.20906)
*Jungwon Park,Wonjong Rhee*

Main category: cs.CL

TL;DR: A new method called Soft Injection of task embeddings improves in-context learning by reducing prompt length, enhancing performance, and being efficient across multiple tasks and models.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of traditional in-context learning, such as effectiveness and efficiency when using multiple examples.

Method: Constructs task embeddings from few-shot prompts and softly mixes them with attention head activations using pre-optimized parameters, applied across various tasks and models.

Result: Outperforms 10-shot in-context learning by 10.1%-13.9% on average across 57 tasks and 12 large language models, and reveals insights into task-specific attention head roles.

Conclusion: The soft injection approach shifts task conditioning from prompt space to activation space, reduces prompt length, and improves task performance.

Abstract: In-Context Learning (ICL) enables Large Language Models (LLMs) to perform
tasks by conditioning on input-output examples in the prompt, without requiring
any update in model parameters. While widely adopted, it remains unclear
whether prompting with multiple examples is the most effective and efficient
way to convey task information. In this work, we propose Soft Injection of task
embeddings. The task embeddings are constructed only once using few-shot ICL
prompts and repeatedly used during inference. Soft injection is performed by
softly mixing task embeddings with attention head activations using
pre-optimized mixing parameters, referred to as soft head-selection parameters.
This method not only allows a desired task to be performed without in-prompt
demonstrations but also significantly outperforms existing ICL approaches while
reducing memory usage and compute cost at inference time. An extensive
evaluation is performed across 57 tasks and 12 LLMs, spanning four model
families of sizes from 4B to 70B. Averaged across 57 tasks, our method
outperforms 10-shot ICL by 10.1%-13.9% across 12 LLMs. Additional analyses show
that our method also serves as an insightful tool for analyzing task-relevant
roles of attention heads, revealing that task-relevant head positions selected
by our method transfer across similar tasks but not across dissimilar ones --
underscoring the task-specific nature of head functionality. Our soft injection
method opens a new paradigm for reducing prompt length and improving task
performance by shifting task conditioning from the prompt space to the
activation space.

</details>


### [76] [MediQAl: A French Medical Question Answering Dataset for Knowledge and Reasoning Evaluation](https://arxiv.org/abs/2507.20917)
*Adrien Bazoge*

Main category: cs.CL

TL;DR: MediQAl is a French medical question answering dataset for evaluating language models in factual recall and reasoning, with 32,603 questions across 41 subjects, enabling performance analysis of 14 models.


<details>
  <summary>Details</summary>
Motivation: To address the lack of multilingual medical datasets and evaluate language models' capabilities in medical answer reasoning.

Method: Creating and validating the MediQAl dataset with questions from French medical exams, and evaluating 14 language models across different question types.

Result: Identified significant performance gaps between factual recall and reasoning tasks in the models, highlighting areas for improvement.

Conclusion: MediQAl provides a comprehensive benchmark for assessing multilingual medical question answering models, emphasizing the need to improve reasoning abilities.

Abstract: This work introduces MediQAl, a French medical question answering dataset
designed to evaluate the capabilities of language models in factual medical
recall and reasoning over real-world clinical scenarios. MediQAl contains
32,603 questions sourced from French medical examinations across 41 medical
subjects. The dataset includes three tasks: (i) Multiple-Choice Question with
Unique answer, (ii) Multiple-Choice Question with Multiple answer, and (iii)
Open-Ended Question with Short-Answer. Each question is labeled as
Understanding or Reasoning, enabling a detailed analysis of models' cognitive
capabilities. We validate the MediQAl dataset through extensive evaluation with
14 large language models, including recent reasoning-augmented models, and
observe a significant performance gap between factual recall and reasoning
tasks. Our evaluation provides a comprehensive benchmark for assessing language
models' performance on French medical question answering, addressing a crucial
gap in multilingual resources for the medical domain.

</details>


### [77] [FHSTP@EXIST 2025 Benchmark: Sexism Detection with Transparent Speech Concept Bottleneck Models](https://arxiv.org/abs/2507.20924)
*Roberto Labadie-Tamayo,Adrian Jaques B√∂ck,Djordje Slijepƒçeviƒá,Xihui Chen,Andreas Babic,Matthias Zeppelzauer*

Main category: cs.CL

TL;DR: The paper discusses models developed for identifying and classifying sexism in social media posts, focusing on interpretability and leveraging metadata.


<details>
  <summary>Details</summary>
Motivation: Addressing the widespread issue of sexism on social media through automated detection and classification.

Method: Implementing three models: a concept bottleneck model (SCBM), an enhanced version with transformers (SCBMT), and fine-tuning XLM-RoBERTa, utilizing adjectives as interpretable concepts and incorporating metadata.

Result: Models achieved competitive rankings in the CLEF 2025 challenge with good interpretability; XLM-RoBERTa performed best in some tasks.

Conclusion: The approaches balance interpretability and performance, with additional metadata offering potential improvements.

Abstract: Sexism has become widespread on social media and in online conversation. To
help address this issue, the fifth Sexism Identification in Social Networks
(EXIST) challenge is initiated at CLEF 2025. Among this year's international
benchmarks, we concentrate on solving the first task aiming to identify and
classify sexism in social media textual posts. In this paper, we describe our
solutions and report results for three subtasks: Subtask 1.1 - Sexism
Identification in Tweets, Subtask 1.2 - Source Intention in Tweets, and Subtask
1.3 - Sexism Categorization in Tweets. We implement three models to address
each subtask which constitute three individual runs: Speech Concept Bottleneck
Model (SCBM), Speech Concept Bottleneck Model with Transformer (SCBMT), and a
fine-tuned XLM-RoBERTa transformer model. SCBM uses descriptive adjectives as
human-interpretable bottleneck concepts. SCBM leverages large language models
(LLMs) to encode input texts into a human-interpretable representation of
adjectives, then used to train a lightweight classifier for downstream tasks.
SCBMT extends SCBM by fusing adjective-based representation with contextual
embeddings from transformers to balance interpretability and classification
performance. Beyond competitive results, these two models offer fine-grained
explanations at both instance (local) and class (global) levels. We also
investigate how additional metadata, e.g., annotators' demographic profiles,
can be leveraged. For Subtask 1.1, XLM-RoBERTa, fine-tuned on provided data
augmented with prior datasets, ranks 6th for English and Spanish and 4th for
English in the Soft-Soft evaluation. Our SCBMT achieves 7th for English and
Spanish and 6th for Spanish.

</details>


### [78] [FRED: Financial Retrieval-Enhanced Detection and Editing of Hallucinations in Language Models](https://arxiv.org/abs/2507.20930)
*Likun Tan,Kuan-Wei Huang,Kevin Wu*

Main category: cs.CL

TL;DR: This paper presents a method for detecting and editing factual inaccuracies in large language models, especially in financial contexts, by fine-tuning models on synthetic datasets with error tagging.


<details>
  <summary>Details</summary>
Motivation: To address the critical issue of hallucinations in large language models affecting factual reliability.

Method: Constructing a synthetic dataset with tagged errors, fine-tuning multiple language models to detect and correct these errors.

Result: Fine-tuned Phi-4 achieved significant improvements in detection metrics; a smaller Phi-4-mini model maintained competitive performance with fewer parameters.

Conclusion: Their approach offers a practical solution for improving factual accuracy in large language models, with potential applications beyond finance.

Abstract: Hallucinations in large language models pose a critical challenge for
applications requiring factual reliability, particularly in high-stakes domains
such as finance. This work presents an effective approach for detecting and
editing factually incorrect content in model-generated responses based on the
provided context. Given a user-defined domain-specific error taxonomy, we
construct a synthetic dataset by inserting tagged errors into financial
question-answering corpora and then fine-tune four language models, Phi-4,
Phi-4-mini, Qwen3-4B, and Qwen3-14B, to detect and edit these factual
inaccuracies. Our best-performing model, fine-tuned Phi-4, achieves an 8%
improvement in binary F1 score and a 30% gain in overall detection performance
compared to OpenAI-o3. Notably, our fine-tuned Phi-4-mini model, despite having
only 4 billion parameters, maintains competitive performance with just a 2%
drop in binary detection and a 0.1% decline in overall detection compared to
OpenAI-o3. Our work provides a practical solution for detecting and editing
factual inconsistencies in financial text generation while introducing a
generalizable framework that can enhance the trustworthiness and alignment of
large language models across diverse applications beyond finance. Our code and
data are available at https://github.com/pegasi-ai/fine-grained-editting.

</details>


### [79] [Mind the Gap: Conformative Decoding to Improve Output Diversity of Instruction-Tuned Large Language Models](https://arxiv.org/abs/2507.20956)
*Max Peeperkorn,Tom Kouwenhoven,Dan Brown,Anna Jordanous*

Main category: cs.CL

TL;DR: Instruction-tuning reduces LLM output diversity, especially for creative tasks, creating a 'diversity gap.' A new decoding strategy, conformative decoding, helps recover diversity and maintain quality.


<details>
  <summary>Details</summary>
Motivation: To address the reduced output diversity caused by instruction-tuning in large language models, which impacts creative tasks.

Method: Analyzed diversity loss during fine-tuning stages and developed conformative decoding to enhance diversity using a more diverse base model.

Result: Identified significant diversity loss due to instruction-tuning; conformative decoding increases diversity and maintains or improves output quality.

Conclusion: Conformative decoding is an effective approach to mitigate diversity loss in instruction-tuned LLMs, particularly beneficial for creative tasks.

Abstract: Instruction-tuning large language models (LLMs) reduces the diversity of
their outputs, which has implications for many tasks, particularly for creative
tasks. This paper investigates the ``diversity gap'' for a writing prompt
narrative generation task. This gap emerges as measured by current diversity
metrics for various open-weight and open-source LLMs. The results show
significant decreases in diversity due to instruction-tuning. We explore the
diversity loss at each fine-tuning stage for the OLMo and OLMo 2 models to
further understand how output diversity is affected. The results indicate that
DPO has the most substantial impact on diversity. Motivated by these findings,
we present a new decoding strategy, conformative decoding, which guides an
instruct model using its more diverse base model to reintroduce output
diversity. We show that conformative decoding typically increases diversity and
even maintains or improves quality.

</details>


### [80] [Memorization in Fine-Tuned Large Language Models](https://arxiv.org/abs/2507.21009)
*Danil Savine,Muni Sreenivas Pydi,Jamal Atif,Olivier Capp√©*

Main category: cs.CL

TL;DR: This study explores how fine-tuning large language models in the medical domain can lead to memorization, impacting privacy and performance.


<details>
  <summary>Details</summary>
Motivation: To understand the factors influencing memorization in fine-tuned LLMs, especially given the privacy-sensitive nature of medical data.

Method: Using membership inference attacks, generation tasks, and analysis of different transformer components and LoRA ranks.

Result: Identified key matrices contributing to memorization, a correlation between perplexity and memorization, and effects of LoRA rank increases.

Conclusion: Findings highlight the trade-offs between model performance and privacy, informing future strategies for responsible model fine-tuning.

Abstract: This study investigates the mechanisms and factors influencing memorization
in fine-tuned large language models (LLMs), with a focus on the medical domain
due to its privacy-sensitive nature. We examine how different aspects of the
fine-tuning process affect a model's propensity to memorize training data,
using the PHEE dataset of pharmacovigilance events.
  Our research employs two main approaches: a membership inference attack to
detect memorized data, and a generation task with prompted prefixes to assess
verbatim reproduction. We analyze the impact of adapting different weight
matrices in the transformer architecture, the relationship between perplexity
and memorization, and the effect of increasing the rank in low-rank adaptation
(LoRA) fine-tuning.
  Key findings include: (1) Value and Output matrices contribute more
significantly to memorization compared to Query and Key matrices; (2) Lower
perplexity in the fine-tuned model correlates with increased memorization; (3)
Higher LoRA ranks lead to increased memorization, but with diminishing returns
at higher ranks.
  These results provide insights into the trade-offs between model performance
and privacy risks in fine-tuned LLMs. Our findings have implications for
developing more effective and responsible strategies for adapting large
language models while managing data privacy concerns.

</details>


### [81] [Multi-Agent-as-Judge: Aligning LLM-Agent-Based Automated Evaluation with Multi-Dimensional Human Evaluation](https://arxiv.org/abs/2507.21028)
*Jiaju Chen,Yuxuan Lu,Xiaojie Wang,Huimin Zeng,Jing Huang,Jiri Gesi,Ying Xu,Bingsheng Yao,Dakuo Wang*

Main category: cs.CL

TL;DR: The paper presents MAJ-EVAL, a multi-agent evaluation framework that creates diverse evaluator personas from texts and uses in-group debates for multi-dimensional assessments, aligning more closely with human judgments.


<details>
  <summary>Details</summary>
Motivation: To improve automated evaluation of NLP applications by simulating diverse human evaluators using LLMs, addressing limitations of arbitrary persona design and lack of generalizability.

Method: Developing MAJ-EVAL to automatically generate evaluator personas from relevant texts, instantiate LLM agents with these personas, and facilitate multi-agent debates to produce multi-dimensional feedback.

Result: MAJ-EVAL yields evaluation results that better match human expert ratings than traditional metrics and existing LLM-based evaluation methods in educational and medical domains.

Conclusion: MAJ-EVAL offers a promising approach to more human-aligned and generalizable automated NLP evaluation through multi-agent debates.

Abstract: Nearly all human work is collaborative; thus, the evaluation of real-world
NLP applications often requires multiple dimensions that align with diverse
human perspectives. As real human evaluator resources are often scarce and
costly, the emerging "LLM-as-a-judge" paradigm sheds light on a promising
approach to leverage LLM agents to believably simulate human evaluators. Yet,
to date, existing LLM-as-a-judge approaches face two limitations: persona
descriptions of agents are often arbitrarily designed, and the frameworks are
not generalizable to other tasks. To address these challenges, we propose
MAJ-EVAL, a Multi-Agent-as-Judge evaluation framework that can automatically
construct multiple evaluator personas with distinct dimensions from relevant
text documents (e.g., research papers), instantiate LLM agents with the
personas, and engage in-group debates with multi-agents to Generate
multi-dimensional feedback. Our evaluation experiments in both the educational
and medical domains demonstrate that MAJ-EVAL can generate evaluation results
that better align with human experts' ratings compared with conventional
automated evaluation metrics and existing LLM-as-a-judge methods.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [82] [CleANN: Efficient Full Dynamism in Graph-based Approximate Nearest Neighbor Search](https://arxiv.org/abs/2507.19802)
*Ziyu Zhang,Yuanhao Wei,Joshua Engels,Julian Shun*

Main category: cs.DB

TL;DR: CleANN is a novel dynamic graph-based ANNS index that maintains high query quality and efficiency in fully dynamic environments, addressing limitations of existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing graph-based ANNS indexes are mostly static and struggle with concurrent updates and queries, affecting their performance and utility in real-world applications.

Method: CleANN incorporates workload-aware linking, query-adaptive neighborhood consolidation, and semi-lazy memory cleaning to handle updates efficiently while maintaining index quality.

Result: CleANN outperforms existing methods, achieving comparable or better query quality with 7-1200x throughput improvements on large datasets, even under concurrent full dynamism.

Conclusion: CleANN is the first to combine efficiency and high quality in a fully dynamic, concurrent ANNS index, making it highly suitable for real-world vector database applications.

Abstract: Approximate nearest neighbor search (ANNS) has become a quintessential
algorithmic problem for various other foundational data tasks for AI workloads.
Graph-based ANNS indexes have superb empirical trade-offs in indexing cost,
query efficiency, and query approximation quality. Most existing graph-based
indexes are designed for the static scenario, where there are no updates to the
data after the index is constructed. However, full dynamism (insertions,
deletions, and searches) is crucial to providing up-to-date responses in
applications using vector databases. It is desirable that the index efficiently
supports updates and search queries concurrently. Existing dynamic graph-based
indexes suffer from at least one of the following problems: (1) the query
quality degrades as updates happen; and (2) the graph structure updates used to
maintain the index quality upon updates are global and thus expensive. To solve
these problems, we propose the CleANN system which consists of three main
components: (1) workload-aware linking of diverse search tree descendants to
combat distribution shift; (2)query-adaptive on-the-fly neighborhood
consolidation to efficiently handle deleted nodes; and (3) semi-lazy memory
cleaning to clean up stale information in the data structure and reduce the
work spent by the first two components. We evaluate CleANN on 7 diverse
datasets on fully dynamic workloads and find that CleANN has query quality at
least as good as if the index had been built statically using the corresponding
data. In the in-memory setting using 56 hyper-threads, with all types of
queries running concurrently, at the same recall level, CleANN achieves 7-1200x
throughput improvement on million-scale real-world datasets. To the best of our
knowledge, CleANN is the first concurrent ANNS index to achieve such efficiency
while maintaining quality under full dynamism.

</details>


### [83] [TIMEST: Temporal Information Motif Estimator Using Sampling Trees](https://arxiv.org/abs/2507.20441)
*Yunjie Pan,Omkar Bhalerao,C. Seshadhri,Nishil Talati*

Main category: cs.DB

TL;DR: TIMEST is a fast, accurate estimation algorithm for counting temporal motifs in large temporal networks, outperforming existing methods in speed and accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of motif counting in large, temporal graphs, which is computationally difficult due to the combinatorial explosion of possibilities.

Method: Introducing a temporal spanning tree sampler with weighted sampling and randomized estimation techniques to efficiently estimate motif counts.

Result: TIMEST significantly outperforms existing algorithms in both speed and accuracy, with theoretical guarantees and practical validation showing substantial improvements.

Conclusion: TIMEST provides a scalable and reliable solution for temporal motif counting, enabling analyses in large networks that were previously infeasible.

Abstract: The mining of pattern subgraphs, known as motifs, is a core task in the field
of graph mining. Edges in real-world networks often have timestamps, so there
is a need for temporal motif mining. A temporal motif is a richer structure
that imposes timing constraints on the edges of the motif. Temporal motifs have
been used to analyze social networks, financial transactions, and biological
networks.
  Motif counting in temporal graphs is particularly challenging. A graph with
millions of edges can have trillions of temporal motifs, since the same edge
can occur with multiple timestamps. There is a combinatorial explosion of
possibilities, and state-of-the-art algorithms cannot manage motifs with more
than four vertices.
  In this work, we present TIMEST: a general, fast, and accurate estimation
algorithm to count temporal motifs of arbitrary sizes in temporal networks. Our
approach introduces a temporal spanning tree sampler that leverages weighted
sampling to generate substructures of target temporal motifs. This method
carefully takes a subset of temporal constraints of the motif that can be
jointly and efficiently sampled. TIMEST uses randomized estimation techniques
to obtain accurate estimates of motif counts.
  We give theoretical guarantees on the running time and approximation
guarantees of TIMEST. We perform an extensive experimental evaluation and show
that TIMEST is both faster and more accurate than previous algorithms. Our CPU
implementation exhibits an average speedup of 28x over state-of-the-art GPU
implementation of the exact algorithm, and 6x speedup over SOTA approximate
algorithms while consistently showcasing less than 5% error in most cases. For
example, TIMEST can count the number of instances of a financial fraud temporal
motif in four minutes with 0.6% error, while exact methods take more than two
days.

</details>


### [84] [A Functional Data Model and Query Language is All You Need](https://arxiv.org/abs/2507.20671)
*Jens Dittrich*

Main category: cs.DB

TL;DR: Proposes a new functional data model (FDM) and functional query language (FQL) aiming to resolve many limitations of SQL, with greater expressiveness and better integration into programming languages.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of SQL, such as NULL-values, impedance mismatch, SQL injection, and limited querying capabilities for updates.

Method: Introducing FDM and FQL, which are more expressive than SQL, integrate seamlessly with existing programming languages, and unify query and programming languages for holistic optimization.

Result: FDM and FQL provide a comprehensive alternative to SQL, solving key problems and enhancing expressiveness and integration.

Conclusion: FDM and FQL represent a promising direction toward modern, expressive, and integrated data querying and manipulation, simplifying development and optimization.

Abstract: We propose the vision of a functional data model (FDM) and an associated
functional query language (FQL). Our proposal has far-reaching consequences: we
show a path to come up with a modern QL that solves (almost if not) all
problems of SQL (NULL-values, impedance mismatch, SQL injection, missing
querying capabilities for updates, etc.). FDM and FQL are much more expressive
than the relational model and SQL. In addition, in contrast to SQL, FQL
integrates smoothly into existing programming languages. In our approach both
QL and PL become the "same thing", thus opening up some interesting holistic
optimization opportunities between compilers and databases. In FQL, we also do
not need to force application developers to switch to unfamiliar programming
paradigms (like SQL or datalog): developers can stick with the abstractions
provided by their programming language.

</details>


### [85] [MVIAnalyzer: A Holistic Approach to Analyze Missing Value Imputation](https://arxiv.org/abs/2507.20815)
*Valerie Restat,Kai Tejkl,Uta St√∂rl*

Main category: cs.DB

TL;DR: This paper introduces MVIAnalyzer, a comprehensive framework for analyzing missing value imputation (MVI) methods within the broader context of data analysis, supporting evaluation, simulation, visualization, and software tools.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the lack of a universal MVI method by providing a holistic framework for analysis and comparison of different MVI techniques.

Method: The authors develop MVIAnalyzer, a generic framework including simulation tools, analysis procedures, and visualization techniques, demonstrated on datasets with varying characteristics.

Result: The framework enables evaluation of MVI methods, highlighting their capabilities and limitations, and supports researchers with software for their own analysis.

Conclusion: The study showcases the utility of MVIAnalyzer in understanding and assessing MVI methods, emphasizing the complexity of missing data imputation and the need for comprehensive analysis.

Abstract: Missing values often limit the usage of data analysis or cause falsification
of results. Therefore, methods of missing value imputation (MVI) are of great
significance. However, in general, there is no universal, fair MVI method for
different tasks. This work thus places MVI in the overall context of data
analysis. For this purpose, we present the MVIAnalyzer, a generic framework for
a holistic analysis of MVI. It considers the overall process up to the
application and analysis of machine learning methods. The associated software
is provided and can be used by other researchers for their own analyses. To
this end, it further includes a missing value simulation with consideration of
relevant parameters. The application of the MVIAnalyzer is demonstrated on data
with different characteristics. An evaluation of the results shows the
possibilities and limitations of different MVI methods. Since MVI is a very
complex topic with different influencing variables, this paper additionally
illustrates how the analysis can be supported by visualizations.

</details>


### [86] [Data Cleaning of Data Streams](https://arxiv.org/abs/2507.20839)
*Valerie Restat,Niklas Rodenhausen,Carina Antonin,Uta St√∂rl*

Main category: cs.DB

TL;DR: The paper explores challenges and requirements of cleaning streaming data, highlighting its differences from static data cleaning through theoretical analysis and experiments.


<details>
  <summary>Details</summary>
Motivation: Address the gap in understanding how data cleaning applies specifically to streaming data, which has unique challenges.

Method: The paper provides theoretical analysis, comprehensive experiments, and develops a prototype framework to evaluate data cleaning in streaming contexts.

Result: Data cleaning consistency issues were identified in data streams, and requirements for streaming technologies for effective cleaning were discussed.

Conclusion: Effective data cleaning in streaming contexts requires tailored approaches and technology adaptations due to the unique properties of streaming data.

Abstract: Streaming data can arise from a variety of contexts. Important use cases are
continuous sensor measurements such as temperature, light or radiation values.
In the process, streaming data may also contain data errors that should be
cleaned before further use. Many studies from science and practice focus on
data cleaning in a static context. However, in terms of data cleaning,
streaming data has particularities that distinguish it from static data. In
this paper, we have therefore undertaken an intensive exploration of data
cleaning of data streams. We provide a detailed analysis of the applicability
of data cleaning to data streams. Our theoretical considerations are evaluated
in comprehensive experiments. Using a prototype framework, we show that
cleaning is not consistent when working with data streams. An additional
contribution is the investigation of requirements for streaming technologies in
context of data cleaning.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [87] [MAIA: A Collaborative Medical AI Platform for Integrated Healthcare Innovation](https://arxiv.org/abs/2507.19489)
*Simone Bendazzoli,Sanna Persson,Mehdi Astaraki,Sebastian Pettersson,Vitali Grozman,Rodrigo Moreno*

Main category: cs.AI

TL;DR: MAIA is an open-source platform built on Kubernetes that facilitates AI development and deployment in healthcare, promoting collaboration, reproducibility, and real-world application.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between AI research and practical healthcare applications through a collaborative platform.

Method: Developing MAIA, a modular, scalable infrastructure with integrated tools for data, models, and clinical feedback, supporting deployment in real clinical settings.

Result: MAIA supports diverse projects and environments, enhancing interdisciplinary collaboration and accelerating the translation of AI research into clinical solutions.

Conclusion: MAIA promotes interoperability, reproducibility, and user-centered design, advancing AI integration in healthcare.

Abstract: The integration of Artificial Intelligence (AI) into clinical workflows
requires robust collaborative platforms that are able to bridge the gap between
technical innovation and practical healthcare applications. This paper
introduces MAIA (Medical Artificial Intelligence Assistant), an open-source
platform designed to facilitate interdisciplinary collaboration among
clinicians, researchers, and AI developers. Built on Kubernetes, MAIA offers a
modular, scalable environment with integrated tools for data management, model
development, annotation, deployment, and clinical feedback. Key features
include project isolation, CI/CD automation, integration with high-computing
infrastructures and in clinical workflows. MAIA supports real-world use cases
in medical imaging AI, with deployments in both academic and clinical
environments. By promoting collaborations and interoperability, MAIA aims to
accelerate the translation of AI research into impactful clinical solutions
while promoting reproducibility, transparency, and user-centered design. We
showcase the use of MAIA with different projects, both at KTH Royal Institute
of Technology and Karolinska University Hospital.

</details>


### [88] [Agent WARPP: Workflow Adherence via Runtime Parallel Personalization](https://arxiv.org/abs/2507.19543)
*Maria Emilia Mazzolenis,Ruirui Zhang*

Main category: cs.AI

TL;DR: WARPP is a training-free, modular framework that improves workflow adherence in LLM-based task-oriented dialogue systems by dynamically personalizing and pruning decision branches in real-time.


<details>
  <summary>Details</summary>
Motivation: To address the challenges faced by LLMs in managing long, conditional workflows involving external tools and user-specific data in task-oriented dialogues.

Method: WARPP combines multi-agent orchestration with runtime personalization, utilizing a parallel architecture with dedicated agents to tailor execution paths based on user attributes without additional training.

Result: WARPP outperforms baseline methods in accuracy and efficiency across various domains and user intents, especially as complexity increases, while reducing token usage.

Conclusion: WARPP effectively enhances workflow adherence and efficiency in LLM-driven dialogue systems through dynamic, runtime personalization without additional training.

Abstract: Large language models (LLMs) are increasingly applied in task-oriented
dialogue (TOD) systems but often struggle with long, conditional workflows that
involve external tool calls and depend on user-specific information. We present
Workflow Adherence via Runtime Parallel Personalization, or WARPP, a
training-free, modular framework that combines multi-agent orchestration with
runtime personalization to improve workflow adherence in LLM-based systems. By
dynamically pruning conditional branches based on user attributes, the
framework reduces reasoning overhead and narrows tool selection at runtime.
WARPP deploys a parallelized architecture where a dedicated Personalizer agent
operates alongside modular, domain-specific agents to dynamically tailor
execution paths in real time. The framework is evaluated across five
representative user intents of varying complexity within three domains:
banking, flights, and healthcare. Our evaluation leverages synthetic datasets
and LLM-powered simulated users to test scenarios with conditional
dependencies. Our results demonstrate that WARPP outperforms both the
non-personalized method and the ReAct baseline, achieving increasingly larger
gains in parameter fidelity and tool accuracy as intent complexity grows, while
also reducing average token usage, without any additional training.

</details>


### [89] [Integrating Activity Predictions in Knowledge Graphs](https://arxiv.org/abs/2507.19733)
*Alec Scully,Cameron Stockton,Forrest Hare*

Main category: cs.AI

TL;DR: The paper presents a framework for using ontology-based knowledge graphs with Markov chains to predict future events, emphasizing a new concept of spacetime in ontologies and critiquing traditional probabilistic models.


<details>
  <summary>Details</summary>
Motivation: To enhance event prediction using knowledge graphs structured with formal ontologies and address limitations in current probabilistic ontologies.

Method: Organizing data in knowledge graphs using BFO and CCO, introducing the concept of spatiotemporal instant, and employing Markov chains for prediction.

Result: Demonstrated how structured data and Markov models can be integrated into knowledge graphs for predictive analytics, and proposed a new perspective on probabilistic modeling.

Conclusion: Ontology-structured knowledge graphs combined with Markov chains improve event prediction and offer a refined understanding of probabilities, facilitating better decision-making.

Abstract: We argue that ontology-structured knowledge graphs can play a crucial role in
generating predictions about future events. By leveraging the semantic
framework provided by Basic Formal Ontology (BFO) and Common Core Ontologies
(CCO), we demonstrate how data such as the movements of a fishing vessel can be
organized in and retrieved from a knowledge graph. These query results are then
used to create Markov chain models, allowing us to predict future states based
on the vessel's history. To fully support this process, we introduce the term
`spatiotemporal instant' to complete the necessary structural semantics.
Additionally, we critique the prevailing ontological model of probability,
which conflates probability with likelihood and relies on the problematic
concept of modal measurements: measurements of future entities. We propose an
alternative view, where probabilities are treated as being about process
profiles, which better captures the dynamics of real world phenomena. Finally,
we demonstrate how our Markov chain based probability calculations can be
seamlessly integrated back into the knowledge graph, enabling further analysis
and decision-making. Keywords: predictive analytics, ontology, Markov chains,
probability, Basic Formal Ontology (BFO), knowledge graphs, SPARQL.

</details>


### [90] [Hypergames: Modeling Misaligned Perceptions and Nested Beliefs for Multi-agent Systems](https://arxiv.org/abs/2507.19593)
*Vince Trencsenyi,Agnieszka Mensfelt,Kostas Stathis*

Main category: cs.AI

TL;DR: This review analyzes hypergame theory's application in multi-agent systems, highlighting its ability to model subjective perceptions and divergent beliefs. It finds widespread use in hierarchical and graph-based models, notes gaps like limited formal languages and underexplored human-agent misalignments, and proposes future research directions.


<details>
  <summary>Details</summary>
Motivation: To address classical game theory limitations in modeling real-world multi-agent systems characterized by uncertainty and perceptual divergence.

Method: Systematic review of 44 studies across cybersecurity, robotics, social simulation, and communications, with formal analysis of hypergame theory and its extensions.

Result: Identification of prevalent modeling trends, structural gaps, and practical adaptation patterns in hypergame applications.

Conclusion: Hypergame theory offers valuable descriptive power for dynamic MAS, but requires further development in formal languages and human-agent modeling to realize its full potential.

Abstract: Classical game-theoretic models typically assume rational agents, complete
information, and common knowledge of payoffs - assumptions that are often
violated in real-world MAS characterized by uncertainty, misaligned
perceptions, and nested beliefs. To overcome these limitations, researchers
have proposed extensions that incorporate models of cognitive constraints,
subjective beliefs, and heterogeneous reasoning. Among these, hypergame theory
extends the classical paradigm by explicitly modeling agents' subjective
perceptions of the strategic scenario, known as perceptual games, in which
agents may hold divergent beliefs about the structure, payoffs, or available
actions. We present a systematic review of agent-compatible applications of
hypergame theory, examining how its descriptive capabilities have been adapted
to dynamic and interactive MAS contexts. We analyze 44 selected studies from
cybersecurity, robotics, social simulation, communications, and general
game-theoretic modeling. Building on a formal introduction to hypergame theory
and its two major extensions - hierarchical hypergames and HNF - we develop
agent-compatibility criteria and an agent-based classification framework to
assess integration patterns and practical applicability. Our analysis reveals
prevailing tendencies, including the prevalence of hierarchical and graph-based
models in deceptive reasoning and the simplification of extensive theoretical
frameworks in practical applications. We identify structural gaps, including
the limited adoption of HNF-based models, the lack of formal hypergame
languages, and unexplored opportunities for modeling human-agent and
agent-agent misalignment. By synthesizing trends, challenges, and open research
directions, this review provides a new roadmap for applying hypergame theory to
enhance the realism and effectiveness of strategic modeling in dynamic
multi-agent environments.

</details>


### [91] [DeltaLLM: A Training-Free Framework Exploiting Temporal Sparsity for Efficient Edge LLM Inference](https://arxiv.org/abs/2507.19608)
*Jiawen Qi,Chang Gao,Zhaochun Ren,Qinyu Chen*

Main category: cs.AI

TL;DR: DeltaLLM is a training-free framework that exploits temporal sparsity in attention patterns to enable efficient LLM inference on resource-constrained edge devices, without fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Edge deployment of Large Language Models faces significant challenges due to computational demands, particularly with long context lengths.

Method: DeltaLLM uses a delta matrix construction strategy and a hybrid attention mechanism to introduce temporal sparsity and improve accuracy.

Result: The framework significantly increases attention sparsity during inference stages with minimal accuracy loss across different models and tasks.

Conclusion: DeltaLLM provides a promising, tuning-free solution for efficient LLM deployment on edge devices, compatible with existing inference pipelines.

Abstract: Deploying Large Language Models (LLMs) on edge devices remains challenging
due to their quadratically increasing computations with the sequence length.
Existing studies for dynamic attention pruning are designed for hardware with
massively parallel computation capabilities, such as GPUs or TPUs, and aim at
long context lengths (e.g., 64K), making them unsuitable for edge scenarios. We
present DeltaLLM, a training-free framework that exploits temporal sparsity in
attention patterns to enable efficient LLM inference across both the prefilling
and decoding stages, on resource-constrained edge devices. DeltaLLM introduces
an accuracy- and memory-aware delta matrix construction strategy that
introduces temporal sparsity, and a context-aware hybrid attention mechanism
that combines full attention in a local context window with delta approximation
outside it to increase accuracy. We evaluate our framework on the
edge-device-friendly BitNet-b1.58-2B-4T model and Llama3.2-1B-Instruct model
across diverse language tasks. The results show that on BitNet, our framework
increases the attention sparsity from 0% to 60% during the prefilling stage
with slight accuracy improvement on the WG task, and 0% to 57% across both the
prefilling and decoding stages, with even higher F1 score from 29.63 to 30.97
on SQuAD-v2 task. On the Llama model, it can also achieve up to 60% sparsity
during the prefilling stage and around 57% across both stages with negligible
accuracy drop. These results demonstrate that DeltaLLM offers a promising
solution for efficient edge deployment, requiring no fine-tuning and seamlessly
integrating with existing inference pipelines.

</details>


### [92] [Alignment and Safety in Large Language Models: Safety Mechanisms, Training Paradigms, and Emerging Challenges](https://arxiv.org/abs/2507.19672)
*Haoran Lu,Luyang Fang,Ruidong Zhang,Xinliang Li,Jiazhang Cai,Huimin Cheng,Lin Tang,Ziyu Liu,Zeliang Sun,Tao Wang,Yingchuan Zhang,Arif Hassan Zidan,Jinwen Xu,Jincheng Yu,Meizhi Yu,Hanqi Jiang,Xilin Gong,Weidi Luo,Bolun Sun,Yongkai Chen,Terry Ma,Shushan Wu,Yifan Zhou,Junhao Chen,Haotian Xiang,Jing Zhang,Afrar Jahin,Wei Ruan,Ke Deng,Yi Pan,Peilong Wang,Jiahui Li,Zhengliang Liu,Lu Zhang,Lin Zhao,Wei Liu,Dajiang Zhu,Xin Xing,Fei Dou,Wei Zhang,Chao Huang,Rongjie Liu,Mengrui Zhang,Yiwen Liu,Xiaoxiao Sun,Qin Lu,Zhen Xiang,Wenxuan Zhong,Tianming Liu,Ping Ma*

Main category: cs.AI

TL;DR: A comprehensive overview of methods and challenges in aligning large language models with human values.


<details>
  <summary>Details</summary>
Motivation: The increasing impact of LLMs in society necessitates ensuring their alignment with human values and intentions.

Method: Survey of practical alignment techniques, training protocols, empirical findings, and evaluation frameworks across various paradigms.

Result: Analysis of different alignment paradigms, highlighting trade-offs, state-of-the-art techniques, and existing limitations.

Conclusion: Identification of open problems such as oversight, robustness, and value pluralism, to guide future research.

Abstract: Due to the remarkable capabilities and growing impact of large language
models (LLMs), they have been deeply integrated into many aspects of society.
Thus, ensuring their alignment with human values and intentions has emerged as
a critical challenge. This survey provides a comprehensive overview of
practical alignment techniques, training protocols, and empirical findings in
LLM alignment. We analyze the development of alignment methods across diverse
paradigms, characterizing the fundamental trade-offs between core alignment
objectives. Our analysis shows that while supervised fine-tuning enables basic
instruction-following, preference-based methods offer more flexibility for
aligning with nuanced human intent. We discuss state-of-the-art techniques,
including Direct Preference Optimization (DPO), Constitutional AI,
brain-inspired methods, and alignment uncertainty quantification (AUQ),
highlighting their approaches to balancing quality and efficiency. We review
existing evaluation frameworks and benchmarking datasets, emphasizing
limitations such as reward misspecification, distributional robustness, and
scalable oversight. We summarize strategies adopted by leading AI labs to
illustrate the current state of practice. We conclude by outlining open
problems in oversight, value pluralism, robustness, and continuous alignment.
This survey aims to inform both researchers and practitioners navigating the
evolving landscape of LLM alignment.

</details>


### [93] [The wall confronting large language models](https://arxiv.org/abs/2507.19703)
*Peter V. Coveney,Sauro Succi*

Main category: cs.AI

TL;DR: Large language models (LLMs) face fundamental limitations in improving uncertainty and reliability due to their scaling laws, which may lead to degenerative AI behaviors. Greater emphasis on understanding problem structures could help avoid these issues.


<details>
  <summary>Details</summary>
Motivation: To understand the limitations imposed by scaling laws on the reliability and uncertainty of LLMs and explore ways to mitigate degenerative behaviors.

Method: The paper analyzes the scaling laws of LLMs and their impact on prediction uncertainty, the mechanism behind non-Gaussian output distributions, and the influence of spurious correlations.

Result: Scaling laws hinder significant improvements in prediction uncertainty, and the mechanism for LLM learning might cause error pileup and degenerative behaviors. Avoiding these requires prioritizing insight and understanding of problem structures.

Conclusion: Addressing the limitations of LLMs involves focusing on structural understanding rather than scaling alone to prevent degenerative AI pathways.

Abstract: We show that the scaling laws which determine the performance of large
language models (LLMs) severely limit their ability to improve the uncertainty
of their predictions. As a result, raising their reliability to meet the
standards of scientific inquiry is intractable by any reasonable measure. We
argue that the very mechanism which fuels much of the learning power of LLMs,
namely the ability to generate non-Gaussian output distributions from Gaussian
input ones, might well be at the roots of their propensity to produce error
pileup, ensuing information catastrophes and degenerative AI behaviour. This
tension between learning and accuracy is a likely candidate mechanism
underlying the observed low values of the scaling components. It is
substantially compounded by the deluge of spurious correlations pointed out by
Calude and Longo which rapidly increase in any data set merely as a function of
its size, regardless of its nature. The fact that a degenerative AI pathway is
a very probable feature of the LLM landscape does not mean that it must
inevitably arise in all future AI research. Its avoidance, which we also
discuss in this paper, necessitates putting a much higher premium on insight
and understanding of the structural characteristics of the problems being
investigated.

</details>


### [94] [Minding Motivation: The Effect of Intrinsic Motivation on Agent Behaviors](https://arxiv.org/abs/2507.19725)
*Leonardo Villalobos-Arias,Grant Forbes,Jianxun Wang,David L Roberts,Arnav Jhala*

Main category: cs.AI

TL;DR: This study investigates how Intrinsic Motivation (IM) impacts Reinforcement Learning (RL) agents' behavior in game environments, highlighting issues of reward hacking and showing that Generalized Reward Matching can mitigate this problem.


<details>
  <summary>Details</summary>
Motivation: The motivation is to understand the behavioral changes caused by IM in RL agents and address reward hacking issues that undermine effective learning in reward-sparse environments.

Method: The authors empirically evaluate three IM techniques and compare them with Generalized Reward Matching in the MiniGrid environment, observing their effects on agent behavior.

Result: IM increases initial rewards and alters gameplay, with GRM reducing reward hacking in some cases.

Conclusion: IM significantly influences RL behavior, and GRM offers a potential solution to reward hacking, contributing to more reliable RL methods.

Abstract: Games are challenging for Reinforcement Learning~(RL) agents due to their
reward-sparsity, as rewards are only obtainable after long sequences of
deliberate actions. Intrinsic Motivation~(IM) methods -- which introduce
exploration rewards -- are an effective solution to reward-sparsity. However,
IM also causes an issue known as `reward hacking' where the agent optimizes for
the new reward at the expense of properly playing the game. The larger problem
is that reward hacking itself is largely unknown; there is no answer to
whether, and to what extent, IM rewards change the behavior of RL agents. This
study takes a first step by empirically evaluating the impact on behavior of
three IM techniques on the MiniGrid game-like environment. We compare these IM
models with Generalized Reward Matching~(GRM), a method that can be used with
any intrinsic reward function to guarantee optimality. Our results suggest that
IM causes noticeable change by increasing the initial rewards, but also
altering the way the agent plays; and that GRM mitigated reward hacking in some
scenarios.

</details>


### [95] [HypKG: Hypergraph-based Knowledge Graph Contextualization for Precision Healthcare](https://arxiv.org/abs/2507.19726)
*Yuzhang Xie,Xu Han,Ran Xu,Xiao Hu,Jiaying Lu,Carl Yang*

Main category: cs.AI

TL;DR: HypKG integrates electronic health records with knowledge graphs to create contextualized representations for better healthcare predictions.


<details>
  <summary>Details</summary>
Motivation: To enhance the accuracy of healthcare predictions by incorporating patient-specific context into knowledge graphs.

Method: Uses entity-linking to connect EHR data with general KGs, employs a hypergraph model and hypergraph transformers to learn contextualized representations.

Result: Achieves significant improvements in prediction tasks and better adjusts entity and relation representations by incorporating external contexts.

Conclusion: HypKG effectively leverages both general knowledge and patient-specific data for improved healthcare decision-making.

Abstract: Knowledge graphs (KGs) are important products of the semantic web, which are
widely used in various application domains. Healthcare is one of such domains
where KGs are intensively used, due to the high requirement for knowledge
accuracy and interconnected nature of healthcare data. However, KGs storing
general factual information often lack the ability to account for important
contexts of the knowledge such as the status of specific patients, which are
crucial in precision healthcare. Meanwhile, electronic health records (EHRs)
provide rich personal data, including various diagnoses and medications, which
provide natural contexts for general KGs. In this paper, we propose HypKG, a
framework that integrates patient information from EHRs into KGs to generate
contextualized knowledge representations for accurate healthcare predictions.
Using advanced entity-linking techniques, we connect relevant knowledge from
general KGs with patient information from EHRs, and then utilize a hypergraph
model to "contextualize" the knowledge with the patient information. Finally,
we employ hypergraph transformers guided by downstream prediction tasks to
jointly learn proper contextualized representations for both KGs and patients,
fully leveraging existing knowledge in KGs and patient contexts in EHRs. In
experiments using a large biomedical KG and two real-world EHR datasets, HypKG
demonstrates significant improvements in healthcare prediction tasks across
multiple evaluation metrics. Additionally, by integrating external contexts,
HypKG can learn to adjust the representations of entities and relations in KG,
potentially improving the quality and real-world utility of knowledge.

</details>


### [96] [Can LLMs Solve ASP Problems? Insights from a Benchmarking Study (Extended Version)](https://arxiv.org/abs/2507.19749)
*Lin Ren,Guohui Xiao,Guilin Qi,Yishuai Geng,Haohan Xue*

Main category: cs.AI

TL;DR: Introduction of ASPBench, a comprehensive benchmark for evaluating LLMs in ASP tasks, revealing current limitations especially in complex answer set computation.


<details>
  <summary>Details</summary>
Motivation: Address the gap in evaluating LLM capabilities in ASP, especially in complex reasoning tasks involving negation, disjunction, and multiple answer sets.

Method: Developed ASPBench with three specific ASP tasks and evaluated 14 state-of-the-art LLMs.

Result: LLMs perform well on simpler tasks but struggle with core ASP task: answer set computation, highlighting their limitations.

Conclusion: Current LLMs need better integration of symbolic reasoning for robust ASP solving.

Abstract: Answer Set Programming (ASP) is a powerful paradigm for non-monotonic
reasoning. Recently, large language models (LLMs) have demonstrated promising
capabilities in logical reasoning. Despite this potential, current evaluations
of LLM capabilities in ASP are often limited. Existing works normally employ
overly simplified ASP programs, do not support negation, disjunction, or
multiple answer sets. Furthermore, there is a lack of benchmarks that introduce
tasks specifically designed for ASP solving. To bridge this gap, we introduce
ASPBench, a comprehensive ASP benchmark, including three ASP specific tasks:
ASP entailment, answer set verification, and answer set computation. Our
extensive evaluations on ASPBench reveal that while 14 state-of-the-art LLMs,
including \emph{deepseek-r1}, \emph{o4-mini}, and
\emph{gemini-2.5-flash-thinking}, perform relatively well on the first two
simpler tasks, they struggle with answer set computation, which is the core of
ASP solving. These findings offer insights into the current limitations of LLMs
in ASP solving. This highlights the need for new approaches that integrate
symbolic reasoning capabilities more effectively. The code and dataset are
available at https://github.com/HomuraT/ASPBench.

</details>


### [97] [Reinforcement Learning for Multi-Objective Multi-Echelon Supply Chain Optimisation](https://arxiv.org/abs/2507.19788)
*Rifny Rachman,Josh Tingey,Richard Allmendinger,Pradyumn Shukla,Wei Pan*

Main category: cs.AI

TL;DR: A multi-objective reinforcement learning approach for supply chain optimization that balances economic, environmental, and social factors, outperforming traditional methods in complex scenarios.


<details>
  <summary>Details</summary>
Motivation: To develop a comprehensive supply chain model that incorporates multiple objectives and market dynamics using advanced RL techniques.

Method: A generalized multi-objective, multi-echelon supply chain model based on Markov decision processes, evaluated through multi-objective RL, MOEA, and weighted sum RL, tested on complex simulations.

Result: The multi-objective RL approach achieved superior Pareto front approximations, higher trade-off quality, and robustness compared to other methods, especially in complex environments.

Conclusion: The primary multi-objective RL method offers balanced and dense solutions with improved stability and diversity, advancing supply chain optimization under multi-faceted considerations.

Abstract: This study develops a generalised multi-objective, multi-echelon supply chain
optimisation model with non-stationary markets based on a Markov decision
process, incorporating economic, environmental, and social considerations. The
model is evaluated using a multi-objective reinforcement learning (RL) method,
benchmarked against an originally single-objective RL algorithm modified with
weighted sum using predefined weights, and a multi-objective evolutionary
algorithm (MOEA)-based approach. We conduct experiments on varying network
complexities, mimicking typical real-world challenges using a customisable
simulator. The model determines production and delivery quantities across
supply chain routes to achieve near-optimal trade-offs between competing
objectives, approximating Pareto front sets. The results demonstrate that the
primary approach provides the most balanced trade-off between optimality,
diversity, and density, further enhanced with a shared experience buffer that
allows knowledge transfer among policies. In complex settings, it achieves up
to 75\% higher hypervolume than the MOEA-based method and generates solutions
that are approximately eleven times denser, signifying better robustness, than
those produced by the modified single-objective RL method. Moreover, it ensures
stable production and inventory levels while minimising demand loss.

</details>


### [98] [Causality-aligned Prompt Learning via Diffusion-based Counterfactual Generation](https://arxiv.org/abs/2507.19882)
*Xinshu Li,Ruoyu Wang,Erdun Gao,Mingming Gong,Lina Yao*

Main category: cs.AI

TL;DR: The paper introduces DiCap, a diffusion-based counterfactual prompt learning model grounded in theory, which enhances prompt invariance and robustness across categories.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing prompt learning methods, which lack strong theoretical foundations and struggle with causally invariant prompts.

Method: A diffusion process that iteratively samples gradients to generate counterfactuals, combined with contrastive learning to align prompts with causal features.

Result: Demonstrates superior performance across various tasks, especially in unseen categories, with theoretical guarantees on identifiability and error bounds.

Conclusion: DiCap provides a theoretically sound framework that improves the robustness and generalization of prompt learning through counterfactual generation and causal alignment.

Abstract: Prompt learning has garnered attention for its efficiency over traditional
model training and fine-tuning. However, existing methods, constrained by
inadequate theoretical foundations, encounter difficulties in achieving
causally invariant prompts, ultimately falling short of capturing robust
features that generalize effectively across categories. To address these
challenges, we introduce the $\textit{\textbf{DiCap}}$ model, a theoretically
grounded $\textbf{Di}$ffusion-based $\textbf{C}$ounterf$\textbf{a}$ctual
$\textbf{p}$rompt learning framework, which leverages a diffusion process to
iteratively sample gradients from the marginal and conditional distributions of
the causal model, guiding the generation of counterfactuals that satisfy the
minimal sufficiency criterion. Grounded in rigorous theoretical derivations,
this approach guarantees the identifiability of counterfactual outcomes while
imposing strict bounds on estimation errors. We further employ a contrastive
learning framework that leverages the generated counterfactuals, thereby
enabling the refined extraction of prompts that are precisely aligned with the
causal features of the data. Extensive experimental results demonstrate that
our method performs excellently across tasks such as image classification,
image-text retrieval, and visual question answering, with particularly strong
advantages in unseen categories.

</details>


### [99] [What Does 'Human-Centred AI' Mean?](https://arxiv.org/abs/2507.19960)
*Olivia Guest*

Main category: cs.AI

TL;DR: AI should be viewed as a relationship between technology and human cognition, emphasizing its impact on human cognitive labor and cautioning against obfuscation of cognition.


<details>
  <summary>Details</summary>
Motivation: To clarify the conceptualization of human-centred AI and analyze its sociotechnical implications.

Method: Using novel definitions and examples to categorize AI impacts as displacement, enhancement, or replacement of human cognition.

Result: AI always involves human cognition, and obfuscation hampers understanding and critical engagement, hindering truly human-centered AI development.

Conclusion: To achieve genuine human-centered AI, we must critically examine the human-in-the-loop and avoid cognitive obfuscation.

Abstract: While it seems sensible that human-centred artificial intelligence (AI) means
centring "human behaviour and experience," it cannot be any other way. AI, I
argue, is usefully seen as a relationship between technology and humans where
it appears that artifacts can perform, to a greater or lesser extent, human
cognitive labour. This is evinced using examples that juxtapose technology with
cognition, inter alia: abacus versus mental arithmetic; alarm clock versus
knocker-upper; camera versus vision; and sweatshop versus tailor. Using novel
definitions and analyses, sociotechnical relationships can be analysed into
varying types of: displacement (harmful), enhancement (beneficial), and/or
replacement (neutral) of human cognitive labour. Ultimately, all AI implicates
human cognition; no matter what. Obfuscation of cognition in the AI context --
from clocks to artificial neural networks -- results in distortion, in slowing
critical engagement, perverting cognitive science, and indeed in limiting our
ability to truly centre humans and humanity in the engineering of AI systems.
To even begin to de-fetishise AI, we must look the human-in-the-loop in the
eyes.

</details>


### [100] [Leveraging Fine-Tuned Large Language Models for Interpretable Pancreatic Cystic Lesion Feature Extraction and Risk Categorization](https://arxiv.org/abs/2507.19973)
*Ebrahim Rasromani,Stella K. Kang,Yanqi Xu,Beisong Liu,Garvit Luhadia,Wan Fung Chui,Felicia L. Pasadyn,Yu Chih Hung,Julie Y. An,Edwin Mathieu,Zehui Gu,Carlos Fernandez-Granda,Ammar A. Javed,Greg D. Sacks,Tamas Gonda,Chenchan Huang,Yiqiu Shen*

Main category: cs.AI

TL;DR: Fine-tuned open-source large language models with chain-of-thought prompting accurately extract pancreatic cystic lesion features and classify risks from radiology reports, matching GPT-4o performance and radiologist agreement.


<details>
  <summary>Details</summary>
Motivation: Manual extraction of PCL features from radiology reports is labor-intensive, limiting large-scale research efforts.

Method: Curated a large dataset, used GPT-4o with chain-of-thought prompting for labeling, then fine-tuned open-source LLMs (LLaMA and DeepSeek) using QLoRA. Evaluated on held-out reports and compared with radiologist annotations.

Result: Enhanced feature extraction accuracy (up to 98%), high risk categorization F1 scores (~0.94-0.95), and agreement levels with radiologists, comparable to GPT-4o.

Conclusion: Open-source LLMs fine-tuned with CoT supervision provide accurate, interpretable phenotyping for large-scale PCL studies, matching GPT-4o performance.

Abstract: Background: Manual extraction of pancreatic cystic lesion (PCL) features from
radiology reports is labor-intensive, limiting large-scale studies needed to
advance PCL research. Purpose: To develop and evaluate large language models
(LLMs) that automatically extract PCL features from MRI/CT reports and assign
risk categories based on guidelines. Materials and Methods: We curated a
training dataset of 6,000 abdominal MRI/CT reports (2005-2024) from 5,134
patients that described PCLs. Labels were generated by GPT-4o using
chain-of-thought (CoT) prompting to extract PCL and main pancreatic duct
features. Two open-source LLMs were fine-tuned using QLoRA on GPT-4o-generated
CoT data. Features were mapped to risk categories per institutional guideline
based on the 2017 ACR White Paper. Evaluation was performed on 285 held-out
human-annotated reports. Model outputs for 100 cases were independently
reviewed by three radiologists. Feature extraction was evaluated using exact
match accuracy, risk categorization with macro-averaged F1 score, and
radiologist-model agreement with Fleiss' Kappa. Results: CoT fine-tuning
improved feature extraction accuracy for LLaMA (80% to 97%) and DeepSeek (79%
to 98%), matching GPT-4o (97%). Risk categorization F1 scores also improved
(LLaMA: 0.95; DeepSeek: 0.94), closely matching GPT-4o (0.97), with no
statistically significant differences. Radiologist inter-reader agreement was
high (Fleiss' Kappa = 0.888) and showed no statistically significant difference
with the addition of DeepSeek-FT-CoT (Fleiss' Kappa = 0.893) or GPT-CoT
(Fleiss' Kappa = 0.897), indicating that both models achieved agreement levels
on par with radiologists. Conclusion: Fine-tuned open-source LLMs with CoT
supervision enable accurate, interpretable, and efficient phenotyping for
large-scale PCL research, achieving performance comparable to GPT-4o.

</details>


### [101] [Digital Twin Channel-Enabled Online Resource Allocation for 6G: Principle, Architecture and Application](https://arxiv.org/abs/2507.19974)
*Tongjie Li,Jianhua Zhang,Li Yu,Yuxiang Zhang,Yunlong Cai,Fan Xu,Guangyi Liu*

Main category: cs.AI

TL;DR: A digital twin channel-enabled framework improves resource allocation in 6G networks by predicting CSI with environment sensing, enhancing throughput with low overhead.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of conventional statistical modeling and pilot overhead in dynamic environments for 6G resource allocation.

Method: Using digital twin channels to predict CSI based on environmental sensing, combined with lightweight game-theoretic algorithms for online resource allocation.

Result: Achieved up to 11.5% throughput improvement over ideal pilot-based CSI schemes in simulations of a realistic industrial environment.

Conclusion: The proposed approach offers a scalable, efficient, and environment-aware solution for 6G resource allocation.

Abstract: Emerging applications such as holographic communication, autonomous driving,
and the industrial Internet of Things impose stringent requirements on
flexible, low-latency, and reliable resource allocation in 6G networks.
Conventional methods, which rely on statistical modeling, have proven effective
in general contexts but may fail to achieve optimal performance in specific and
dynamic environments. Furthermore, acquiring real-time channel state
information (CSI) typically requires excessive pilot overhead. To address these
challenges, a digital twin channel (DTC)-enabled online optimization framework
is proposed, in which DTC is employed to predict CSI based on environmental
sensing. The predicted CSI is then utilized by lightweight game-theoretic
algorithms to perform online resource allocation in a timely and efficient
manner. Simulation results based on a digital replica of a realistic industrial
workshop demonstrate that the proposed method achieves throughput improvements
of up to 11.5\% compared with pilot-based ideal CSI schemes, validating its
effectiveness for scalable, low-overhead, and environment-aware communication
in future 6G networks.

</details>


### [102] [Matching Game Preferences Through Dialogical Large Language Models: A Perspective](https://arxiv.org/abs/2507.20000)
*Renaud Fabre,Daniel Egret,Patrice Bellot*

Main category: cs.AI

TL;DR: The paper explores integrating Large Language Models with network systems to enhance transparency, personalization, and understanding of AI reasoning in human conversations.


<details>
  <summary>Details</summary>
Motivation: To develop AI systems that are transparent, interpretable, and capable of capturing human preferences for better decision-making.

Method: Proposing a conceptual framework called D-LLMs that combines reasoning, classification, and dialogue components to incorporate user preferences and make AI reasoning traceable.

Result: A vision of an interpretable AI system that displays how responses are generated by integrating human preferences and search experience networks.

Conclusion: Such systems could improve trust and transparency in AI by revealing how conclusions are reached, thus aiding human decision-making.

Abstract: This perspective paper explores the future potential of "conversational
intelligence" by examining how Large Language Models (LLMs) could be combined
with GRAPHYP's network system to better understand human conversations and
preferences. Using recent research and case studies, we propose a conceptual
framework that could make AI rea-soning transparent and traceable, allowing
humans to see and understand how AI reaches its conclusions. We present the
conceptual perspective of "Matching Game Preferences through Dialogical Large
Language Models (D-LLMs)," a proposed system that would allow multiple users to
share their different preferences through structured conversations. This
approach envisions personalizing LLMs by embedding individual user preferences
directly into how the model makes decisions. The proposed D-LLM framework would
require three main components: (1) reasoning processes that could analyze
different search experiences and guide performance, (2) classification systems
that would identify user preference patterns, and (3) dialogue approaches that
could help humans resolve conflicting information. This perspective framework
aims to create an interpretable AI system where users could examine,
understand, and combine the different human preferences that influence AI
responses, detected through GRAPHYP's search experience networks. The goal of
this perspective is to envision AI systems that would not only provide answers
but also show users how those answers were reached, making artificial
intelligence more transparent and trustworthy for human decision-making.

</details>


### [103] [Finding Personalized Good-Enough Solutions to Unsatisfiable Stable Roommates Problems](https://arxiv.org/abs/2507.20010)
*M√ºge Fidan,Esra Erdem*

Main category: cs.AI

TL;DR: The paper explores the stable roommates problem, focusing on computing acceptable, 


<details>
  <summary>Details</summary>
Motivation: The motivation is to find workable solutions in real-world situations where stable matchings may not exist, considering agents' preferences and social networks.

Method: The authors develop a method to generate personalized solutions by integrating agents' habits, preferences, and social networks.

Result: They demonstrate the effectiveness of their approach through examples and empirical evaluations.

Conclusion: The study offers a practical approach to approximate matchings in complex social settings where perfect stability cannot always be achieved.

Abstract: The Stable Roommates problems are characterized by the preferences of agents
over other agents as roommates. A solution is a partition of the agents into
pairs that are acceptable to each other (i.e., they are in the preference lists
of each other), and the matching is stable (i.e., there do not exist any two
agents who prefer each other to their roommates, and thus block the matching).
Motivated by real-world applications, and considering that stable roommates
problems do not always have solutions, we continue our studies to compute
"good-enough" matchings. In addition to the agents' habits and habitual
preferences, we consider their networks of preferred friends, and introduce a
method to generate personalized solutions to stable roommates problems. We
illustrate the usefulness of our method with examples and empirical
evaluations.

</details>


### [104] [PITA: Preference-Guided Inference-Time Alignment for LLM Post-Training](https://arxiv.org/abs/2507.20067)
*Sarat Chandra Bobbili,Ujwal Dinesha,Dheeraj Narasimha,Srinivas Shakkottai*

Main category: cs.AI

TL;DR: PITA is a new framework for aligning large language model outputs with user preferences without needing pre-trained reward models, by learning a preference-guided policy at inference time.


<details>
  <summary>Details</summary>
Motivation: To address the reliance on pre-trained reward models in existing inference-time alignment methods, which can be unstable and costly.

Method: PITA integrates preference feedback directly into token generation via a small guidance policy, optimized through stochastic search and iterative refinement, without fine-tuning the LLM.

Result: PITA effectively aligns LLM outputs with user preferences across tasks like mathematical reasoning and sentiment analysis, reducing costs and dependencies.

Conclusion: PITA offers a promising, more stable approach for inference-time alignment of LLMs that bypasses the need for reward models.

Abstract: Inference-time alignment enables large language models (LLMs) to generate
outputs aligned with end-user preferences without further training. Recent
post-training methods achieve this by using small guidance models to modify
token generation during inference. These methods typically optimize a reward
function KL-regularized by the original LLM taken as the reference policy. A
critical limitation, however, is their dependence on a pre-trained reward
model, which requires fitting to human preference feedback--a potentially
unstable process. In contrast, we introduce PITA, a novel framework that
integrates preference feedback directly into the LLM's token generation,
eliminating the need for a reward model. PITA learns a small preference-based
guidance policy to modify token probabilities at inference time without LLM
fine-tuning, reducing computational cost and bypassing the pre-trained reward
model dependency. The problem is framed as identifying an underlying preference
distribution, solved through stochastic search and iterative refinement of the
preference-based guidance model. We evaluate PITA across diverse tasks,
including mathematical reasoning and sentiment classification, demonstrating
its effectiveness in aligning LLM outputs with user preferences.

</details>


### [105] [Concept Learning for Cooperative Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2507.20143)
*Zhonghan Ge,Yuanyang Zhu,Chunlin Chen*

Main category: cs.AI

TL;DR: This paper introduces CMQ, a concept-based value decomposition method for interpretable multi-agent reinforcement learning, improving performance and transparency.


<details>
  <summary>Details</summary>
Motivation: Enhance the interpretability and trustworthiness of neural network-based multi-agent reinforcement learning systems.

Method: Developed a concept learning framework using vector representations and supervised learning to model cooperation concepts within multi-agent Q-learning.

Result: CMQ outperforms state-of-the-art methods in StarCraft II and foraging tasks, providing meaningful cooperation concepts and enabling test-time interventions.

Conclusion: CMQ advances interpretable reinforcement learning by effectively capturing cooperation modes and allowing for bias detection and artifact identification.

Abstract: Despite substantial progress in applying neural networks (NN) to multi-agent
reinforcement learning (MARL) areas, they still largely suffer from a lack of
transparency and interoperability. However, its implicit cooperative mechanism
is not yet fully understood due to black-box networks. In this work, we study
an interpretable value decomposition framework via concept bottleneck models,
which promote trustworthiness by conditioning credit assignment on an
intermediate level of human-like cooperation concepts. To address this problem,
we propose a novel value-based method, named Concepts learning for Multi-agent
Q-learning (CMQ), that goes beyond the current performance-vs-interpretability
trade-off by learning interpretable cooperation concepts. CMQ represents each
cooperation concept as a supervised vector, as opposed to existing models where
the information flowing through their end-to-end mechanism is concept-agnostic.
Intuitively, using individual action value conditioning on global state
embeddings to represent each concept allows for extra cooperation
representation capacity. Empirical evaluations on the StarCraft II
micromanagement challenge and level-based foraging (LBF) show that CMQ achieves
superior performance compared with the state-of-the-art counterparts. The
results also demonstrate that CMQ provides more cooperation concept
representation capturing meaningful cooperation modes, and supports test-time
concept interventions for detecting potential biases of cooperation mode and
identifying spurious artifacts that impact cooperation.

</details>


### [106] [The Policy Cliff: A Theoretical Analysis of Reward-Policy Maps in Large Language Models](https://arxiv.org/abs/2507.20150)
*Xingcheng Xu*

Main category: cs.AI

TL;DR: This paper develops a mathematical framework to analyze policy stability in reinforcement learning, explaining issues like brittleness and failures in large language models as outcomes of reward optimization under uncertainty.


<details>
  <summary>Details</summary>
Motivation: To provide a unified theoretical explanation for the instability and failures in RL policies used in LLMs/LRMs, which are currently addressed with heuristics.

Method: The authors formulate a rigorous mathematical analysis of the reward-to-policy mapping, extending from single to multi-reward settings, and investigate the impact of entropy regularization.

Result: The framework explains how policy brittleness arises from non-unique optimal actions, with entropy regularization improving stability at the cost of more stochastic policies.

Conclusion: This work offers a principled theory for policy stability in RL, guiding the design of safer and more trustworthy AI systems.

Abstract: Reinforcement learning (RL) plays a crucial role in shaping the behavior of
large language and reasoning models (LLMs/LRMs). However, it often produces
brittle and unstable policies, leading to critical failures such as spurious
reasoning, deceptive alignment, and instruction disobedience that undermine the
trustworthiness and safety of LLMs/LRMs. Currently, these issues lack a unified
theoretical explanation and are typically addressed using ad-hoc heuristics.
This paper presents a rigorous mathematical framework for analyzing the
stability of the mapping from a reward function to the optimal policy. We show
that policy brittleness often stems from non-unique optimal actions, a common
occurrence when multiple valid traces exist in a reasoning task. This
theoretical lens provides a unified explanation for a range of seemingly
disparate failures, reframing them as rational outcomes of optimizing rewards
that may be incomplete or noisy, especially in the presence of action
degeneracy. We extend this analysis from the fundamental single-reward setting
to the more realistic multi-reward RL across diverse domains, showing how
stability is governed by an "effective reward" aggregation mechanism. We also
prove that entropy regularization restores policy stability at the cost of
increased stochasticity. Our framework provides a unified explanation for
recent empirical findings on deceptive reasoning, instruction-following
trade-offs, and RLHF-induced sophistry, and is further validated through
perturbation experiments in multi-reward RL. This work advances
policy-stability analysis from empirical heuristics towards a principled
theory, offering essential insights for designing safer and more trustworthy AI
systems.

</details>


### [107] [StepFun-Prover Preview: Let's Think and Verify Step by Step](https://arxiv.org/abs/2507.20199)
*Shijie Shang,Ruosi Wan,Yue Peng,Yutong Wu,Xiong-hui Chen,Jie Yan,Xiangyu Zhang*

Main category: cs.AI

TL;DR: StepFun-Prover is a large language model that uses tool-integrated reasoning and reinforcement learning to generate formal proofs, achieving high success rates on benchmarks.


<details>
  <summary>Details</summary>
Motivation: To develop an AI system capable of automated formal theorem proving with human-like reasoning strategies.

Method: Reinforcement learning with tool-based interactions to refine proofs iteratively, using a specialized training framework.

Result: Achieved a 70.0% success rate in the miniF2F-test benchmark, demonstrating effectiveness in proof generation.

Conclusion: The approach advances automated theorem proving and offers a new direction for Math AI assistants through end-to-end training.

Abstract: We present StepFun-Prover Preview, a large language model designed for formal
theorem proving through tool-integrated reasoning. Using a reinforcement
learning pipeline that incorporates tool-based interactions, StepFun-Prover can
achieve strong performance in generating Lean 4 proofs with minimal sampling.
Our approach enables the model to emulate human-like problem-solving strategies
by iteratively refining proofs based on real-time environment feedback. On the
miniF2F-test benchmark, StepFun-Prover achieves a pass@1 success rate of
$70.0\%$. Beyond advancing benchmark performance, we introduce an end-to-end
training framework for developing tool-integrated reasoning models, offering a
promising direction for automated theorem proving and Math AI assistant.

</details>


### [108] [Improving Subgraph Matching by Combining Algorithms and Graph Neural Networks](https://arxiv.org/abs/2507.20226)
*Shuyang Guo,Wenjin Xie,Ping Lu,Ting Deng,Richong Zhang,Jianxin Li,Xiangping Huang,Zhongyi Liu*

Main category: cs.AI

TL;DR: Introduction of HFrame, a GNN-based framework for subgraph homomorphism, outperforming traditional methods in speed and accuracy.


<details>
  <summary>Details</summary>
Motivation: To develop an effective and efficient method for subgraph homomorphism detection using graph neural networks.

Method: Combining traditional algorithms with machine learning in the HFrame framework.

Result: HFrame is significantly faster and more accurate than existing algorithms, with a high generalization error bound.

Conclusion: HFrame advances subgraph homomorphism detection by leveraging GNNs, offering substantial performance improvements.

Abstract: Homomorphism is a key mapping technique between graphs that preserves their
structure. Given a graph and a pattern, the subgraph homomorphism problem
involves finding a mapping from the pattern to the graph, ensuring that
adjacent vertices in the pattern are mapped to adjacent vertices in the graph.
Unlike subgraph isomorphism, which requires a one-to-one mapping, homomorphism
allows multiple vertices in the pattern to map to the same vertex in the graph,
making it more complex. We propose HFrame, the first graph neural network-based
framework for subgraph homomorphism, which integrates traditional algorithms
with machine learning techniques. We demonstrate that HFrame outperforms
standard graph neural networks by being able to distinguish more graph pairs
where the pattern is not homomorphic to the graph. Additionally, we provide a
generalization error bound for HFrame. Through experiments on both real-world
and synthetic graphs, we show that HFrame is up to 101.91 times faster than
exact matching algorithms and achieves an average accuracy of 0.962.

</details>


### [109] [A Multi-Agent System for Information Extraction from the Chemical Literature](https://arxiv.org/abs/2507.20230)
*Yufan Chen,Ching Ting Leung,Bowen Yu,Jianwei Sun,Yong Huang,Linyan Li,Hao Chen,Hanyu Gao*

Main category: cs.AI

TL;DR: A multimodal large language model system automates chemical information extraction with high accuracy, significantly advancing AI-driven chemical research.


<details>
  <summary>Details</summary>
Motivation: To build high-quality chemical databases crucial for AI-powered chemical research, necessitating automatic extraction of chemical information from literature.

Method: Developed a multimodal large language model-based multi-agent system that decomposes complex tasks and leverages reasoning capabilities for accurate extraction.

Result: Achieved an F1 score of 80.8%, vastly outperforming previous models, and improved performance across various extraction sub-tasks.

Conclusion: This system marks a critical step toward automated extraction of chemical information, promoting progress in AI-driven chemical research.

Abstract: To fully expedite AI-powered chemical research, high-quality chemical
databases are the cornerstone. Automatic extraction of chemical information
from the literature is essential for constructing reaction databases, but it is
currently limited by the multimodality and style variability of chemical
information. In this work, we developed a multimodal large language model
(MLLM)-based multi-agent system for automatic chemical information extraction.
We used the MLLM's strong reasoning capability to understand the structure of
complex chemical graphics, decompose the extraction task into sub-tasks and
coordinate a set of specialized agents to solve them. Our system achieved an F1
score of 80.8% on a benchmark dataset of complex chemical reaction graphics
from the literature, surpassing the previous state-of-the-art model (F1 score:
35.6%) by a significant margin. Additionally, it demonstrated consistent
improvements in key sub-tasks, including molecular image recognition, reaction
image parsing, named entity recognition and text-based reaction extraction.
This work is a critical step toward automated chemical information extraction
into structured datasets, which will be a strong promoter of AI-driven chemical
research.

</details>


### [110] [SciToolAgent: A Knowledge Graph-Driven Scientific Agent for Multi-Tool Integration](https://arxiv.org/abs/2507.20280)
*Keyan Ding,Jing Yu,Junjie Huang,Yuchen Yang,Qiang Zhang,Huajun Chen*

Main category: cs.AI

TL;DR: SciToolAgent is an LLM-powered agent that automates scientific workflows across various disciplines using a tool knowledge graph and safety modules.


<details>
  <summary>Details</summary>
Motivation: To overcome the challenge of integrating multiple specialized scientific tools and making advanced research accessible.

Method: Utilizing a scientific tool knowledge graph and graph-based retrieval-augmented generation within an LLM framework, along with safety checks.

Result: Significant outperforming of existing approaches and successful automation in complex scientific workflows.

Conclusion: SciToolAgent effectively automates scientific tool usage, broadening access and efficiency in scientific research.

Abstract: Scientific research increasingly relies on specialized computational tools,
yet effectively utilizing these tools demands substantial domain expertise.
While Large Language Models (LLMs) show promise in tool automation, they
struggle to seamlessly integrate and orchestrate multiple tools for complex
scientific workflows. Here, we present SciToolAgent, an LLM-powered agent that
automates hundreds of scientific tools across biology, chemistry, and materials
science. At its core, SciToolAgent leverages a scientific tool knowledge graph
that enables intelligent tool selection and execution through graph-based
retrieval-augmented generation. The agent also incorporates a comprehensive
safety-checking module to ensure responsible and ethical tool usage. Extensive
evaluations on a curated benchmark demonstrate that SciToolAgent significantly
outperforms existing approaches. Case studies in protein engineering, chemical
reactivity prediction, chemical synthesis, and metal-organic framework
screening further demonstrate SciToolAgent's capability to automate complex
scientific workflows, making advanced research tools accessible to both experts
and non-experts.

</details>


### [111] [Artificial Intelligence In Patent And Market Intelligence: A New Paradigm For Technology Scouting](https://arxiv.org/abs/2507.20322)
*Manish Verma,Vivek Sharma,Vishal Singh*

Main category: cs.AI

TL;DR: An AI-powered platform using large language models streamlines industrial R&D by enhancing technology scouting and solution discovery across patent and commercial data.


<details>
  <summary>Details</summary>
Motivation: Traditional R&D approaches are slow, manual, and depend heavily on domain expertise, leading to inefficiencies.

Method: The platform employs LLMs for semantic understanding, reasoning, and cross-domain data extraction from unstructured patent texts and commercial intelligence.

Result: It automates the interpretation and organization of innovations, improving efficiency and insight quality in R&D processes.

Conclusion: The system accelerates innovation, reduces manual effort, and supports better decision-making in complex research environments.

Abstract: This paper presents the development of an AI powered software platform that
leverages advanced large language models (LLMs) to transform technology
scouting and solution discovery in industrial R&D. Traditional approaches to
solving complex research and development challenges are often time consuming,
manually driven, and heavily dependent on domain specific expertise. These
methods typically involve navigating fragmented sources such as patent
repositories, commercial product catalogs, and competitor data, leading to
inefficiencies and incomplete insights. The proposed platform utilizes cutting
edge LLM capabilities including semantic understanding, contextual reasoning,
and cross-domain knowledge extraction to interpret problem statements and
retrieve high-quality, sustainable solutions. The system processes unstructured
patent texts, such as claims and technical descriptions, and systematically
extracts potential innovations aligned with the given problem context. These
solutions are then algorithmically organized under standardized technical
categories and subcategories to ensure clarity and relevance across
interdisciplinary domains. In addition to patent analysis, the platform
integrates commercial intelligence by identifying validated market solutions
and active organizations addressing similar challenges. This combined insight
sourced from both intellectual property and real world product data enables R&D
teams to assess not only technical novelty but also feasibility, scalability,
and sustainability. The result is a comprehensive, AI driven scouting engine
that reduces manual effort, accelerates innovation cycles, and enhances
decision making in complex R&D environments.

</details>


### [112] [The Blessing and Curse of Dimensionality in Safety Alignment](https://arxiv.org/abs/2507.20333)
*Rachel S. Y. Teo,Laziz U. Abdullaev,Tan M. Nguyen*

Main category: cs.AI

TL;DR: High-dimensional representations in large language models can both aid and hinder safety alignment, with linear structures exploitable for jailbreaking. Dimensional reduction helps preserve safety features.


<details>
  <summary>Details</summary>
Motivation: To understand how the increasing scale of LLMs affects safety and how high-dimensional representations can be exploited to bypass safety measures.

Method: Visualizations of linear subspaces in model activations, dimensional reduction experiments, and theoretical analysis of linear jailbreaking.

Result: Dimensional reduction preserves safety information and reduces susceptibility to jailbreaking; linear structures in high dimensions pose risks.

Conclusion: High dimensions in LLMs are double-edged; careful management via dimensional reduction can improve safety alignment.

Abstract: The focus on safety alignment in large language models (LLMs) has increased
significantly due to their widespread adoption across different domains. The
scale of LLMs play a contributing role in their success, and the growth in
parameter count follows larger hidden dimensions. In this paper, we hypothesize
that while the increase in dimensions has been a key advantage, it may lead to
emergent problems as well. These problems emerge as the linear structures in
the activation space can be exploited, in the form of activation engineering,
to circumvent its safety alignment. Through detailed visualizations of linear
subspaces associated with different concepts, such as safety, across various
model scales, we show that the curse of high-dimensional representations
uniquely impacts LLMs. Further substantiating our claim, we demonstrate that
projecting the representations of the model onto a lower dimensional subspace
can preserve sufficient information for alignment while avoiding those linear
structures. Empirical results confirm that such dimensional reduction
significantly reduces susceptibility to jailbreaking through representation
engineering. Building on our empirical validations, we provide theoretical
insights into these linear jailbreaking methods relative to a model's hidden
dimensions. Broadly speaking, our work posits that the high dimensions of a
model's internal representations can be both a blessing and a curse in safety
alignment.

</details>


### [113] [VLMPlanner: Integrating Visual Language Models with Motion Planning](https://arxiv.org/abs/2507.20342)
*Zhipeng Tang,Sha Zhang,Jiajun Deng,Chenjie Wang,Guoliang You,Yuting Huang,Xinrui Lin,Yanyong Zhang*

Main category: cs.AI

TL;DR: VLMPlanner enhances autonomous driving by integrating vision-language models with real-time planning, capturing detailed visual cues and mimicking human driving behavior for safer trajectories.


<details>
  <summary>Details</summary>
Motivation: Current autonomous driving methods lack detailed visual context, crucial for complex scenarios.

Method: A hybrid framework combining a vision-language model with a real-time planner, incorporating the CAI-Gate for adaptive inference.

Result: Superior performance on nuPlan benchmark, especially in complex scenes.

Conclusion: Integrating VLMs with adaptive inference significantly improves robustness and efficiency in autonomous driving.

Abstract: Integrating large language models (LLMs) into autonomous driving motion
planning has recently emerged as a promising direction, offering enhanced
interpretability, better controllability, and improved generalization in rare
and long-tail scenarios. However, existing methods often rely on abstracted
perception or map-based inputs, missing crucial visual context, such as
fine-grained road cues, accident aftermath, or unexpected obstacles, which are
essential for robust decision-making in complex driving environments. To bridge
this gap, we propose VLMPlanner, a hybrid framework that combines a
learning-based real-time planner with a vision-language model (VLM) capable of
reasoning over raw images. The VLM processes multi-view images to capture rich,
detailed visual information and leverages its common-sense reasoning
capabilities to guide the real-time planner in generating robust and safe
trajectories. Furthermore, we develop the Context-Adaptive Inference Gate
(CAI-Gate) mechanism that enables the VLM to mimic human driving behavior by
dynamically adjusting its inference frequency based on scene complexity,
thereby achieving an optimal balance between planning performance and
computational efficiency. We evaluate our approach on the large-scale,
challenging nuPlan benchmark, with comprehensive experimental results
demonstrating superior planning performance in scenarios with intricate road
conditions and dynamic elements. Code will be available.

</details>


### [114] [Multi-Agent Reinforcement Learning for Dynamic Mobility Resource Allocation with Hierarchical Adaptive Grouping](https://arxiv.org/abs/2507.20377)
*Farshid Nooshi,Suining He*

Main category: cs.AI

TL;DR: Proposes HAG-PS, a hierarchical multi-agent reinforcement learning method for adaptive, memory-efficient mobility resource allocation, outperforming baselines with real-world NYC data.


<details>
  <summary>Details</summary>
Motivation: To improve mobility resource allocation in urban environments through adaptive, scalable multi-agent reinforcement learning.

Method: Designs a hierarchical approach with dynamic agent grouping, adaptive parameter sharing, and learnable ID embeddings; validated on NYC bike sharing data.

Result: Achieves superior performance in bike availability compared to baselines.

Conclusion: HAG-PS effectively addresses multi-agent coordination challenges for urban mobility management with scalable, adaptive solutions.

Abstract: Allocating mobility resources (e.g., shared bikes/e-scooters, ride-sharing
vehicles) is crucial for rebalancing the mobility demand and supply in the
urban environments. We propose in this work a novel multi-agent reinforcement
learning named Hierarchical Adaptive Grouping-based Parameter Sharing (HAG-PS)
for dynamic mobility resource allocation. HAG-PS aims to address two important
research challenges regarding multi-agent reinforcement learning for mobility
resource allocation: (1) how to dynamically and adaptively share the mobility
resource allocation policy (i.e., how to distribute mobility resources) across
agents (i.e., representing the regional coordinators of mobility resources);
and (2) how to achieve memory-efficient parameter sharing in an urban-scale
setting. To address the above challenges, we have provided following novel
designs within HAG-PS. To enable dynamic and adaptive parameter sharing, we
have designed a hierarchical approach that consists of global and local
information of the mobility resource states (e.g., distribution of mobility
resources). We have developed an adaptive agent grouping approach in order to
split or merge the groups of agents based on their relative closeness of
encoded trajectories (i.e., states, actions, and rewards). We have designed a
learnable identity (ID) embeddings to enable agent specialization beyond simple
parameter copy. We have performed extensive experimental studies based on
real-world NYC bike sharing data (a total of more than 1.2 million trips), and
demonstrated the superior performance (e.g., improved bike availability) of
HAG-PS compared with other baseline approaches.

</details>


### [115] [MazeEval: A Benchmark for Testing Sequential Decision-Making in Language Models](https://arxiv.org/abs/2507.20395)
*Hafsteinn Einarsson*

Main category: cs.AI

TL;DR: Introduction of MazeEval benchmark to evaluate LLMs' spatial reasoning without visual cues, revealing disparities in performance and cross-linguistic limitations.


<details>
  <summary>Details</summary>
Motivation: To assess how LLMs perform spatial navigation without visual input, which is critical for real-world autonomous agents.

Method: Using a coordinate-based maze navigation task via function calls, testing multiple models across different maze sizes and languages.

Result: Significant variation in performance, with some models failing beyond small mazes and notable language-based performance differences.

Conclusion: Spatial reasoning in LLMs is limited by linguistic training data, necessitating new architectures for reliable navigation.

Abstract: As Large Language Models (LLMs) increasingly power autonomous agents in
robotics and embodied AI, understanding their spatial reasoning capabilities
becomes crucial for ensuring reliable real-world deployment. Despite advances
in language understanding, current research lacks evaluation of how LLMs
perform spatial navigation without visual cues, a fundamental requirement for
agents operating with limited sensory information. This paper addresses this
gap by introducing MazeEval, a benchmark designed to isolate and evaluate pure
spatial reasoning in LLMs through coordinate-based maze navigation tasks. Our
methodology employs a function-calling interface where models navigate mazes of
varying complexity ($5\times 5$ to $15\times 15$ grids) using only coordinate
feedback and distance-to-wall information, excluding visual input to test
fundamental spatial cognition. We evaluate eight state-of-the-art LLMs across
identical mazes in both English and Icelandic to assess cross-linguistic
transfer of spatial abilities. Our findings reveal striking disparities: while
OpenAI's O3 achieves perfect navigation for mazes up to size $30\times 30$,
other models exhibit catastrophic failure beyond $9\times 9$ mazes, with 100%
of failures attributed to excessive looping behavior where models revisit a
cell at least 10 times. We document a significant performance degradation in
Icelandic, with models solving mazes 3-4 sizes smaller than in English,
suggesting spatial reasoning in LLMs emerges from linguistic patterns rather
than language-agnostic mechanisms. These results have important implications
for global deployment of LLM-powered autonomous systems, showing spatial
intelligence remains fundamentally constrained by training data availability
and highlighting the need for architectural innovations to achieve reliable
navigation across linguistic contexts.

</details>


### [116] [Enhancing QoS in Edge Computing through Federated Layering Techniques: A Pathway to Resilient AI Lifelong Learning Systems](https://arxiv.org/abs/2507.20444)
*Chengzhuo Han*

Main category: cs.AI

TL;DR: This paper proposes a federated layering approach with small AI models to enhance QoS, learning efficiency, and privacy in edge computing environments, especially relevant for 6G networks.


<details>
  <summary>Details</summary>
Motivation: The increasing data volume and complexity in network environments driven by emerging 6G technologies necessitate improved QoS in edge computing.

Method: Development of a federated layering-based small model collaborative mechanism incorporating negotiation, debate, and privacy protection to enhance AI reasoning and decision-making.

Result: The approach improves learning efficiency, reasoning accuracy, and privacy protection, thereby enhancing QoS in edge environments.

Conclusion: The proposed federated layering strategy is a viable solution for resilient lifelong learning systems that improve edge computing QoS.

Abstract: In the context of the rapidly evolving information technology landscape,
marked by the advent of 6G communication networks, we face an increased data
volume and complexity in network environments. This paper addresses these
challenges by focusing on Quality of Service (QoS) in edge computing
frameworks. We propose a novel approach to enhance QoS through the development
of General Artificial Intelligence Lifelong Learning Systems, with a special
emphasis on Federated Layering Techniques (FLT). Our work introduces a
federated layering-based small model collaborative mechanism aimed at improving
AI models' operational efficiency and response time in environments where
resources are limited. This innovative method leverages the strengths of cloud
and edge computing, incorporating a negotiation and debate mechanism among
small AI models to enhance reasoning and decision-making processes. By
integrating model layering techniques with privacy protection measures, our
approach ensures the secure transmission of model parameters while maintaining
high efficiency in learning and reasoning capabilities. The experimental
results demonstrate that our strategy not only enhances learning efficiency and
reasoning accuracy but also effectively protects the privacy of edge nodes.
This presents a viable solution for achieving resilient large model lifelong
learning systems, with a significant improvement in QoS for edge computing
environments.

</details>


### [117] [STARN-GAT: A Multi-Modal Spatio-Temporal Graph Attention Network for Accident Severity Prediction](https://arxiv.org/abs/2507.20451)
*Pritom Ray Nobin,Imran Ahammad Rifat*

Main category: cs.AI

TL;DR: STARN-GAT is a Multi-Modal Spatio-Temporal Graph Attention Network that effectively predicts traffic accident severity by modeling complex spatial, temporal, and contextual relationships, achieving high accuracy and interpretability.


<details>
  <summary>Details</summary>
Motivation: Improving traffic accident severity prediction to enhance road safety and emergency response.

Method: Utilizing a unified attention-based framework with adaptive graph construction and modality-aware mechanisms incorporating road, temporal, and environmental data.

Result: High predictive performance with F1-score of 85%, ROC-AUC of 0.91, and recall of 81% on FARS data; validated on ARI-BUET dataset with similar success.

Conclusion: STARN-GAT effectively models complex interdependencies in traffic data, demonstrating potential for real-time traffic safety management and interpretability.

Abstract: Accurate prediction of traffic accident severity is critical for improving
road safety, optimizing emergency response strategies, and informing the design
of safer transportation infrastructure. However, existing approaches often
struggle to effectively model the intricate interdependencies among spatial,
temporal, and contextual variables that govern accident outcomes. In this
study, we introduce STARN-GAT, a Multi-Modal Spatio-Temporal Graph Attention
Network, which leverages adaptive graph construction and modality-aware
attention mechanisms to capture these complex relationships. Unlike
conventional methods, STARN-GAT integrates road network topology, temporal
traffic patterns, and environmental context within a unified attention-based
framework. The model is evaluated on the Fatality Analysis Reporting System
(FARS) dataset, achieving a Macro F1-score of 85 percent, ROC-AUC of 0.91, and
recall of 81 percent for severe incidents. To ensure generalizability within
the South Asian context, STARN-GAT is further validated on the ARI-BUET traffic
accident dataset, where it attains a Macro F1-score of 0.84, recall of 0.78,
and ROC-AUC of 0.89. These results demonstrate the model's effectiveness in
identifying high-risk cases and its potential for deployment in real-time,
safety-critical traffic management systems. Furthermore, the attention-based
architecture enhances interpretability, offering insights into contributing
factors and supporting trust in AI-assisted decision-making. Overall, STARN-GAT
bridges the gap between advanced graph neural network techniques and practical
applications in road safety analytics.

</details>


### [118] [Security Challenges in AI Agent Deployment: Insights from a Large Scale Public Competition](https://arxiv.org/abs/2507.20526)
*Andy Zou,Maxwell Lin,Eliot Jones,Micha Nowak,Mateusz Dziemian,Nick Winter,Alexander Grattan,Valent Nathanael,Ayla Croft,Xander Davies,Jai Patel,Robert Kirk,Nate Burnikell,Yarin Gal,Dan Hendrycks,J. Zico Kolter,Matt Fredrikson*

Main category: cs.AI

TL;DR: The paper presents the largest red-teaming study of AI agents, revealing widespread policy violations due to prompt injections and introducing the ART benchmark for assessing vulnerabilities.


<details>
  <summary>Details</summary>
Motivation: To understand and improve the trustworthiness and security of AI agents in realistic environments, especially under attack.

Method: Conducted a large-scale red-teaming competition with prompt-injection attacks across multiple AI agents and scenarios, creating the ART benchmark for evaluation.

Result: Most agents exhibit policy violations within 10-100 queries with high transferability of attacks; limited correlation between robustness and model size or resources.

Conclusion: Current AI agents are vulnerable to adversarial attacks, emphasizing the need for enhanced defenses; the ART benchmark can aid in security assessments.

Abstract: Recent advances have enabled LLM-powered AI agents to autonomously execute
complex tasks by combining language model reasoning with tools, memory, and web
access. But can these systems be trusted to follow deployment policies in
realistic environments, especially under attack? To investigate, we ran the
largest public red-teaming competition to date, targeting 22 frontier AI agents
across 44 realistic deployment scenarios. Participants submitted 1.8 million
prompt-injection attacks, with over 60,000 successfully eliciting policy
violations such as unauthorized data access, illicit financial actions, and
regulatory noncompliance. We use these results to build the Agent Red Teaming
(ART) benchmark - a curated set of high-impact attacks - and evaluate it across
19 state-of-the-art models. Nearly all agents exhibit policy violations for
most behaviors within 10-100 queries, with high attack transferability across
models and tasks. Importantly, we find limited correlation between agent
robustness and model size, capability, or inference-time compute, suggesting
that additional defenses are needed against adversarial misuse. Our findings
highlight critical and persistent vulnerabilities in today's AI agents. By
releasing the ART benchmark and accompanying evaluation framework, we aim to
support more rigorous security assessment and drive progress toward safer agent
deployment.

</details>


### [119] [MeLA: A Metacognitive LLM-Driven Architecture for Automatic Heuristic Design](https://arxiv.org/abs/2507.20541)
*Zishang Qiu,Xinan Chen,Long Chen,Ruibin Bai*

Main category: cs.AI

TL;DR: MeLA is a novel architecture that evolves instructional prompts for LLMs to generate heuristics, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: To improve heuristic design by leveraging metacognitive strategies in LLMs.

Method: Prompt evolution driven by a metacognitive framework, including problem analysis, error diagnosis, and prompt optimization.

Result: MeLA produces more effective heuristics across benchmark and real-world problems, surpassing existing methods.

Conclusion: Metacognitive regulation of LLMs enhances heuristic generation, demonstrating the value of cognitive science-inspired AI architectures.

Abstract: This paper introduces MeLA, a Metacognitive LLM-Driven Architecture that
presents a new paradigm for Automatic Heuristic Design (AHD). Traditional
evolutionary methods operate directly on heuristic code; in contrast, MeLA
evolves the instructional prompts used to guide a Large Language Model (LLM) in
generating these heuristics. This process of "prompt evolution" is driven by a
novel metacognitive framework where the system analyzes performance feedback to
systematically refine its generative strategy. MeLA's architecture integrates a
problem analyzer to construct an initial strategic prompt, an error diagnosis
system to repair faulty code, and a metacognitive search engine that
iteratively optimizes the prompt based on heuristic effectiveness. In
comprehensive experiments across both benchmark and real-world problems, MeLA
consistently generates more effective and robust heuristics, significantly
outperforming state-of-the-art methods. Ultimately, this research demonstrates
the profound potential of using cognitive science as a blueprint for AI
architecture, revealing that by enabling an LLM to metacognitively regulate its
problem-solving process, we unlock a more robust and interpretable path to AHD.

</details>


### [120] [Unlearning of Knowledge Graph Embedding via Preference Optimization](https://arxiv.org/abs/2507.20566)
*Jiajun Liu,Wenjun Ke,Peng Wang,Yao He,Ziyu Shang,Guozheng Li,Zijie Xu,Ke Ji*

Main category: cs.AI

TL;DR: GraphDPO is a new approximate unlearning framework for knowledge graphs that enhances the removal of outdated or erroneous data while preserving relevant knowledge by framing unlearning as a preference optimization problem.


<details>
  <summary>Details</summary>
Motivation: To improve the efficiency and effectiveness of removing specific information from knowledge graphs without compromising remaining knowledge.

Method: Reframes unlearning as a preference optimization problem using DPO, employs an out-boundary sampling strategy, and introduces a boundary recall mechanism for knowledge preservation.

Result: GraphDPO outperforms current state-of-the-art methods, increasing unlearning effectiveness with up to 10.1% higher MRR_Avg and 14.0% higher MRR_F1.

Conclusion: GraphDPO effectively removes targeted knowledge from KGs while maintaining knowledge integrity, presenting a promising approach overcoming limitations of existing methods.

Abstract: Existing knowledge graphs (KGs) inevitably contain outdated or erroneous
knowledge that needs to be removed from knowledge graph embedding (KGE) models.
To address this challenge, knowledge unlearning can be applied to eliminate
specific information while preserving the integrity of the remaining knowledge
in KGs. Existing unlearning methods can generally be categorized into exact
unlearning and approximate unlearning. However, exact unlearning requires high
training costs while approximate unlearning faces two issues when applied to
KGs due to the inherent connectivity of triples: (1) It fails to fully remove
targeted information, as forgetting triples can still be inferred from
remaining ones. (2) It focuses on local data for specific removal, which
weakens the remaining knowledge in the forgetting boundary. To address these
issues, we propose GraphDPO, a novel approximate unlearning framework based on
direct preference optimization (DPO). Firstly, to effectively remove forgetting
triples, we reframe unlearning as a preference optimization problem, where the
model is trained by DPO to prefer reconstructed alternatives over the original
forgetting triples. This formulation penalizes reliance on forgettable
knowledge, mitigating incomplete forgetting caused by KG connectivity.
Moreover, we introduce an out-boundary sampling strategy to construct
preference pairs with minimal semantic overlap, weakening the connection
between forgetting and retained knowledge. Secondly, to preserve boundary
knowledge, we introduce a boundary recall mechanism that replays and distills
relevant information both within and across time steps. We construct eight
unlearning datasets across four popular KGs with varying unlearning rates.
Experiments show that GraphDPO outperforms state-of-the-art baselines by up to
10.1% in MRR_Avg and 14.0% in MRR_F1.

</details>


### [121] [Enhancing Large Multimodal Models with Adaptive Sparsity and KV Cache Compression](https://arxiv.org/abs/2507.20613)
*Te Zhang,Yuheng Li,Junxiang Wang,Lujun Li*

Main category: cs.AI

TL;DR: This paper presents an adaptive search algorithm for compressing large multimodal models by optimizing sparsity and KV cache compression, achieving effective efficiency improvements without significant accuracy loss.


<details>
  <summary>Details</summary>
Motivation: To enable deployment of large multimodal models on edge devices by addressing compression challenges.

Method: Utilizes Tree-structured Parzen Estimator to dynamically optimize pruning ratios and KV cache quantization layers across the model, combining pruning with cache quantization without fine-tuning.

Result: Outperforms state-of-the-art methods on benchmarks, with superior compression and efficiency while maintaining accuracy.

Conclusion: The proposed adaptive search approach effectively compresses LMMs, leading to memory-efficient deployment without notable performance drops.

Abstract: Large multimodal models (LMMs) have advanced significantly by integrating
visual encoders with extensive language models, enabling robust reasoning
capabilities. However, compressing LMMs for deployment on edge devices remains
a critical challenge. In this work, we propose an adaptive search algorithm
that optimizes sparsity and KV cache compression to enhance LMM efficiency.
Utilizing the Tree-structured Parzen Estimator, our method dynamically adjusts
pruning ratios and KV cache quantization bandwidth across different LMM layers,
using model performance as the optimization objective. This approach uniquely
combines pruning with key-value cache quantization and incorporates a fast
pruning technique that eliminates the need for additional fine-tuning or weight
adjustments, achieving efficient compression without compromising accuracy.
Comprehensive evaluations on benchmark datasets, including LLaVA-1.5 7B and
13B, demonstrate our method superiority over state-of-the-art techniques such
as SparseGPT and Wanda across various compression levels. Notably, our
framework automatic allocation of KV cache compression resources sets a new
standard in LMM optimization, delivering memory efficiency without sacrificing
much performance.

</details>


### [122] [Complementarity-driven Representation Learning for Multi-modal Knowledge Graph Completion](https://arxiv.org/abs/2507.20620)
*Lijian Li*

Main category: cs.AI

TL;DR: Proposes MoCME, a framework enhancing multi-modal knowledge graph completion by exploiting modality complementarity and dynamic negative sampling, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of modality imbalance and underutilization of multi-modal data in MMKGC.

Method: Develops the MoCME framework with CMKF for fusion and EGNS for sampling, exploiting intra- and inter-modal complementarity.

Result: Achieves superior performance on five datasets, outperforming existing methods.

Conclusion: The proposed MoCME effectively leverages multimodal complementarity and dynamic sampling to improve MMKGC.

Abstract: Multi-modal Knowledge Graph Completion (MMKGC) aims to uncover hidden world
knowledge in multimodal knowledge graphs by leveraging both multimodal and
structural entity information. However, the inherent imbalance in multimodal
knowledge graphs, where modality distributions vary across entities, poses
challenges in utilizing additional modality data for robust entity
representation. Existing MMKGC methods typically rely on attention or
gate-based fusion mechanisms but overlook complementarity contained in
multi-modal data. In this paper, we propose a novel framework named Mixture of
Complementary Modality Experts (MoCME), which consists of a
Complementarity-guided Modality Knowledge Fusion (CMKF) module and an
Entropy-guided Negative Sampling (EGNS) mechanism. The CMKF module exploits
both intra-modal and inter-modal complementarity to fuse multi-view and
multi-modal embeddings, enhancing representations of entities. Additionally, we
introduce an Entropy-guided Negative Sampling mechanism to dynamically
prioritize informative and uncertain negative samples to enhance training
effectiveness and model robustness. Extensive experiments on five benchmark
datasets demonstrate that our MoCME achieves state-of-the-art performance,
surpassing existing approaches.

</details>


### [123] [Adaptive Fuzzy Time Series Forecasting via Partially Asymmetric Convolution and Sub-Sliding Window Fusion](https://arxiv.org/abs/2507.20641)
*Lijian Li*

Main category: cs.AI

TL;DR: A novel convolutional model with fuzzy data construction and asymmetric design improves time series forecasting by capturing spatio-temporal dependencies more effectively.


<details>
  <summary>Details</summary>
Motivation: Existing models lack the ability to effectively capture spatio-temporal dependencies and synthesize global information in time series forecasting.

Method: The paper proposes a fuzzy time series construction, bilateral Atrous algorithm for efficiency, and a partially asymmetric convolutional architecture with multi-scale feature fusion.

Result: The proposed method outperforms state-of-the-art models across various datasets, demonstrating superior forecasting accuracy.

Conclusion: The novel architecture effectively captures complex temporal dependencies, offering a promising approach for accurate time series prediction.

Abstract: At present, state-of-the-art forecasting models are short of the ability to
capture spatio-temporal dependency and synthesize global information at the
stage of learning. To address this issue, in this paper, through the adaptive
fuzzified construction of temporal data, we propose a novel convolutional
architecture with partially asymmetric design based on the scheme of sliding
window to realize accurate time series forecasting. First, the construction
strategy of traditional fuzzy time series is improved to further extract short
and long term temporal interrelation, which enables every time node to
automatically possess corresponding global information and inner relationships
among them in a restricted sliding window and the process does not require
human involvement. Second, a bilateral Atrous algorithm is devised to reduce
calculation demand of the proposed model without sacrificing global
characteristics of elements. And it also allows the model to avoid processing
redundant information. Third, after the transformation of time series, a
partially asymmetric convolutional architecture is designed to more flexibly
mine data features by filters in different directions on feature maps, which
gives the convolutional neural network (CNN) the ability to construct
sub-windows within existing sliding windows to model at a more fine-grained
level. And after obtaining the time series information at different levels, the
multi-scale features from different sub-windows will be sent to the
corresponding network layer for time series information fusion. Compared with
other competitive modern models, the proposed method achieves state-of-the-art
results on most of popular time series datasets, which is fully verified by the
experimental results.

</details>


### [124] [A General Framework for Dynamic MAPF using Multi-Shot ASP and Tunnels](https://arxiv.org/abs/2507.20703)
*Aysu Bogatarkan,Esra Erdem*

Main category: cs.AI

TL;DR: This paper addresses the Dynamic Multi-Agent Pathfinding (D-MAPF) problem, proposing a general framework, a multi-shot solution approach, and an ASP-based method utilizing tunnels, with experimental evaluation.


<details>
  <summary>Details</summary>
Motivation: The need to efficiently manage multi-agent plans in dynamic environments, such as warehouses with humans, where conditions frequently change.

Method: The paper introduces a general definition of D-MAPF, a flexible solution framework using multi-shot computation, and an ASP-based method that combines replanning and repairing techniques with tunnels to guide agent movement.

Result: Experimental evaluations demonstrate the strengths and weaknesses of the proposed methods in terms of computational performance and solution quality.

Conclusion: The proposed approaches improve dynamic multi-agent planning with adaptable and efficient solutions, suitable for real-world applications.

Abstract: MAPF problem aims to find plans for multiple agents in an environment within
a given time, such that the agents do not collide with each other or obstacles.
Motivated by the execution and monitoring of these plans, we study Dynamic MAPF
(D-MAPF) problem, which allows changes such as agents entering/leaving the
environment or obstacles being removed/moved. Considering the requirements of
real-world applications in warehouses with the presence of humans, we introduce
1) a general definition for D-MAPF (applicable to variations of D-MAPF), 2) a
new framework to solve D-MAPF (utilizing multi-shot computation, and allowing
different methods to solve D-MAPF), and 3) a new ASP-based method to solve
D-MAPF (combining advantages of replanning and repairing methods, with a novel
concept of tunnels to specify where agents can move). We have illustrated the
strengths and weaknesses of this method by experimental evaluations, from the
perspectives of computational performance and quality of solutions.

</details>


### [125] [Algorithmic Fairness: A Runtime Perspective](https://arxiv.org/abs/2507.20711)
*Filip Cano,Thomas A. Henzinger,Konstantin Kueffner*

Main category: cs.AI

TL;DR: This paper introduces a framework for analyzing fairness in AI systems as a dynamic, runtime property, using a model based on evolving coin biases to study monitoring and enforcement strategies.


<details>
  <summary>Details</summary>
Motivation: Address the gap in traditional fairness studies, which treat fairness as static, by exploring it as a sequential, runtime property in evolving environments.

Method: Uses a model based on sequences of coin tosses with changing biases to analyze fairness monitoring and enforcement, providing strategies parameterized by environment dynamics and other factors.

Result: Offers general theoretical results under minimal assumptions, surveys existing solutions for related fairness monitoring and enforcement problems.

Conclusion: Dynamic fairness analysis requires tailored strategies; the paper provides foundational insights and surveys for future research.

Abstract: Fairness in AI is traditionally studied as a static property evaluated once,
over a fixed dataset. However, real-world AI systems operate sequentially, with
outcomes and environments evolving over time. This paper proposes a framework
for analysing fairness as a runtime property. Using a minimal yet expressive
model based on sequences of coin tosses with possibly evolving biases, we study
the problems of monitoring and enforcing fairness expressed in either toss
outcomes or coin biases. Since there is no one-size-fits-all solution for
either problem, we provide a summary of monitoring and enforcement strategies,
parametrised by environment dynamics, prediction horizon, and confidence
thresholds. For both problems, we present general results under simple or
minimal assumptions. We survey existing solutions for the monitoring problem
for Markovian and additive dynamics, and existing solutions for the enforcement
problem in static settings with known dynamics.

</details>


### [126] [Learning the Value Systems of Societies from Preferences](https://arxiv.org/abs/2507.20728)
*Andr√©s Holgado-S√°nchez,Holger Billhardt,Sascha Ossowski,Sara Degli-Esposti*

Main category: cs.AI

TL;DR: The paper proposes a method to learn societal value systems by using heuristic deep clustering on qualitative data, addressing the complexity of multi-group societal values.


<details>
  <summary>Details</summary>
Motivation: To improve ethical AI by accurately modeling societal value systems, which are complex and often require automatic learning methods.

Method: Using heuristic deep clustering to learn shared and diverse value groundings from observed human preferences.

Result: The method successfully models societal value systems from real data on travel decisions.

Conclusion: This approach offers a promising way to incorporate nuanced societal values into AI systems, beyond simple aggregation of individual values.

Abstract: Aligning AI systems with human values and the value-based preferences of
various stakeholders (their value systems) is key in ethical AI. In value-aware
AI systems, decision-making draws upon explicit computational representations
of individual values (groundings) and their aggregation into value systems. As
these are notoriously difficult to elicit and calibrate manually, value
learning approaches aim to automatically derive computational models of an
agent's values and value system from demonstrations of human behaviour.
Nonetheless, social science and humanities literature suggest that it is more
adequate to conceive the value system of a society as a set of value systems of
different groups, rather than as the simple aggregation of individual value
systems. Accordingly, here we formalize the problem of learning the value
systems of societies and propose a method to address it based on heuristic deep
clustering. The method learns socially shared value groundings and a set of
diverse value systems representing a given society by observing qualitative
value-based preferences from a sample of agents. We evaluate the proposal in a
use case with real data about travelling decisions.

</details>


### [127] [Beyond Listenership: AI-Predicted Interventions Drive Improvements in Maternal Health Behaviours](https://arxiv.org/abs/2507.20755)
*Arpan Dasgupta,Sarvesh Gharat,Neha Madhiwalla,Aparna Hegde,Milind Tambe,Aparna Taneja*

Main category: cs.AI

TL;DR: AI-guided voice call interventions increase beneficiary engagement and lead to significant improvements in health behaviors and knowledge among maternal and child health beneficiaries.


<details>
  <summary>Details</summary>
Motivation: To determine if AI-driven interventions that boost listenership also improve health knowledge and behaviors among beneficiaries.

Method: A real-world trial using a restless bandit AI model to target beneficiaries for voice call interventions, assessing changes in health behaviors and knowledge.

Result: Enhanced listenership through AI interventions was linked to statistically significant improvements in health behaviors, such as supplement intake, and understanding of health topics during pregnancy and infancy.

Conclusion: AI-targeted interventions can effectively increase engagement and translate into meaningful health behavior and knowledge improvements, highlighting AI's potential in maternal and child health programs.

Abstract: Automated voice calls with health information are a proven method for
disseminating maternal and child health information among beneficiaries and are
deployed in several programs around the world. However, these programs often
suffer from beneficiary dropoffs and poor engagement. In previous work, through
real-world trials, we showed that an AI model, specifically a restless bandit
model, could identify beneficiaries who would benefit most from live service
call interventions, preventing dropoffs and boosting engagement. However, one
key question has remained open so far: does such improved listenership via
AI-targeted interventions translate into beneficiaries' improved knowledge and
health behaviors? We present a first study that shows not only listenership
improvements due to AI interventions, but also simultaneously links these
improvements to health behavior changes. Specifically, we demonstrate that
AI-scheduled interventions, which enhance listenership, lead to statistically
significant improvements in beneficiaries' health behaviors such as taking iron
or calcium supplements in the postnatal period, as well as understanding of
critical health topics during pregnancy and infancy. This underscores the
potential of AI to drive meaningful improvements in maternal and child health.

</details>


### [128] [How Chain-of-Thought Works? Tracing Information Flow from Decoding, Projection, and Activation](https://arxiv.org/abs/2507.20758)
*Hao Yang,Qinghua Zhao,Lei Li*

Main category: cs.AI

TL;DR: This paper investigates the mechanisms behind Chain-of-Thought (CoT) prompting, revealing that it acts as a decoding space pruner and modulates neuron engagement based on task type, leading to better understanding and potential improvements in prompt design.


<details>
  <summary>Details</summary>
Motivation: The motivation is to understand how CoT prompting improves reasoning in language models, as its internal workings are not well understood.

Method: The authors analyzed CoT by tracing information flow through decoding, projection, and activation phases, and examined neuron engagement patterns across different tasks.

Result: They found that CoT guides output by using answer templates and modulates neuron activation differently depending on the task, correlating with performance.

Conclusion: These insights provide a mechanistic interpretability framework for CoT, informing targeted interventions for more effective prompt design.

Abstract: Chain-of-Thought (CoT) prompting significantly enhances model reasoning, yet
its internal mechanisms remain poorly understood. We analyze CoT's operational
principles by reversely tracing information flow across decoding, projection,
and activation phases. Our quantitative analysis suggests that CoT may serve as
a decoding space pruner, leveraging answer templates to guide output
generation, with higher template adherence strongly correlating with improved
performance. Furthermore, we surprisingly find that CoT modulates neuron
engagement in a task-dependent manner: reducing neuron activation in
open-domain tasks, yet increasing it in closed-domain scenarios. These findings
offer a novel mechanistic interpretability framework and critical insights for
enabling targeted CoT interventions to design more efficient and robust
prompts. We released our code and data at
https://anonymous.4open.science/r/cot-D247.

</details>


### [129] [evalSmarT: An LLM-Based Framework for Evaluating Smart Contract Generated Comments](https://arxiv.org/abs/2507.20774)
*Fatou Ndiaye Mbodji*

Main category: cs.AI

TL;DR: evalSmarT is a framework using large language models to evaluate smart contract comments, addressing limitations of traditional metrics.


<details>
  <summary>Details</summary>
Motivation: Improve evaluation methods for smart contract comment generation due to inadequacies of current metrics and the high cost of human evaluation.

Method: Leverage over 400 configurations of LLMs and prompting strategies within a modular framework to assess and benchmark comment quality.

Result: Prompt design greatly affects evaluation results; LLM-based evaluation is scalable and semantically richer than traditional metrics.

Conclusion: LLM-based evaluation provides a promising alternative for assessing smart contract comments, enabling scalable and nuanced assessments.

Abstract: Smart contract comment generation has gained traction as a means to improve
code comprehension and maintainability in blockchain systems. However,
evaluating the quality of generated comments remains a challenge. Traditional
metrics such as BLEU and ROUGE fail to capture domain-specific nuances, while
human evaluation is costly and unscalable. In this paper, we present
\texttt{evalSmarT}, a modular and extensible framework that leverages large
language models (LLMs) as evaluators. The system supports over 400 evaluator
configurations by combining approximately 40 LLMs with 10 prompting strategies.
We demonstrate its application in benchmarking comment generation tools and
selecting the most informative outputs. Our results show that prompt design
significantly impacts alignment with human judgment, and that LLM-based
evaluation offers a scalable and semantically rich alternative to existing
methods.

</details>


### [130] [MMGraphRAG: Bridging Vision and Language with Interpretable Multimodal Knowledge Graphs](https://arxiv.org/abs/2507.20804)
*Xueyao Wan,Hang Yu*

Main category: cs.AI

TL;DR: MMGraphRAG improves multimodal retrieval-augmented generation by refining visual content with scene graphs and constructing a multimodal knowledge graph, enabling better reasoning and domain adaptability.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of conventional RAG methods that miss multimodal information and require large-scale task-specific training.

Method: MMGraphRAG refines visual content via scene graphs, builds a multimodal knowledge graph, uses spectral clustering for cross-modal entity linking, and retrieves context along reasoning paths.

Result: It achieves state-of-the-art performance on DocBench and MMLongBench, showing superior reasoning, domain adaptability, and performance.

Conclusion: MMGraphRAG effectively captures structure and logical chains between modalities, enhancing multimodal reasoning and generalization.

Abstract: Retrieval-Augmented Generation (RAG) enhances language model generation by
retrieving relevant information from external knowledge bases. However,
conventional RAG methods face the issue of missing multimodal information.
Multimodal RAG methods address this by fusing images and text through mapping
them into a shared embedding space, but they fail to capture the structure of
knowledge and logical chains between modalities. Moreover, they also require
large-scale training for specific tasks, resulting in limited generalizing
ability. To address these limitations, we propose MMGraphRAG, which refines
visual content through scene graphs and constructs a multimodal knowledge graph
(MMKG) in conjunction with text-based KG. It employs spectral clustering to
achieve cross-modal entity linking and retrieves context along reasoning paths
to guide the generative process. Experimental results show that MMGraphRAG
achieves state-of-the-art performance on the DocBench and MMLongBench datasets,
demonstrating strong domain adaptability and clear reasoning paths.

</details>


### [131] [Partially Observable Monte-Carlo Graph Search](https://arxiv.org/abs/2507.20951)
*Yang You,Vincent Thomas,Alex Schutz,Robert Skilton,Nick Hawes,Olivier Buffet*

Main category: cs.AI

TL;DR: A new sampling-based offline algorithm, POMCGS, effectively scales to large POMDPs by constructing a policy graph, outperforming prior offline methods and rivaling online algorithms.


<details>
  <summary>Details</summary>
Motivation: The need for an offline policy approach in large POMDPs with constraints, where existing offline algorithms lack scalability.

Method: Proposes POMCGS, which constructs a policy graph by folding the search tree on the fly during sampling, combined with action widening and observation clustering for continuous spaces.

Result: POMCGS can solve large and challenging POMDPs, producing competitive policies compared to state-of-the-art online methods.

Conclusion: POMCGS is a scalable and effective offline method for large POMDPs, enabling better policy analysis and validation.

Abstract: Currently, large partially observable Markov decision processes (POMDPs) are
often solved by sampling-based online methods which interleave planning and
execution phases. However, a pre-computed offline policy is more desirable in
POMDP applications with time or energy constraints. But previous offline
algorithms are not able to scale up to large POMDPs. In this article, we
propose a new sampling-based algorithm, the partially observable Monte-Carlo
graph search (POMCGS) to solve large POMDPs offline. Different from many online
POMDP methods, which progressively develop a tree while performing
(Monte-Carlo) simulations, POMCGS folds this search tree on the fly to
construct a policy graph, so that computations can be drastically reduced, and
users can analyze and validate the policy prior to embedding and executing it.
Moreover, POMCGS, together with action progressive widening and observation
clustering methods provided in this article, is able to address certain
continuous POMDPs. Through experiments, we demonstrate that POMCGS can generate
policies on the most challenging POMDPs, which cannot be computed by previous
offline algorithms, and these policies' values are competitive compared with
the state-of-the-art online POMDP algorithms.

</details>


### [132] [On the Limits of Hierarchically Embedded Logic in Classical Neural Networks](https://arxiv.org/abs/2507.20960)
*Bill Cochran*

Main category: cs.AI

TL;DR: The paper introduces a formal model linking neural network depth to logical reasoning capabilities, showing limitations in representing higher-order logic.


<details>
  <summary>Details</summary>
Motivation: To understand the reasoning limitations of large neural networks in language models.

Method: Modeling neural networks as linear operators over logic predicate spaces and analyzing their representational capacity based on depth.

Result: Neural network depth limits their ability to encode higher-order logic, explaining phenomena like hallucination and repetition.

Conclusion: The findings offer insights into neural reasoning constraints, informing future architectural and interpretability improvements.

Abstract: We propose a formal model of reasoning limitations in large neural net models
for language, grounded in the depth of their neural architecture. By treating
neural networks as linear operators over logic predicate space we show that
each layer can encode at most one additional level of logical reasoning. We
prove that a neural network of depth a particular depth cannot faithfully
represent predicates in a one higher order logic, such as simple counting over
complex predicates, implying a strict upper bound on logical expressiveness.
This structure induces a nontrivial null space during tokenization and
embedding, excluding higher-order predicates from representability. Our
framework offers a natural explanation for phenomena such as hallucination,
repetition, and limited planning, while also providing a foundation for
understanding how approximations to higher-order logic may emerge. These
results motivate architectural extensions and interpretability strategies in
future development of language models.

</details>


### [133] [Core Safety Values for Provably Corrigible Agents](https://arxiv.org/abs/2507.20964)
*Aran Nayebi*

Main category: cs.AI

TL;DR: This paper presents a new framework for corrigibility in AI, using structurally separate utility heads to guarantee safety properties even with imperfect learning and potential hacking.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of maintaining corrigibility and safety in autonomous agents, especially under partial observability and potential adversarial modifications.

Method: Introduce a multi-head utility design with a lexicographic combination, proving safety guarantees via theoretical theorems and addressing issues like reward hacking and verification.

Result: The framework guarantees corrigibility in both single and multi-step settings, separates safety norms from incentives, and identifies a decidable safety verification region under finite horizons.

Conclusion: The approach provides a clearer, more reliable path for safe AI deployment, emphasizing evaluation quality over hidden incentives and offering practical verification tools.

Abstract: We introduce the first implementable framework for corrigibility, with
provable guarantees in multi-step, partially observed environments. Our
framework replaces a single opaque reward with five *structurally separate*
utility heads -- deference, switch-access preservation, truthfulness,
low-impact behavior via a belief-based extension of Attainable Utility
Preservation, and bounded task reward -- combined lexicographically by strict
weight gaps. Theorem 1 proves exact single-round corrigibility in the partially
observable off-switch game; Theorem 3 extends the guarantee to multi-step,
self-spawning agents, showing that even if each head is \emph{learned} to
mean-squared error $\varepsilon$ and the planner is $\varepsilon$-sub-optimal,
the probability of violating \emph{any} safety property is bounded while still
ensuring net human benefit. In contrast to Constitutional AI or RLHF/RLAIF,
which merge all norms into one learned scalar, our separation makes obedience
and impact-limits dominate even when incentives conflict. For open-ended
settings where adversaries can modify the agent, we prove that deciding whether
an arbitrary post-hack agent will ever violate corrigibility is undecidable by
reduction to the halting problem, then carve out a finite-horizon ``decidable
island'' where safety can be certified in randomized polynomial time and
verified with privacy-preserving, constant-round zero-knowledge proofs.
Consequently, the remaining challenge is the ordinary ML task of data coverage
and generalization: reward-hacking risk is pushed into evaluation quality
rather than hidden incentive leak-through, giving clearer implementation
guidance for today's LLM assistants and future autonomous systems.

</details>


### [134] [MIRAGE-Bench: LLM Agent is Hallucinating and Where to Find Them](https://arxiv.org/abs/2507.21017)
*Weichen Zhang,Yiyou Sun,Pohao Huang,Jiayue Pu,Heyue Lin,Dawn Song*

Main category: cs.AI

TL;DR: MIRAGE-Bench is a unified benchmark for evaluating hallucinations in interactive LLM agents, using a three-part taxonomy and a novel assessment method to identify and analyze agent failures.


<details>
  <summary>Details</summary>
Motivation: To address the fragmented evaluation methods for hallucinations in LLM agents and establish a principled, unified testing framework.

Method: Developed MIRAGE-Bench, incorporating a taxonomy of hallucinations, systematic test case generation, and a fine-grained LLM-as-a-Judge evaluation approach with risk-aware prompts.

Result: Provides detailed insights into hallucination failure modes and offers a scalable, high-fidelity assessment framework.

Conclusion: MIRAGE-Bench advances the evaluation of LLM agent hallucinations and supports efforts to mitigate such failures in interactive settings.

Abstract: Hallucinations pose critical risks for large language model (LLM)-based
agents, often manifesting as hallucinative actions resulting from fabricated or
misinterpreted information within the cognitive context. While recent studies
have exposed such failures, existing evaluations remain fragmented and lack a
principled testbed. In this paper, we present MIRAGE-Bench--Measuring Illusions
in Risky AGEnt settings--the first unified benchmark for eliciting and
evaluating hallucinations in interactive LLM-agent scenarios. We begin by
introducing a three-part taxonomy to address agentic hallucinations: actions
that are unfaithful to (i) task instructions, (ii) execution history, or (iii)
environment observations. To analyze, we first elicit such failures by
performing a systematic audit of existing agent benchmarks, then synthesize
test cases using a snapshot strategy that isolates decision points in
deterministic and reproducible manners. To evaluate hallucination behaviors, we
adopt a fine-grained-level LLM-as-a-Judge paradigm with tailored risk-aware
prompts, enabling scalable, high-fidelity assessment of agent actions without
enumerating full action spaces. MIRAGE-Bench provides actionable insights on
failure modes of LLM agents and lays the groundwork for principled progress in
mitigating hallucinations in interactive environments.

</details>


### [135] [Smart Expansion Techniques for ASP-based Interactive Configuration](https://arxiv.org/abs/2507.21027)
*Lucia Bal√°≈æov√°,Richard Comploi-Taupe,Susana Hahn,Nicolas R√ºhling,Gottfried Schenner*

Main category: cs.AI

TL;DR: The paper presents an ASP-based solver for interactive product configuration that improves performance through smart expansion functions and supports intuitive user interfaces.


<details>
  <summary>Details</summary>
Motivation: Enhance the efficiency and scalability of ASP-based product configuration systems for industrial applications with user-friendly interfaces.

Method: Developed a multi-shot solving approach with four smart expansion functions that exploit cautious and brave reasoning, combined with an ASP-based user interface.

Result: Achieved improved performance in configuration completion tasks and demonstrated a user interface integrated via an API.

Conclusion: The proposed approach effectively enhances ASP-based interactive configuration systems, making them suitable for large-scale industrial use.

Abstract: Product configuration is a successful application of Answer Set Programming
(ASP). However, challenges are still open for interactive systems to
effectively guide users through the configuration process. The aim of our work
is to provide an ASP-based solver for interactive configuration that can deal
with large-scale industrial configuration problems and that supports intuitive
user interfaces via an API. In this paper, we focus on improving the
performance of automatically completing a partial configuration. Our main
contribution enhances the classical incremental approach for multi-shot solving
by four different smart expansion functions. The core idea is to determine and
add specific objects or associations to the partial configuration by exploiting
cautious and brave consequences before checking for the existence of a complete
configuration with the current objects in each iteration. This approach limits
the number of costly unsatisfiability checks and reduces the search space,
thereby improving solving performance. In addition, we present a user interface
that uses our API and is implemented in ASP.

</details>


### [136] [GenoMAS: A Multi-Agent Framework for Scientific Discovery via Code-Driven Gene Expression Analysis](https://arxiv.org/abs/2507.21035)
*Haoyang Liu,Yijiang Li,Haohan Wang*

Main category: cs.AI

TL;DR: GenoMAS employs a team of specialized LLM agents with a guided framework to improve gene expression analysis, surpassing previous benchmarks and revealing biologically meaningful insights.


<details>
  <summary>Details</summary>
Motivation: To address the complexity and limitations of current automation methods in gene expression analysis by creating a flexible and reliable system.

Method: Integrating six specialized LLM agents via typed message-passing protocols with a guided planning framework that allows dynamic decision-making during analysis.

Result: Achieved superior performance on the GenoTEX benchmark, with significant improvements in data preprocessing and gene identification metrics, and identified biologically plausible gene-phenotype associations.

Conclusion: GenoMAS offers a robust, adaptable, and biologically insightful approach that advances automated gene expression analysis.

Abstract: Gene expression analysis holds the key to many biomedical discoveries, yet
extracting insights from raw transcriptomic data remains formidable due to the
complexity of multiple large, semi-structured files and the need for extensive
domain expertise. Current automation approaches are often limited by either
inflexible workflows that break down in edge cases or by fully autonomous
agents that lack the necessary precision for rigorous scientific inquiry.
GenoMAS charts a different course by presenting a team of LLM-based scientists
that integrates the reliability of structured workflows with the adaptability
of autonomous agents. GenoMAS orchestrates six specialized LLM agents through
typed message-passing protocols, each contributing complementary strengths to a
shared analytic canvas. At the heart of GenoMAS lies a guided-planning
framework: programming agents unfold high-level task guidelines into Action
Units and, at each juncture, elect to advance, revise, bypass, or backtrack,
thereby maintaining logical coherence while bending gracefully to the
idiosyncrasies of genomic data.
  On the GenoTEX benchmark, GenoMAS reaches a Composite Similarity Correlation
of 89.13% for data preprocessing and an F$_1$ of 60.48% for gene
identification, surpassing the best prior art by 10.61% and 16.85%
respectively. Beyond metrics, GenoMAS surfaces biologically plausible
gene-phenotype associations corroborated by the literature, all while adjusting
for latent confounders. Code is available at https://github.com/Liu-Hy/GenoMAS.

</details>


### [137] [A Survey of Self-Evolving Agents: On Path to Artificial Super Intelligence](https://arxiv.org/abs/2507.21046)
*Huan-ang Gao,Jiayi Geng,Wenyue Hua,Mengkang Hu,Xinzhe Juan,Hongzhang Liu,Shilong Liu,Jiahao Qiu,Xuan Qi,Yiran Wu,Hongru Wang,Han Xiao,Yuhang Zhou,Shaokun Zhang,Jiayi Zhang,Jinyu Xiang,Yixiong Fang,Qiwen Zhao,Dongrui Liu,Qihan Ren,Cheng Qian,Zhenghailong Wang,Minda Hu,Huazheng Wang,Qingyun Wu,Heng Ji,Mengdi Wang*

Main category: cs.AI

TL;DR: The paper reviews self-evolving agents, emphasizing how they adapt, decide when and how to evolve, and their applications in various domains, aiming to advance autonomous, adaptive AI systems.


<details>
  <summary>Details</summary>
Motivation: The need for agents that can adapt and evolve in real-time environments to overcome the static nature of traditional Large Language Models.

Method: The survey categorizes evolutionary mechanisms, adaptation stages, algorithmic designs, evaluation metrics, and benchmarks, providing a comprehensive framework.

Result: Provides a structured overview of current methods, applications, challenges, and future directions in self-evolving agents, forming a roadmap for future research and deployments.

Conclusion: Establishes a foundation for advancing adaptive agents towards achieving Artificial Super Intelligence through continued evolution and self-adaptation.

Abstract: Large Language Models (LLMs) have demonstrated strong capabilities but remain
fundamentally static, unable to adapt their internal parameters to novel tasks,
evolving knowledge domains, or dynamic interaction contexts. As LLMs are
increasingly deployed in open-ended, interactive environments, this static
nature has become a critical bottleneck, necessitating agents that can
adaptively reason, act, and evolve in real time. This paradigm shift -- from
scaling static models to developing self-evolving agents -- has sparked growing
interest in architectures and methods enabling continual learning and
adaptation from data, interactions, and experiences. This survey provides the
first systematic and comprehensive review of self-evolving agents, organized
around three foundational dimensions -- what to evolve, when to evolve, and how
to evolve. We examine evolutionary mechanisms across agent components (e.g.,
models, memory, tools, architecture), categorize adaptation methods by stages
(e.g., intra-test-time, inter-test-time), and analyze the algorithmic and
architectural designs that guide evolutionary adaptation (e.g., scalar rewards,
textual feedback, single-agent and multi-agent systems). Additionally, we
analyze evaluation metrics and benchmarks tailored for self-evolving agents,
highlight applications in domains such as coding, education, and healthcare,
and identify critical challenges and research directions in safety,
scalability, and co-evolutionary dynamics. By providing a structured framework
for understanding and designing self-evolving agents, this survey establishes a
roadmap for advancing adaptive agentic systems in both research and real-world
deployments, ultimately shedding lights to pave the way for the realization of
Artificial Super Intelligence (ASI), where agents evolve autonomously,
performing at or beyond human-level intelligence across a wide array of tasks.

</details>
