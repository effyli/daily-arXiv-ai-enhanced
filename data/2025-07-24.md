<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 31]
- [cs.DB](#cs.DB) [Total: 3]
- [cs.AI](#cs.AI) [Total: 21]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [A Unifying Scheme for Extractive Content Selection Tasks](https://arxiv.org/abs/2507.16922)
*Shmuel Amar,Ori Shapira,Aviv Slobodkin,Ido Dagan*

Main category: cs.CL

TL;DR: The paper proposes a unified framework called instruction-guided content selection (IGCS) for various NLP content selection tasks, introduces a benchmark and synthetic dataset, and demonstrates benefits of transfer learning.


<details>
  <summary>Details</summary>
Motivation: To unify the modeling of diverse content selection tasks in NLP, which are typically studied in isolation.

Method: Introduce IGCS framework, create a comprehensive benchmark (	extit{igcsbench}), and develop a synthetic dataset to facilitate transfer learning.

Result: IGCS and associated resources improve performance, transfer learning is effective even without dedicated training data, and solutions are proposed for inference and evaluation challenges.

Conclusion: The resources and methods presented advance future models for content selection in NLP.

Abstract: A broad range of NLP tasks involve selecting relevant text spans from given
source texts. Despite this shared objective, such \textit{content selection}
tasks have traditionally been studied in isolation, each with its own modeling
approaches, datasets, and evaluation metrics. In this work, we propose
\textit{instruction-guided content selection (IGCS)} as a beneficial unified
framework for such settings, where the task definition and any
instance-specific request are encapsulated as instructions to a language model.
To promote this framework, we introduce \igcsbench{}, the first unified
benchmark covering diverse content selection tasks. Further, we create a large
generic synthetic dataset that can be leveraged for diverse content selection
tasks, and show that transfer learning with these datasets often boosts
performance, whether dedicated training for the targeted task is available or
not. Finally, we address generic inference time issues that arise in LLM-based
modeling of content selection, assess a generic evaluation metric, and overall
propose the utility of our resources and methods for future content selection
models. Models and datasets available at https://github.com/shmuelamar/igcs.

</details>


### [2] [AI-based Clinical Decision Support for Primary Care: A Real-World Study](https://arxiv.org/abs/2507.16947)
*Robert Korom,Sarah Kiptinness,Najib Adan,Kassim Said,Catherine Ithuli,Oliver Rotich,Boniface Kimani,Irene King'ori,Stellah Kamau,Elizabeth Atemba,Muna Aden,Preston Bowman,Michael Sharman,Rebecca Soskin Hicks,Rebecca Distler,Johannes Heidecke,Rahul K. Arora,Karan Singhal*

Main category: cs.CL

TL;DR: AI Consult, an LLM-based decision support tool in Kenyan clinics, reduces diagnostic and treatment errors, enhancing care quality.


<details>
  <summary>Details</summary>
Motivation: To assess the real-world impact of large language model-based clinical decision support on patient care quality.

Method: A comparative quality improvement study involving 39,849 patient visits across 15 clinics, with independent physician-rated outcome assessments.

Result: Clinicians with AI Consult made 16% fewer diagnostic and 13% fewer treatment errors, preventing thousands of errors annually, with positive clinician feedback.

Conclusion: LLM-based clinical decision support can effectively reduce errors when properly implemented, supporting safer, higher-quality care.

Abstract: We evaluate the impact of large language model-based clinical decision
support in live care. In partnership with Penda Health, a network of primary
care clinics in Nairobi, Kenya, we studied AI Consult, a tool that serves as a
safety net for clinicians by identifying potential documentation and clinical
decision-making errors. AI Consult integrates into clinician workflows,
activating only when needed and preserving clinician autonomy. We conducted a
quality improvement study, comparing outcomes for 39,849 patient visits
performed by clinicians with or without access to AI Consult across 15 clinics.
Visits were rated by independent physicians to identify clinical errors.
Clinicians with access to AI Consult made relatively fewer errors: 16% fewer
diagnostic errors and 13% fewer treatment errors. In absolute terms, the
introduction of AI Consult would avert diagnostic errors in 22,000 visits and
treatment errors in 29,000 visits annually at Penda alone. In a survey of
clinicians with AI Consult, all clinicians said that AI Consult improved the
quality of care they delivered, with 75% saying the effect was "substantial".
These results required a clinical workflow-aligned AI Consult implementation
and active deployment to encourage clinician uptake. We hope this study
demonstrates the potential for LLM-based clinical decision support tools to
reduce errors in real-world settings and provides a practical framework for
advancing responsible adoption.

</details>


### [3] [Harnessing RLHF for Robust Unanswerability Recognition and Trustworthy Response Generation in LLMs](https://arxiv.org/abs/2507.16951)
*Shuyuan Lin,Lei Duan,Philip Hughes,Yuxuan Sheng*

Main category: cs.CL

TL;DR: SALU is a novel Large Language Model integrated with unanswerability detection, reducing hallucinations and improving reliability in conversational systems.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of handling unanswerable questions in Conversational Information Retrieval systems without relying on external classifiers.

Method: Developing a multi-task trained LLM with confidence-score-guided reinforcement learning and human feedback, embedding unanswerability detection directly within the model.

Result: SALU outperforms baselines in accuracy and human evaluations, with significant reduction in hallucinations and better handling of unanswerable questions.

Conclusion: SALU demonstrates that integrating self-awareness of knowledge boundaries into LLMs enhances reliability and safety in conversational AI.

Abstract: Conversational Information Retrieval (CIR) systems, while offering intuitive
access to information, face a significant challenge: reliably handling
unanswerable questions to prevent the generation of misleading or hallucinated
content. Traditional approaches often rely on external classifiers, which can
introduce inconsistencies with the core generative Large Language Models
(LLMs). This paper introduces Self-Aware LLM for Unanswerability (SALU), a
novel approach that deeply integrates unanswerability detection directly within
the LLM's generative process. SALU is trained using a multi-task learning
framework for both standard Question Answering (QA) and explicit abstention
generation for unanswerable queries. Crucially, it incorporates a
confidence-score-guided reinforcement learning with human feedback (RLHF)
phase, which explicitly penalizes hallucinated responses and rewards
appropriate abstentions, fostering intrinsic self-awareness of knowledge
boundaries. Through extensive experiments on our custom-built
C-IR_Answerability dataset, SALU consistently outperforms strong baselines,
including hybrid LLM-classifier systems, in overall accuracy for correctly
answering or abstaining from questions. Human evaluation further confirms
SALU's superior reliability, achieving high scores in factuality, appropriate
abstention, and, most importantly, a dramatic reduction in hallucination,
demonstrating its ability to robustly "know when to say 'I don't know'."

</details>


### [4] [Text-to-SPARQL Goes Beyond English: Multilingual Question Answering Over Knowledge Graphs through Human-Inspired Reasoning](https://arxiv.org/abs/2507.16971)
*Aleksandr Perevalov,Andreas Both*

Main category: cs.CL

TL;DR: mKGQAgent is a modular, human-inspired framework that improves multilingual knowledge graph question answering by converting natural language into SPARQL queries, achieving first place in a major benchmark.


<details>
  <summary>Details</summary>
Motivation: To enhance multilingual semantic parsing and querying of knowledge graphs.

Method: Decomposing question-to-query conversion into interpretable subtasks using LLMs guided by an experience pool.

Result: First place in the Text2SPARQL challenge 2025 on DBpedia and Corporate KGQA benchmarks.

Conclusion: This approach advances human-like reasoning in multilingual semantic parsing systems.

Abstract: Accessing knowledge via multilingual natural-language interfaces is one of
the emerging challenges in the field of information retrieval and related ones.
Structured knowledge stored in knowledge graphs can be queried via a specific
query language (e.g., SPARQL). Therefore, one needs to transform
natural-language input into a query to fulfill an information need. Prior
approaches mostly focused on combining components (e.g., rule-based or
neural-based) that solve downstream tasks and come up with an answer at the
end. We introduce mKGQAgent, a human-inspired framework that breaks down the
task of converting natural language questions into SPARQL queries into modular,
interpretable subtasks. By leveraging a coordinated LLM agent workflow for
planning, entity linking, and query refinement - guided by an experience pool
for in-context learning - mKGQAgent efficiently handles multilingual KGQA.
Evaluated on the DBpedia- and Corporate-based KGQA benchmarks within the
Text2SPARQL challenge 2025, our approach took first place among the other
participants. This work opens new avenues for developing human-like reasoning
systems in multilingual semantic parsing.

</details>


### [5] [Leveraging Synthetic Data for Question Answering with Multilingual LLMs in the Agricultural Domain](https://arxiv.org/abs/2507.16974)
*Rishemjit Kaur,Arshdeep Singh Bhankhar,Surangika Ranathunga,Jashanpreet Singh Salh,Sudhir Rajput,Vidhi,Kashish Mahendra,Bhavika Berwal,Ritesh Kumar*

Main category: cs.CL

TL;DR: This study enhances language-specific LLMs with synthetic multilingual agricultural data, significantly improving their accuracy and relevance in providing localized agricultural advice, especially in low-resource, multilingual settings.


<details>
  <summary>Details</summary>
Motivation: To improve the precision and relevance of AI-driven agricultural advisories in multilingual and low-resource contexts by overcoming the limitations of general-purpose LLMs.

Method: Generating synthetic multilingual agricultural datasets and fine-tuning language-specific LLMs based on these datasets.

Result: Significant improvements in factual accuracy, relevance, and consensus in the fine-tuned models compared to baseline models.

Conclusion: Synthetic data-driven, language-specific fine-tuning is an effective strategy for enhancing LLM performance in agriculture, aiding more localized and accurate advisory services.

Abstract: Enabling farmers to access accurate agriculture-related information in their
native languages in a timely manner is crucial for the success of the
agriculture field. Although large language models (LLMs) can be used to
implement Question Answering (QA) systems, simply using publicly available
general-purpose LLMs in agriculture typically offer generic advisories, lacking
precision in local and multilingual contexts due to insufficient
domain-specific training and scarcity of high-quality, region-specific
datasets. Our study addresses these limitations by generating multilingual
synthetic agricultural datasets (English, Hindi, Punjabi) from
agriculture-specific documents and fine-tuning language-specific LLMs. Our
evaluation on curated multilingual datasets demonstrates significant
improvements in factual accuracy, relevance, and agricultural consensus for the
fine-tuned models compared to their baseline counterparts. These results
highlight the efficacy of synthetic data-driven, language-specific fine-tuning
as an effective strategy to improve the performance of LLMs in agriculture,
especially in multilingual and low-resource settings. By enabling more accurate
and localized agricultural advisory services, this study provides a meaningful
step toward bridging the knowledge gap in AI-driven agricultural solutions for
diverse linguistic communities.

</details>


### [6] [Obscured but Not Erased: Evaluating Nationality Bias in LLMs via Name-Based Bias Benchmarks](https://arxiv.org/abs/2507.16989)
*Giulio Pelosio,Devesh Batra,Noémie Bovey,Robert Hankache,Cristovao Iglesias,Greig Cowan,Raad Khraishi*

Main category: cs.CL

TL;DR: The paper investigates how substituting explicit nationality labels with culturally indicative names impacts biases and accuracy in Large Language Models, finding smaller models exhibit more bias and retain more errors.


<details>
  <summary>Details</summary>
Motivation: To understand how culturally indicative names influence bias and accuracy in LLMs, reflecting real-world applications.

Method: A novel name-based benchmarking approach derived from BBQ dataset, testing various LLMs from industry leaders with name substitutions.

Result: Smaller models are less accurate and show more bias; name substitutions increase bias and error retention, with significant differences observed between models.

Conclusion: Biases in LLMs are resilient and have important implications for AI deployment in diverse global contexts.

Abstract: Large Language Models (LLMs) can exhibit latent biases towards specific
nationalities even when explicit demographic markers are not present. In this
work, we introduce a novel name-based benchmarking approach derived from the
Bias Benchmark for QA (BBQ) dataset to investigate the impact of substituting
explicit nationality labels with culturally indicative names, a scenario more
reflective of real-world LLM applications. Our novel approach examines how this
substitution affects both bias magnitude and accuracy across a spectrum of LLMs
from industry leaders such as OpenAI, Google, and Anthropic. Our experiments
show that small models are less accurate and exhibit more bias compared to
their larger counterparts. For instance, on our name-based dataset and in the
ambiguous context (where the correct choice is not revealed), Claude Haiku
exhibited the worst stereotypical bias scores of 9%, compared to only 3.5% for
its larger counterpart, Claude Sonnet, where the latter also outperformed it by
117.7% in accuracy. Additionally, we find that small models retain a larger
portion of existing errors in these ambiguous contexts. For example, after
substituting names for explicit nationality references, GPT-4o retains 68% of
the error rate versus 76% for GPT-4o-mini, with similar findings for other
model providers, in the ambiguous context. Our research highlights the stubborn
resilience of biases in LLMs, underscoring their profound implications for the
development and deployment of AI systems in diverse, global contexts.

</details>


### [7] [Multi-Label Classification with Generative AI Models in Healthcare: A Case Study of Suicidality and Risk Factors](https://arxiv.org/abs/2507.17009)
*Ming Huang,Zehan Li,Yan Hu,Wanjing Wang,Andrew Wen,Scott Lane,Salih Selek,Lokesh Shahani,Rodrigo Machado-Vieira,Jair Soares,Hua Xu,Hongfang Liu*

Main category: cs.CL

TL;DR: The study explores the application of generative LLMs (GPT-3.5 and GPT-4.5) for multi-label classification of suicidality-related factors in psychiatric EHRs, achieving high accuracy and revealing error patterns, thus supporting complex clinical assessments.


<details>
  <summary>Details</summary>
Motivation: Address the need for nuanced detection of multiple co-occurring suicidality factors in clinical records, moving beyond binary classification methods.

Method: Developed an end-to-end generative multi-label classification pipeline using GPT-3.5 and GPT-4.5, including advanced evaluation metrics and error analysis techniques.

Result: GPT-3.5 attained 0.94 partial match accuracy and 0.91 F1 score; GPT-4.5, with guided prompts, showed superior performance especially on rare labels, and error patterns like conflating SI and SA were identified.

Conclusion: Generative LLMs are feasible for complex clinical multi-label tasks, providing a foundation for large-scale research and improved clinical decision-making.

Abstract: Suicide remains a pressing global health crisis, with over 720,000 deaths
annually and millions more affected by suicide ideation (SI) and suicide
attempts (SA). Early identification of suicidality-related factors (SrFs),
including SI, SA, exposure to suicide (ES), and non-suicidal self-injury
(NSSI), is critical for timely intervention. While prior studies have applied
AI to detect SrFs in clinical notes, most treat suicidality as a binary
classification task, overlooking the complexity of cooccurring risk factors.
This study explores the use of generative large language models (LLMs),
specifically GPT-3.5 and GPT-4.5, for multi-label classification (MLC) of SrFs
from psychiatric electronic health records (EHRs). We present a novel end to
end generative MLC pipeline and introduce advanced evaluation methods,
including label set level metrics and a multilabel confusion matrix for error
analysis. Finetuned GPT-3.5 achieved top performance with 0.94 partial match
accuracy and 0.91 F1 score, while GPT-4.5 with guided prompting showed superior
performance across label sets, including rare or minority label sets,
indicating a more balanced and robust performance. Our findings reveal
systematic error patterns, such as the conflation of SI and SA, and highlight
the models tendency toward cautious over labeling. This work not only
demonstrates the feasibility of using generative AI for complex clinical
classification tasks but also provides a blueprint for structuring unstructured
EHR data to support large scale clinical research and evidence based medicine.

</details>


### [8] [Can External Validation Tools Improve Annotation Quality for LLM-as-a-Judge?](https://arxiv.org/abs/2507.17015)
*Arduin Findeis,Floris Weers,Guoli Yin,Ke Ye,Ruoming Pang,Tom Gunter*

Main category: cs.CL

TL;DR: This paper explores augmenting AI annotation systems with web-search and code execution tools to improve feedback quality on challenging response domains like factual, math, and code tasks. The method shows potential but also highlights sensitivity to parameters and dataset limitations.


<details>
  <summary>Details</summary>
Motivation: To improve the quality of pairwise response comparisons in domains where traditional metrics and human annotation are challenging, especially for factual, mathematical, and coding responses.

Method: Integrating web-search and code execution tools into an agentic system to ground evaluations on external validation sources, thereby enhancing annotation quality.

Result: External tools can improve annotation performance across several tasks, though the results are sensitive to parameters and current benchmarks.

Conclusion: Using external tools can enhance annotation quality but requires careful parameter tuning and better benchmarks.

Abstract: Pairwise preferences over model responses are widely collected to evaluate
and provide feedback to large language models (LLMs). Given two alternative
model responses to the same input, a human or AI annotator selects the "better"
response. This approach can provide feedback for domains where other hard-coded
metrics are difficult to obtain (e.g., chat response quality), thereby helping
model evaluation or training. However, for some domains high-quality pairwise
comparisons can be tricky to obtain - from AI and humans. For example, for
responses with many factual statements, annotators may disproportionately weigh
writing quality rather than underlying facts. In this work, we explore
augmenting standard AI annotator systems with additional tools to improve
performance on three challenging response domains: long-form factual, math and
code tasks. We propose a tool-using agentic system to provide higher quality
feedback on these domains. Our system uses web-search and code execution to
ground itself based on external validation, independent of the LLM's internal
knowledge and biases. We provide extensive experimental results evaluating our
method across the three targeted response domains as well as general annotation
tasks, using RewardBench (incl. AlpacaEval and LLMBar), RewardMath, as well as
three new datasets for domains with saturated pre-existing datasets. Our
results indicate that external tools can indeed improve performance in many,
but not all, cases. More generally, our experiments highlight the sensitivity
of performance to simple parameters (e.g., prompt) and the need for improved
(non-saturated) annotator benchmarks. We share our code at
https://github.com/apple/ml-agent-evaluator.

</details>


### [9] [Evolutionary Feature-wise Thresholding for Binary Representation of NLP Embeddings](https://arxiv.org/abs/2507.17025)
*Soumen Sinha,Shahryar Rahnamayan,Azam Asilian Bidgoli*

Main category: cs.CL

TL;DR: This paper introduces a method to optimize binary representations of NLP embeddings by identifying feature-specific thresholds, leading to improved accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: To enhance the storage and computational efficiency of NLP embeddings while maintaining accuracy.

Method: A Coordinate Search-based optimization framework to find optimal thresholds for each feature during binarization.

Result: Binary embeddings with optimized thresholds outperform traditional methods in accuracy across various NLP tasks and datasets.

Conclusion: Feature-specific threshold optimization significantly improves binary encoding of text embeddings, with broad applicability beyond NLP.

Abstract: Efficient text embedding is crucial for large-scale natural language
processing (NLP) applications, where storage and computational efficiency are
key concerns. In this paper, we explore how using binary representations
(barcodes) instead of real-valued features can be used for NLP embeddings
derived from machine learning models such as BERT. Thresholding is a common
method for converting continuous embeddings into binary representations, often
using a fixed threshold across all features. We propose a Coordinate
Search-based optimization framework that instead identifies the optimal
threshold for each feature, demonstrating that feature-specific thresholds lead
to improved performance in binary encoding. This ensures that the binary
representations are both accurate and efficient, enhancing performance across
various features. Our optimal barcode representations have shown promising
results in various NLP applications, demonstrating their potential to transform
text representation. We conducted extensive experiments and statistical tests
on different NLP tasks and datasets to evaluate our approach and compare it to
other thresholding methods. Binary embeddings generated using using optimal
thresholds found by our method outperform traditional binarization methods in
accuracy. This technique for generating binary representations is versatile and
can be applied to any features, not just limited to NLP embeddings, making it
useful for a wide range of domains in machine learning applications.

</details>


### [10] [CogDual: Enhancing Dual Cognition of LLMs via Reinforcement Learning with Implicit Rule-Based Rewards](https://arxiv.org/abs/2507.17147)
*Cheng Liu,Yifei Lu,Fanghua Ye,Jian Li,Xingyu Chen,Feiliang Ren,Zhaopeng Tu,Xiaolong Li*

Main category: cs.CL

TL;DR: CogDual is a novel role-playing language agent inspired by cognitive psychology that models external and internal awareness to improve character consistency, using reinforcement learning to enhance open-domain text generation.


<details>
  <summary>Details</summary>
Motivation: To address limitations in existing RPLA approaches that neglect underlying cognitive mechanisms, aiming for more consistent and contextually aligned responses.

Method: Introducing CogDual, which adopts a 'cognize-then-respond' reasoning paradigm by jointly modeling external situational awareness and internal self-awareness, combined with reinforcement learning using reward schemes.

Result: CogDual outperforms existing baselines and generalizes well across various role-playing tasks on multiple benchmarks.

Conclusion: Incorporating cognitive-inspired mechanisms improves RPLAs, making responses more consistent and adaptable across tasks.

Abstract: Role-Playing Language Agents (RPLAs) have emerged as a significant
application direction for Large Language Models (LLMs). Existing approaches
typically rely on prompt engineering or supervised fine-tuning to enable models
to imitate character behaviors in specific scenarios, but often neglect the
underlying \emph{cognitive} mechanisms driving these behaviors. Inspired by
cognitive psychology, we introduce \textbf{CogDual}, a novel RPLA adopting a
\textit{cognize-then-respond } reasoning paradigm. By jointly modeling external
situational awareness and internal self-awareness, CogDual generates responses
with improved character consistency and contextual alignment. To further
optimize the performance, we employ reinforcement learning with two
general-purpose reward schemes designed for open-domain text generation.
Extensive experiments on the CoSER benchmark, as well as Cross-MR and
LifeChoice, demonstrate that CogDual consistently outperforms existing
baselines and generalizes effectively across diverse role-playing tasks.

</details>


### [11] [SKA-Bench: A Fine-Grained Benchmark for Evaluating Structured Knowledge Understanding of LLMs](https://arxiv.org/abs/2507.17178)
*Zhiqiang Liu,Enpei Niu,Yin Hua,Mengshu Sun,Lei Liang,Huajun Chen,Wen Zhang*

Main category: cs.CL

TL;DR: A new benchmark, SKA-Bench, assesses large language models' understanding of structured knowledge across various forms, revealing their challenges and guiding future improvements.


<details>
  <summary>Details</summary>
Motivation: To address the lack of comprehensive and rigorous evaluation tools for LLMs' structured knowledge understanding.

Method: Constructed SKA-Bench with four knowledge forms and a three-stage pipeline, testing four fundamental abilities through various question-answer instances.

Result: LLMs still struggle with structured knowledge understanding, affected by noise, order, and hallucinations across eight models including DeepSeek-R1.

Conclusion: Current LLMs need significant improvements in understanding structured knowledge, with the benchmark serving as a tool for diagnosis and enhancement.

Abstract: Although large language models (LLMs) have made significant progress in
understanding Structured Knowledge (SK) like KG and Table, existing evaluations
for SK understanding are non-rigorous (i.e., lacking evaluations of specific
capabilities) and focus on a single type of SK. Therefore, we aim to propose a
more comprehensive and rigorous structured knowledge understanding benchmark to
diagnose the shortcomings of LLMs. In this paper, we introduce SKA-Bench, a
Structured Knowledge Augmented QA Benchmark that encompasses four widely used
structured knowledge forms: KG, Table, KG+Text, and Table+Text. We utilize a
three-stage pipeline to construct SKA-Bench instances, which includes a
question, an answer, positive knowledge units, and noisy knowledge units. To
evaluate the SK understanding capabilities of LLMs in a fine-grained manner, we
expand the instances into four fundamental ability testbeds: Noise Robustness,
Order Insensitivity, Information Integration, and Negative Rejection. Empirical
evaluations on 8 representative LLMs, including the advanced DeepSeek-R1,
indicate that existing LLMs still face significant challenges in understanding
structured knowledge, and their performance is influenced by factors such as
the amount of noise, the order of knowledge units, and hallucination
phenomenon. Our dataset and code are available at
https://github.com/Lza12a/SKA-Bench.

</details>


### [12] [FinGAIA: An End-to-End Benchmark for Evaluating AI Agents in Finance](https://arxiv.org/abs/2507.17186)
*Lingfeng Zeng,Fangqi Lou,Zixuan Wang,Jiajie Xu,Jinyi Niu,Mengping Li,Yifan Dong,Qi Qi,Wei Zhang,Ziwei Yang,Jun Han,Ruilun Feng,Ruiqi Hu,Lejie Zhang,Zhengbo Feng,Yicheng Ren,Xin Guo,Zhaowei Liu,Dongpo Cheng,Weige Cai,Liwen Zhang*

Main category: cs.CL

TL;DR: FinGAIA is a comprehensive benchmark for assessing AI agents' capabilities in complex financial tasks, revealing current limitations and guiding future research.


<details>
  <summary>Details</summary>
Motivation: To evaluate and improve AI agents' multi-step, multi-tool collaboration abilities specifically within the financial sector, where such capabilities are underexplored.

Method: Developed FinGAIA with 407 tasks across seven financial sub-domains, organized into three scenario levels; tested 10 mainstream AI agents in a zero-shot setting.

Result: ChatGPT achieved 48.9% accuracy, outperforming non-professionals but still significantly behind financial experts; error analysis identified five key failure patterns.

Conclusion: FinGAIA provides a vital benchmark to objectively evaluate AI agents in finance, highlighting areas for future enhancement.

Abstract: The booming development of AI agents presents unprecedented opportunities for
automating complex tasks across various domains. However, their multi-step,
multi-tool collaboration capabilities in the financial sector remain
underexplored. This paper introduces FinGAIA, an end-to-end benchmark designed
to evaluate the practical abilities of AI agents in the financial domain.
FinGAIA comprises 407 meticulously crafted tasks, spanning seven major
financial sub-domains: securities, funds, banking, insurance, futures, trusts,
and asset management. These tasks are organized into three hierarchical levels
of scenario depth: basic business analysis, asset decision support, and
strategic risk management. We evaluated 10 mainstream AI agents in a zero-shot
setting. The best-performing agent, ChatGPT, achieved an overall accuracy of
48.9\%, which, while superior to non-professionals, still lags financial
experts by over 35 percentage points. Error analysis has revealed five
recurring failure patterns: Cross-modal Alignment Deficiency, Financial
Terminological Bias, Operational Process Awareness Barrier, among others. These
patterns point to crucial directions for future research. Our work provides the
first agent benchmark closely related to the financial domain, aiming to
objectively assess and promote the development of agents in this crucial field.
Partial data is available at https://github.com/SUFE-AIFLM-Lab/FinGAIA.

</details>


### [13] [The Pluralistic Moral Gap: Understanding Judgment and Value Differences between Humans and Large Language Models](https://arxiv.org/abs/2507.17216)
*Giuseppe Russo,Debora Nozza,Paul Röttger,Dirk Hovy*

Main category: cs.CL

TL;DR: The paper explores how well Large Language Models (LLMs) align with human moral judgments, identifying gaps in value diversity and consensus, and proposes a new method, Dynamic Moral Profiling, to improve alignment.


<details>
  <summary>Details</summary>
Motivation: To understand the extent to which LLMs can accurately mirror human moral judgments and address the observed discrepancies in value diversity and consensus.

Method: Developed the Moral Dilemma Dataset to compare LLM and human moral judgments, used a 60-value taxonomy to analyze reliance on moral values, and introduced Dynamic Moral Profiling (DMP) to improve alignment.

Result: Found that LLMs align with human judgments only under high consensus, rely on fewer moral values, and that DMP significantly enhances both alignment and value diversity.

Conclusion: Addressing the pluralistic moral gap through value-aware conditioning like DMP can lead to more human-aligned and diverse moral guidance from LLMs.

Abstract: People increasingly rely on Large Language Models (LLMs) for moral advice,
which may influence humans' decisions. Yet, little is known about how closely
LLMs align with human moral judgments. To address this, we introduce the Moral
Dilemma Dataset, a benchmark of 1,618 real-world moral dilemmas paired with a
distribution of human moral judgments consisting of a binary evaluation and a
free-text rationale. We treat this problem as a pluralistic distributional
alignment task, comparing the distributions of LLM and human judgments across
dilemmas. We find that models reproduce human judgments only under high
consensus; alignment deteriorates sharply when human disagreement increases. In
parallel, using a 60-value taxonomy built from 3,783 value expressions
extracted from rationales, we show that LLMs rely on a narrower set of moral
values than humans. These findings reveal a pluralistic moral gap: a mismatch
in both the distribution and diversity of values expressed. To close this gap,
we introduce Dynamic Moral Profiling (DMP), a Dirichlet-based sampling method
that conditions model outputs on human-derived value profiles. DMP improves
alignment by 64.3% and enhances value diversity, offering a step toward more
pluralistic and human-aligned moral guidance from LLMs.

</details>


### [14] [CLARIFID: Improving Radiology Report Generation by Reinforcing Clinically Accurate Impressions and Enforcing Detailed Findings](https://arxiv.org/abs/2507.17234)
*Kyeongkyu Lee,Seonghwan Yoon,Hongki Lim*

Main category: cs.CL

TL;DR: CLARIFID improves radiology report generation by optimizing diagnostic correctness through a multi-view, reasoning-aware approach, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of current radiology report generation methods, particularly in ensuring clinical reliability and handling multi-view images.

Method: The framework employs section-aware pretraining, reinforcement learning with F1 score rewards, reasoning-aware decoding, and multi-view fusion, along with inference strategies to enhance report coherence.

Result: CLARIFID surpasses existing baselines on the MIMIC-CXR dataset, achieving superior clinical and NLG metrics.

Conclusion: The proposed approach effectively enhances the clinical fidelity of automatically generated radiology reports.

Abstract: Automatic generation of radiology reports has the potential to alleviate
radiologists' significant workload, yet current methods struggle to deliver
clinically reliable conclusions. In particular, most prior approaches focus on
producing fluent text without effectively ensuring the factual correctness of
the reports and often rely on single-view images, limiting diagnostic
comprehensiveness. We propose CLARIFID, a novel framework that directly
optimizes diagnostic correctness by mirroring the two-step workflow of experts.
Specifically, CLARIFID (1) learns the logical flow from Findings to Impression
through section-aware pretraining, (2) is fine-tuned with Proximal Policy
Optimization in which the CheXbert F1 score of the Impression section serves as
the reward, (3) enforces reasoning-aware decoding that completes "Findings"
before synthesizing the "Impression", and (4) fuses multiple chest X-ray views
via a vision-transformer-based multi-view encoder. During inference, we apply a
reasoning-aware next-token forcing strategy followed by report-level
re-ranking, ensuring that the model first produces a comprehensive Findings
section before synthesizing the Impression and thereby preserving coherent
clinical reasoning. Experimental results on the MIMIC-CXR dataset demonstrate
that our method achieves superior clinical efficacy and outperforms existing
baselines on both standard NLG metrics and clinically aware scores.

</details>


### [15] [Triple X: A LLM-Based Multilingual Speech Recognition System for the INTERSPEECH2025 MLC-SLM Challenge](https://arxiv.org/abs/2507.17288)
*Miaomiao Gao,Xiaoxiao Xiang,Yiwen Guo*

Main category: cs.CL

TL;DR: The paper presents a multilingual speech recognition system using an encoder-adapter-LLM architecture with a multi-stage training strategy, achieving second place in the MLC-SLM Challenge.


<details>
  <summary>Details</summary>
Motivation: To improve speech recognition accuracy in multilingual conversational scenarios by leveraging advanced model architectures and training strategies.

Method: Design of an encoder-adapter-LLM architecture combined with a multi-stage training approach utilizing extensive multilingual datasets.

Result: Achieved competitive Word Error Rate (WER) on dev and test sets, securing second place in the challenge.

Conclusion: The proposed approach effectively enhances multilingual speech recognition performance, demonstrating its competitiveness.

Abstract: This paper describes our Triple X speech recognition system submitted to Task
1 of the Multi-Lingual Conversational Speech Language Modeling (MLC-SLM)
Challenge. Our work focuses on optimizing speech recognition accuracy in
multilingual conversational scenarios through an innovative encoder-adapter-LLM
architecture. This framework harnesses the powerful reasoning capabilities of
text-based large language models while incorporating domain-specific
adaptations. To further enhance multilingual recognition performance, we
adopted a meticulously designed multi-stage training strategy leveraging
extensive multilingual audio datasets. Experimental results demonstrate that
our approach achieves competitive Word Error Rate (WER) performance on both dev
and test sets, obtaining second place in the challenge ranking.

</details>


### [16] [Millions of $\text{GeAR}$-s: Extending GraphRAG to Millions of Documents](https://arxiv.org/abs/2507.17399)
*Zhili Shen,Chenxin Diao,Pascual Merita,Pavlos Vougiouklis,Jeff Z. Pan*

Main category: cs.CL

TL;DR: The paper evaluates the general applicability of a graph-based retrieval-augmented generation method, GeAR, on the SIGIR 2025 LiveRAG Challenge.


<details>
  <summary>Details</summary>
Motivation: To assess whether graph-based RAG approaches are broadly applicable across diverse datasets, beyond task-specific tasks.

Method: Adapting and testing the state-of-the-art GeAR model on the SIGIR 2025 LiveRAG Challenge dataset.

Result: The paper aims to analyze GeAR's performance and limitations, although specific results are not provided in the abstract.

Conclusion: The study investigates the adaptability and constraints of GeAR in a broader context, highlighting potential limitations.

Abstract: Recent studies have explored graph-based approaches to retrieval-augmented
generation, leveraging structured or semi-structured information -- such as
entities and their relations extracted from documents -- to enhance retrieval.
However, these methods are typically designed to address specific tasks, such
as multi-hop question answering and query-focused summarisation, and therefore,
there is limited evidence of their general applicability across broader
datasets. In this paper, we aim to adapt a state-of-the-art graph-based RAG
solution: $\text{GeAR}$ and explore its performance and limitations on the
SIGIR 2025 LiveRAG Challenge.

</details>


### [17] [Investigating Subjective Factors of Argument Strength: Storytelling, Emotions, and Hedging](https://arxiv.org/abs/2507.17409)
*Carlotta Quensel,Neele Falk,Gabriella Lapesa*

Main category: cs.CL

TL;DR: This study examines how subjective features like emotions, storytelling, and hedging influence argument strength, revealing different impacts on objective and subjective measures.


<details>
  <summary>Details</summary>
Motivation: Address the lack of large-scale analysis of subjective factors affecting argument strength and improve understanding of argument quality.

Method: Regression analysis on two datasets annotated for objective and subjective argument quality, along with evaluation of automated annotation methods for subjective features.

Result: Storytelling and hedging have contrasting effects on different facets of argument quality; emotions' impact depends on their usage rather than domain.

Conclusion: Subjective features significantly influence argument strength, with effects varying based on the aspect of argument quality being assessed.

Abstract: In assessing argument strength, the notions of what makes a good argument are
manifold. With the broader trend towards treating subjectivity as an asset and
not a problem in NLP, new dimensions of argument quality are studied. Although
studies on individual subjective features like personal stories exist, there is
a lack of large-scale analyses of the relation between these features and
argument strength. To address this gap, we conduct regression analysis to
quantify the impact of subjective factors $-$ emotions, storytelling, and
hedging $-$ on two standard datasets annotated for objective argument quality
and subjective persuasion. As such, our contribution is twofold: at the level
of contributed resources, as there are no datasets annotated with all studied
dimensions, this work compares and evaluates automated annotation methods for
each subjective feature. At the level of novel insights, our regression
analysis uncovers different patterns of impact of subjective features on the
two facets of argument strength encoded in the datasets. Our results show that
storytelling and hedging have contrasting effects on objective and subjective
argument quality, while the influence of emotions depends on their rhetoric
utilization rather than the domain.

</details>


### [18] [Each to Their Own: Exploring the Optimal Embedding in RAG](https://arxiv.org/abs/2507.17442)
*Shiting Chen,Zijian Zhao,Jinsong Chen*

Main category: cs.CL

TL;DR: Confident RAG enhances retrieval-augmented generation by generating multiple responses with different embeddings and selecting the most confident, improving response quality.


<details>
  <summary>Details</summary>
Motivation: Improve the quality of responses in RAG methods by addressing disparities caused by heterogeneous training data and model architecture.

Method: Introduce two approaches: Mixture-Embedding RAG, which sorts and selects from multiple embeddings, and Confident RAG, which generates multiple responses and selects the highest confidence one.

Result: Confident RAG shows approximately 10% and 5% improvements over vanilla LLMs and RAG, respectively, across various domains and models, demonstrating its effectiveness as a plug-and-play solution.

Conclusion: Confident RAG is an efficient and versatile approach to enhance LLM responses, with plans to release code for further use.

Abstract: Recently, as Large Language Models (LLMs) have fundamentally impacted various
fields, the methods for incorporating up-to-date information into LLMs or
adding external knowledge to construct domain-specific models have garnered
wide attention. Retrieval-Augmented Generation (RAG), serving as an
inference-time scaling method, is notable for its low cost and minimal effort
for parameter tuning. However, due to heterogeneous training data and model
architecture, the variant embedding models used in RAG exhibit different
benefits across various areas, often leading to different similarity
calculation results and, consequently, varying response quality from LLMs. To
address this problem, we propose and examine two approaches to enhance RAG by
combining the benefits of multiple embedding models, named Mixture-Embedding
RAG and Confident RAG. Mixture-Embedding RAG simply sorts and selects
retrievals from multiple embedding models based on standardized similarity;
however, it does not outperform vanilla RAG. In contrast, Confident RAG
generates responses multiple times using different embedding models and then
selects the responses with the highest confidence level, demonstrating average
improvements of approximately 10% and 5% over vanilla LLMs and RAG,
respectively. The consistent results across different LLMs and embedding models
indicate that Confident RAG is an efficient plug-and-play approach for various
domains. We will release our code upon publication.

</details>


### [19] [MultiNRC: A Challenging and Native Multilingual Reasoning Evaluation Benchmark for LLMs](https://arxiv.org/abs/2507.17476)
*Alexander R. Fabbri,Diego Mares,Jorge Flores,Meher Mankikar,Ernesto Hernandez,Dean Lee,Bing Liu,Chen Xing*

Main category: cs.CL

TL;DR: The paper introduces MultiNRC, a new multilingual reasoning benchmark with native-language questions in French, Spanish, and Chinese, assessing LLMs' performance across different reasoning categories. Results show current LLMs struggle with native multilingual reasoning, especially in culturally grounded tasks.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing benchmarks that bias towards English, and to evaluate LLMs' true multilingual reasoning capabilities across diverse languages and cultural contexts.

Method: Development of MultiNRC with over 1,000 native-language reasoning questions in French, Spanish, and Chinese, covering linguistic, cultural, and math reasoning, with English translations for comparison. Evaluation of 14 leading LLMs on this benchmark.

Result: Current LLMs perform poorly on MultiNRC, with scores below 50%. They excel in English math reasoning but encounter challenges in culturally grounded reasoning, indicating persistent difficulties in native multilingual contexts.

Conclusion: LLMs currently lack strong native multilingual reasoning abilities, especially in culturally specific reasoning tasks, highlighting the need for further development in multilingual and culturally aware AI.

Abstract: Although recent Large Language Models (LLMs) have shown rapid improvement on
reasoning benchmarks in English, the evaluation of such LLMs' multilingual
reasoning capability across diverse languages and cultural contexts remains
limited. Existing multilingual reasoning benchmarks are typically constructed
by translating existing English reasoning benchmarks, biasing these benchmarks
towards reasoning problems with context in English language/cultures. In this
work, we introduce the Multilingual Native Reasoning Challenge (MultiNRC), a
benchmark designed to assess LLMs on more than 1,000 native, linguistic and
culturally grounded reasoning questions written by native speakers in French,
Spanish, and Chinese. MultiNRC covers four core reasoning categories:
language-specific linguistic reasoning, wordplay & riddles, cultural/tradition
reasoning, and math reasoning with cultural relevance. For cultural/tradition
reasoning and math reasoning with cultural relevance, we also provide English
equivalent translations of the multilingual questions by manual translation
from native speakers fluent in English. This set of English equivalents can
provide a direct comparison of LLM reasoning capacity in other languages vs.
English on the same reasoning questions. We systematically evaluate current 14
leading LLMs covering most LLM families on MultiNRC and its English equivalent
set. The results show that (1) current LLMs are still not good at native
multilingual reasoning, with none scoring above 50% on MultiNRC; (2) LLMs
exhibit distinct strengths and weaknesses in handling linguistic, cultural, and
logical reasoning tasks; (3) Most models perform substantially better in math
reasoning in English compared to in original languages (+10%), indicating
persistent challenges with culturally grounded knowledge.

</details>


### [20] [Seed LiveInterpret 2.0: End-to-end Simultaneous Speech-to-speech Translation with Your Voice](https://arxiv.org/abs/2507.17527)
*Shanbo Cheng,Yu Bao,Zhichao Huang,Yu Lu,Ningxin Peng,Lu Xu,Runsheng Yu,Rong Cao,Ting Han,Zeyang Li,Sitong Liu,Shengtao Ma,Shiguang Pan,Jiongchen Xiao,Nuo Xu,Meng Yang,Rong Ye,Yiming Yu,Ruofei Zhang,Wanyi Zhang,Wenhao Zhu,Liehao Zou,Lu Lu,Yuxuan Wang,Yonghui Wu*

Main category: cs.CL

TL;DR: Seed-LiveInterpret 2.0 is an advanced end-to-end simultaneous interpretation model that offers high accuracy, low latency, and voice cloning, outperforming existing solutions.


<details>
  <summary>Details</summary>
Motivation: To overcome challenges in automatic simultaneous interpretation such as low quality, high latency, and multi-speaker confusion.

Method: Developing an end-to-end duplex speech-to-speech understanding-generating framework with large-scale pretraining and reinforcement learning.

Result: Achieved over 70% translation correctness in complex scenarios, reduced latency from 10s to 3s, and surpassed commercial solutions in quality.

Conclusion: Seed-LiveInterpret 2.0 provides a practical, high-performing system that significantly advances SI technology.

Abstract: Simultaneous Interpretation (SI) represents one of the most daunting
frontiers in the translation industry, with product-level automatic systems
long plagued by intractable challenges: subpar transcription and translation
quality, lack of real-time speech generation, multi-speaker confusion, and
translated speech inflation, especially in long-form discourses. In this study,
we introduce Seed-LiveInterpret 2.0, an end-to-end SI model that delivers
high-fidelity, ultra-low-latency speech-to-speech generation with voice cloning
capabilities. As a fully operational product-level solution, Seed-LiveInterpret
2.0 tackles these challenges head-on through our novel duplex speech-to-speech
understanding-generating framework. Experimental results demonstrate that
through large-scale pretraining and reinforcement learning, the model achieves
a significantly better balance between translation accuracy and latency,
validated by human interpreters to exceed 70% correctness in complex scenarios.
Notably, Seed-LiveInterpret 2.0 outperforms commercial SI solutions by
significant margins in translation quality, while slashing the average latency
of cloned speech from nearly 10 seconds to a near-real-time 3 seconds, which is
around a near 70% reduction that drastically enhances practical usability.

</details>


### [21] [Synthetic Voice Data for Automatic Speech Recognition in African Languages](https://arxiv.org/abs/2507.17578)
*Brian DeRenzi,Anna Dixon,Mohamed Aymane Farhi,Christian Resch*

Main category: cs.CL

TL;DR: Synthetic voice corpora significantly improve African ASR with cost-effective methods, but more robust evaluation protocols are needed.


<details>
  <summary>Details</summary>
Motivation: Address the lack of speech technology in many African languages.

Method: Generate synthetic text using LLMs, synthesize voice with TTS, and fine-tune ASR models.

Result: Achieved high readability scores, created extensive synthetic datasets at low cost, and improved ASR performance with synthetic data, but highlighted issues in evaluation protocols.

Conclusion: Synthetic data shows promise for low-resource African languages, but better evaluation standards are necessary.

Abstract: Speech technology remains out of reach for most of the over 2300 languages in
Africa. We present the first systematic assessment of large-scale synthetic
voice corpora for African ASR. We apply a three-step process: LLM-driven text
creation, TTS voice synthesis, and ASR fine-tuning. Eight out of ten languages
for which we create synthetic text achieved readability scores above 5 out of
7. We evaluated ASR improvement for three (Hausa, Dholuo, Chichewa) and created
more than 2,500 hours of synthetic voice data at below 1% of the cost of real
data. Fine-tuned Wav2Vec-BERT-2.0 models trained on 250h real and 250h
synthetic Hausa matched a 500h real-data-only baseline, while 579h real and
450h to 993h synthetic data created the best performance. We also present
gender-disaggregated ASR performance evaluation. For very low-resource
languages, gains varied: Chichewa WER improved about 6.5% relative with a 1:2
real-to-synthetic ratio; a 1:1 ratio for Dholuo showed similar improvements on
some evaluation data, but not on others. Investigating intercoder reliability,
ASR errors and evaluation datasets revealed the need for more robust reviewer
protocols and more accurate evaluation data. All data and models are publicly
released to invite further work to improve synthetic data for African
languages.

</details>


### [22] [Bridging Robustness and Generalization Against Word Substitution Attacks in NLP via the Growth Bound Matrix Approach](https://arxiv.org/abs/2507.10330)
*Mohammed Bouri,Adnane Saoud*

Main category: cs.CL

TL;DR: Introduces a novel regularization method using Growth Bound Matrices to enhance NLP model robustness, especially for recurrent and state space models, against adversarial attacks.


<details>
  <summary>Details</summary>
Motivation: Previous robustness techniques focus mainly on feed-forward and convolutional models, leaving recurrent and state space models underexplored.

Method: Develops a regularization technique based on Growth Bound Matrices, specifically computed for LSTM, S4, and CNN architectures.

Result: Achieves up to 8.8% improvement in adversarial robustness and better generalization, demonstrating effectiveness over existing methods.

Conclusion: The proposed GBM-based regularization significantly bolsters NLP models against adversarial attacks, with systematic analysis of SSM robustness.

Abstract: Despite advancements in Natural Language Processing (NLP), models remain
vulnerable to adversarial attacks, such as synonym substitutions. While prior
work has focused on improving robustness for feed-forward and convolutional
architectures, the robustness of recurrent networks and modern state space
models (SSMs), such as S4, remains understudied. These architectures pose
unique challenges due to their sequential processing and complex parameter
dynamics. In this paper, we introduce a novel regularization technique based on
Growth Bound Matrices (GBM) to improve NLP model robustness by reducing the
impact of input perturbations on model outputs. We focus on computing the GBM
for three architectures: Long Short-Term Memory (LSTM), State Space models
(S4), and Convolutional Neural Networks (CNN). Our method aims to (1) enhance
resilience against word substitution attacks, (2) improve generalization on
clean text, and (3) providing the first systematic analysis of SSM (S4)
robustness. Extensive experiments across multiple architectures and benchmark
datasets demonstrate that our method improves adversarial robustness by up to
8.8% over existing baselines. These results highlight the effectiveness of our
approach, outperforming several state-of-the-art methods in adversarial
defense. Codes are available at https://github.com/BouriMohammed/GBM

</details>


### [23] [A Hybrid Early-Exit Algorithm for Large Language Models Based on Space Alignment Decoding (SPADE)](https://arxiv.org/abs/2507.17618)
*Bowen Zheng,Ming Ma,Zhongqiao Lin,Tianming Yang*

Main category: cs.CL

TL;DR: SPADE improves early-exit algorithms for large language models by aligning intermediate and output layer representations, reducing inference costs without accuracy loss.


<details>
  <summary>Details</summary>
Motivation: Large language models are computationally expensive, and early-exit methods can reduce costs but often suffer from misalignment issues.

Method: Introduction of SPADE, which aligns intermediate layer representations with the output layer and combines it with confidence-based early exit decisions.

Result: The hybrid approach reduces inference costs significantly while maintaining accuracy.

Conclusion: SPADE offers an efficient and scalable solution for deploying large language models effectively.

Abstract: Large language models are computationally expensive due to their deep
structures. Prior research has shown that intermediate layers contain
sufficient information to generate accurate answers, leading to the development
of early-exit algorithms that reduce inference costs by terminating computation
at earlier layers. However, these methods often suffer from poor performance
due to misalignment between intermediate and output layer representations that
lead to decoding inaccuracy. To address these challenges, we propose SPADE
(SPace Alignment DEcoding), a novel decoding method that aligns intermediate
layer representations with the output layer by propagating a minimally reduced
sequence consisting of only the start token and the answer token. We further
optimize the early-exit decision-making process by training a linear
approximation of SPADE that computes entropy-based confidence metrics. Putting
them together, we create a hybrid early-exit algorithm that monitors confidence
levels and stops inference at intermediate layers while using SPADE to generate
high-quality outputs. This approach significantly reduces inference costs
without compromising accuracy, offering a scalable and efficient solution for
deploying large language models in real-world applications.

</details>


### [24] [WSM: Decay-Free Learning Rate Schedule via Checkpoint Merging for LLM Pre-training](https://arxiv.org/abs/2507.17634)
*Changxin Tian,Jiapeng Wang,Qian Zhao,Kunlong Chen,Jia Liu,Ziqi Liu,Jiaxin Mao,Wayne Xin Zhao,Zhiqiang Zhang,Jun Zhou*

Main category: cs.CL

TL;DR: The paper introduces WSM, a framework connecting learning rate decay and model merging, improving performance in various benchmarks by emphasizing merge duration.


<details>
  <summary>Details</summary>
Motivation: To explore alternative learning rate schedules without decay and leverage model merging for performance gains.

Method: Theoretical unification of decay strategies with model averaging within the WSM framework, supported by extensive experiments.

Result: WSM outperforms previous decay approaches, with significant performance gains across benchmarks, especially emphasizing the importance of merge duration.

Conclusion: WSM offers a robust, decay-free learning rate scheduling method that enhances model performance and adaptability.

Abstract: Recent advances in learning rate (LR) scheduling have demonstrated the
effectiveness of decay-free approaches that eliminate the traditional decay
phase while maintaining competitive performance. Model merging techniques have
emerged as particularly promising solutions in this domain. We present
Warmup-Stable and Merge (WSM), a general framework that establishes a formal
connection between learning rate decay and model merging. WSM provides a
unified theoretical foundation for emulating various decay strategies-including
cosine decay, linear decay and inverse square root decay-as principled model
averaging schemes, while remaining fully compatible with diverse optimization
methods. Through extensive experiments, we identify merge duration-the training
window for checkpoint aggregation-as the most critical factor influencing model
performance, surpassing the importance of both checkpoint interval and merge
quantity. Our framework consistently outperforms the widely-adopted
Warmup-Stable-Decay (WSD) approach across multiple benchmarks, achieving
significant improvements of +3.5% on MATH, +2.9% on HumanEval, and +5.5% on
MMLU-Pro. The performance advantages extend to supervised fine-tuning
scenarios, highlighting WSM's potential for long-term model refinement.

</details>


### [25] [Who Attacks, and Why? Using LLMs to Identify Negative Campaigning in 18M Tweets across 19 Countries](https://arxiv.org/abs/2507.17636)
*Victor Hartman,Petter Törnberg*

Main category: cs.CL

TL;DR: This study introduces zero-shot Large Language Models (LLMs) for cross-lingual classification of negative campaigning, achieving performance comparable to native speakers. It conducts the largest cross-national analysis of Twitter posts from European parliamentarians, revealing patterns in negative campaigning linked to party ideology.


<details>
  <summary>Details</summary>
Motivation: The research aims to overcome the limitations of traditional methods in studying negative campaigning across multiple languages and scales, facilitating more scalable and accurate analysis.

Method: The paper employs zero-shot LLMs for cross-lingual classification on benchmark datasets in ten languages and applies this approach to analyze 18 million tweets from 19 European countries.

Result: LLMs match native speaker performance and outperform traditional models; empirically, governing parties are less negative, while extremist and populist parties, especially on the radical right, are more negative.

Conclusion: LLMs are effective tools for scalable, transparent, and cross-cultural research in political communication, revealing consistent patterns of negative campaigning across Europe.

Abstract: Negative campaigning is a central feature of political competition, yet
empirical research has been limited by the high cost and limited scalability of
existing classification methods. This study makes two key contributions. First,
it introduces zero-shot Large Language Models (LLMs) as a novel approach for
cross-lingual classification of negative campaigning. Using benchmark datasets
in ten languages, we demonstrate that LLMs achieve performance on par with
native-speaking human coders and outperform conventional supervised machine
learning approaches. Second, we leverage this novel method to conduct the
largest cross-national study of negative campaigning to date, analyzing 18
million tweets posted by parliamentarians in 19 European countries between 2017
and 2022. The results reveal consistent cross-national patterns: governing
parties are less likely to use negative messaging, while ideologically extreme
and populist parties -- particularly those on the radical right -- engage in
significantly higher levels of negativity. These findings advance our
understanding of how party-level characteristics shape strategic communication
in multiparty systems. More broadly, the study demonstrates the potential of
LLMs to enable scalable, transparent, and replicable research in political
communication across linguistic and cultural contexts.

</details>


### [26] [Towards Greater Leverage: Scaling Laws for Efficient Mixture-of-Experts Language Models](https://arxiv.org/abs/2507.17702)
*Changxin Tian,Kunlong Chen,Jia Liu,Ziqi Liu,Zhiqiang Zhang,Jun Zhou*

Main category: cs.CL

TL;DR: The paper introduces Efficiency Leverage, a metric to predict the computational advantage of MoE models, and develops scaling laws validated by experiments showing that MoE models can achieve comparable performance to dense models with less computation.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of predicting model capacity and efficiency in MoE architectures.

Method: Large-scale empirical study training over 300 models, deriving scaling laws and validating with a pilot model.

Result: Discovered key factors influencing efficiency, formulated scaling laws, and validated them with a model achieving similar performance as less resource-intensive dense models.

Conclusion: Provides a principled foundation for the scalable and efficient design of MoE models based on validated empirical laws.

Abstract: Mixture-of-Experts (MoE) has become a dominant architecture for scaling Large
Language Models (LLMs) efficiently by decoupling total parameters from
computational cost. However, this decoupling creates a critical challenge:
predicting the model capacity of a given MoE configurations (e.g., expert
activation ratio and granularity) remains an unresolved problem. To address
this gap, we introduce Efficiency Leverage (EL), a metric quantifying the
computational advantage of an MoE model over a dense equivalent. We conduct a
large-scale empirical study, training over 300 models up to 28B parameters, to
systematically investigate the relationship between MoE architectural
configurations and EL. Our findings reveal that EL is primarily driven by the
expert activation ratio and the total compute budget, both following
predictable power laws, while expert granularity acts as a non-linear modulator
with a clear optimal range. We integrate these discoveries into a unified
scaling law that accurately predicts the EL of an MoE architecture based on its
configuration. To validate our derived scaling laws, we designed and trained
Ling-mini-beta, a pilot model for Ling-2.0 series with only 0.85B active
parameters, alongside a 6.1B dense model for comparison. When trained on an
identical 1T high-quality token dataset, Ling-mini-beta matched the performance
of the 6.1B dense model while consuming over 7x fewer computational resources,
thereby confirming the accuracy of our scaling laws. This work provides a
principled and empirically-grounded foundation for the scaling of efficient MoE
models.

</details>


### [27] [TyDi QA-WANA: A Benchmark for Information-Seeking Question Answering in Languages of West Asia and North Africa](https://arxiv.org/abs/2507.17709)
*Parker Riley,Siamak Shakeri,Waleed Ammar,Jonathan H. Clark*

Main category: cs.CL

TL;DR: A question-answering dataset called TyDi QA-WANA with 28K examples in 10 languages from Western Asia and North Africa was created to evaluate models' ability to use large texts effectively.


<details>
  <summary>Details</summary>
Motivation: To assess the ability of models to utilize large text contexts for information-seeking questions across diverse languages without translation bias.

Method:  Collection of data directly in each language, with questions paired with full articles, and baseline model evaluations.

Result:  Dataset and initial baseline performances are provided to enable future research improvements.

Conclusion: The dataset offers a valuable resource for advancing multilingual QA, emphasizing large-text comprehension and cultural relevance.

Abstract: We present TyDi QA-WANA, a question-answering dataset consisting of 28K
examples divided among 10 language varieties of western Asia and northern
Africa. The data collection process was designed to elicit information-seeking
questions, where the asker is genuinely curious to know the answer. Each
question in paired with an entire article that may or may not contain the
answer; the relatively large size of the articles results in a task suitable
for evaluating models' abilities to utilize large text contexts in answering
questions. Furthermore, the data was collected directly in each language
variety, without the use of translation, in order to avoid issues of cultural
relevance. We present performance of two baseline models, and release our code
and data to facilitate further improvement by the research community.

</details>


### [28] [From Feedback to Checklists: Grounded Evaluation of AI-Generated Clinical Notes](https://arxiv.org/abs/2507.17717)
*Karen Zhou,John Giorgi,Pranav Mani,Peng Xu,Davis Liang,Chenhao Tan*

Main category: cs.CL

TL;DR: A new checklist-based pipeline improves the evaluation of AI-generated clinical notes by aligning with physician preferences and outperforming existing metrics.


<details>
  <summary>Details</summary>
Motivation: Existing automated metrics for clinical note quality lack alignment with real-world physician preferences and face subjectivity and scalability issues.

Method: The study introduces a pipeline that distills real user feedback into interpretable checklists for note evaluation, grounded in human feedback and endorsed by LLM evaluators, using data from over 21,000 clinical encounters.

Result: The checklist-based approach outperforms baseline metrics in coverage, diversity, and predictive power for human ratings, showing robustness and practical value in offline evaluations.

Conclusion: The proposed checklist offers a scalable, interpretable, and clinician-aligned method to evaluate AI-generated clinical notes effectively.

Abstract: AI-generated clinical notes are increasingly used in healthcare, but
evaluating their quality remains a challenge due to high subjectivity and
limited scalability of expert review. Existing automated metrics often fail to
align with real-world physician preferences. To address this, we propose a
pipeline that systematically distills real user feedback into structured
checklists for note evaluation. These checklists are designed to be
interpretable, grounded in human feedback, and enforceable by LLM-based
evaluators. Using deidentified data from over 21,000 clinical encounters,
prepared in accordance with the HIPAA safe harbor standard, from a deployed AI
medical scribe system, we show that our feedback-derived checklist outperforms
baseline approaches in our offline evaluations in coverage, diversity, and
predictive power for human ratings. Extensive experiments confirm the
checklist's robustness to quality-degrading perturbations, significant
alignment with clinician preferences, and practical value as an evaluation
methodology. In offline research settings, the checklist can help identify
notes likely to fall below our chosen quality thresholds.

</details>


### [29] [AI Telephone Surveying: Automating Quantitative Data Collection with an AI Interviewer](https://arxiv.org/abs/2507.17718)
*Danny D. Leybzon,Shreyas Tirumala,Nishant Jain,Summer Gillen,Michael Jackson,Cameron McPhee,Jennifer Schmidt*

Main category: cs.CL

TL;DR: This paper explores the use of AI-powered telephone surveys using large language models, demonstrating that such systems can effectively conduct quantitative research with positive responses in completion, break-off, and satisfaction metrics.


<details>
  <summary>Details</summary>
Motivation: To develop and validate an AI system for conducting more natural, adaptive, and rigorous telephone surveys in quantitative research.

Method: Built and tested an AI system integrated with LLM, ASR, and speech synthesis, then conducted pilot surveys compared with human interviews.

Result: Shorter survey instruments and more responsive AI improved completion rates, reduced break-offs, and increased respondent satisfaction.

Conclusion: AI-powered telephone surveys are feasible and effective, with potential improvements from optimized survey length and interactivity.

Abstract: With the rise of voice-enabled artificial intelligence (AI) systems,
quantitative survey researchers have access to a new data-collection mode: AI
telephone surveying. By using AI to conduct phone interviews, researchers can
scale quantitative studies while balancing the dual goals of human-like
interactivity and methodological rigor. Unlike earlier efforts that used
interactive voice response (IVR) technology to automate these surveys, voice AI
enables a more natural and adaptive respondent experience as it is more robust
to interruptions, corrections, and other idiosyncrasies of human speech.
  We built and tested an AI system to conduct quantitative surveys based on
large language models (LLM), automatic speech recognition (ASR), and speech
synthesis technologies. The system was specifically designed for quantitative
research, and strictly adhered to research best practices like question order
randomization, answer order randomization, and exact wording.
  To validate the system's effectiveness, we deployed it to conduct two pilot
surveys with the SSRS Opinion Panel and followed-up with a separate
human-administered survey to assess respondent experiences. We measured three
key metrics: the survey completion rates, break-off rates, and respondent
satisfaction scores. Our results suggest that shorter instruments and more
responsive AI interviewers may contribute to improvements across all three
metrics studied.

</details>


### [30] [Megrez2 Technical Report](https://arxiv.org/abs/2507.17728)
*Boxun Li,Yadong Li,Zhiyuan Li,Congyi Liu,Weilin Liu,Guowei Niu,Zheyue Tan,Haiyang Xu,Zhuyu Yao,Tao Yuan,Dong Zhou,Yueqing Zhuang,Bo Zhao,Guohao Dai,Yu Wang*

Main category: cs.CL

TL;DR: Megrez2 is a lightweight, high-performance language model with a novel cross-layer expert sharing mechanism, optimized for device deployment, showing competitive results with fewer parameters.


<details>
  <summary>Details</summary>
Motivation: To create an efficient, deployable language model that balances performance with resource constraints.

Method: Introduces the Megrez2 architecture with cross-layer expert sharing and pre-gated routing, pre-trains on large data and fine-tunes with reinforcement learning.

Result: Megrez2-Preview achieves strong performance with fewer activated parameters, outperforming larger models on various tasks.

Conclusion: Megrez2 offers an effective balance of accuracy, efficiency, and deployability, suitable for resource-limited environments.

Abstract: We present Megrez2, a novel lightweight and high-performance language model
architecture optimized for device native deployment. Megrez2 introduces a novel
cross-layer expert sharing mechanism, which significantly reduces total
parameter count by reusing expert modules across adjacent transformer layers
while maintaining most of the model's capacity. It also incorporates pre-gated
routing, enabling memory-efficient expert loading and faster inference. As the
first instantiation of the Megrez2 architecture, we introduce the
Megrez2-Preview model, which is pre-trained on a 5-trillion-token corpus and
further enhanced through supervised fine-tuning and reinforcement learning with
verifiable rewards. With only 3B activated and 7.5B stored parameters,
Megrez2-Preview demonstrates competitive or superior performance compared to
larger models on a wide range of tasks, including language understanding,
instruction following, mathematical reasoning, and code generation. These
results highlight the effectiveness of the Megrez2 architecture to achieve a
balance between accuracy, efficiency, and deployability, making it a strong
candidate for real-world, resource-constrained applications.

</details>


### [31] [Pretraining on the Test Set Is No Longer All You Need: A Debate-Driven Approach to QA Benchmarks](https://arxiv.org/abs/2507.17747)
*Linbo Cao,Jinman Zhao*

Main category: cs.CL

TL;DR: A debate-driven evaluation paradigm transforms QA datasets into adversarial debates to more effectively assess language models' reasoning, reducing data contamination risk and lowering benchmark creation costs.


<details>
  <summary>Details</summary>
Motivation: Address the limitations of current QA benchmarks, such as data contamination, memorization, and high dataset creation costs, by proposing a more rigorous evaluation method.

Method: Converting QA tasks into debate format, where models defend or construct answers in multi-round debates judged by a blind model, using a systematic evaluation pipeline and benchmarking on MMLU-Pro questions.

Result: The approach improves robustness against data contamination, demonstrates varied model performances, and shows that debate-based assessments can scale efficiently with future models.

Conclusion: Debate-based evaluation offers a sustainable and effective alternative for measuring true reasoning in language models, outperforming traditional benchmarks.

Abstract: As frontier language models increasingly saturate standard QA benchmarks,
concerns about data contamination, memorization, and escalating dataset
creation costs persist. We propose a debate-driven evaluation paradigm that
transforms any existing QA dataset into structured adversarial debates--where
one model is given the official answer to defend, and another constructs and
defends an alternative answer--adjudicated by a judge model blind to the
correct solution. By forcing multi-round argumentation, this approach
substantially increases difficulty while penalizing shallow memorization, yet
reuses QA items to reduce curation overhead. We make two main contributions:
(1) an evaluation pipeline to systematically convert QA tasks into debate-based
assessments, and (2) a public benchmark that demonstrates our paradigm's
effectiveness on a subset of MMLU-Pro questions, complete with standardized
protocols and reference models. Empirical results validate the robustness of
the method and its effectiveness against data contamination--a Llama 3.1 model
fine-tuned on test questions showed dramatic accuracy improvements (50% -> 82%)
but performed worse in debates. Results also show that even weaker judges can
reliably differentiate stronger debaters, highlighting how debate-based
evaluation can scale to future, more capable systems while maintaining a
fraction of the cost of creating new benchmarks. Overall, our framework
underscores that "pretraining on the test set is no longer all you need,"
offering a sustainable path for measuring the genuine reasoning ability of
advanced language models.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [32] [Triadic First-Order Logic Queries in Temporal Networks](https://arxiv.org/abs/2507.17215)
*Omkar Bhalerao,Yunjie Pan,C. Seshadhri,Nishil Talati*

Main category: cs.DB

TL;DR: Introduction of FOLTY, an algorithm for thresholded First Order Logic triadic motif queries in massive temporal networks, with efficient implementation and promising empirical results.


<details>
  <summary>Details</summary>
Motivation: To enhance motif counting in temporal graphs using richer query semantics inspired by logic and database theory.

Method: Designing the FOLTY algorithm with specialized data structures for thresholded FOL motif queries.

Result: FOLTY matches the best known temporal triangle counting time, performs efficiently on large graphs, and broadens the scope of motif analysis.

Conclusion: FOLTY offers a new research direction in motif analysis, leveraging logic-based queries for complex temporal patterns.

Abstract: Motif counting is a fundamental problem in network analysis, and there is a
rich literature of theoretical and applied algorithms for this problem. Given a
large input network $G$, a motif $H$ is a small "pattern" graph indicative of
special local structure. Motif/pattern mining involves finding all matches of
this pattern in the input $G$. The simplest, yet challenging, case of motif
counting is when $H$ has three vertices, often called a "triadic" query. Recent
work has focused on "temporal graph mining", where the network $G$ has edges
with timestamps (and directions) and $H$ has time constraints.
  Inspired by concepts in logic and database theory, we introduce the study of
"thresholded First Order Logic (FOL) Motif Analysis" for massive temporal
networks. A typical triadic motif query asks for the existence of three
vertices that form a desired temporal pattern. An "FOL" motif query is obtained
by having both existential and thresholded universal quantifiers. This allows
for query semantics that can mine richer information from networks. A typical
triadic query would be "find all triples of vertices $u,v,w$ such that they
form a triangle within one hour". A thresholded FOL query can express "find all
pairs $u,v$ such that for half of $w$ where $(u,w)$ formed an edge, $(v,w)$
also formed an edge within an hour".
  We design the first algorithm, FOLTY, for mining thresholded FOL triadic
queries. The theoretical running time of FOLTY matches the best known running
time for temporal triangle counting in sparse graphs. We give an efficient
implementation of FOLTY using specialized temporal data structures. FOLTY has
excellent empirical behavior, and can answer triadic FOL queries on graphs with
nearly 70M edges is less than hour on commodity hardware. Our work has the
potential to start a new research direction in the classic well-studied problem
of motif analysis.

</details>


### [33] [Unfolding Data Quality Dimensions in Practice: A Survey](https://arxiv.org/abs/2507.17507)
*Vasileios Papastergios,Lisa Ehrlinger,Anastasios Gounaris*

Main category: cs.DB

TL;DR: The paper systematically maps data quality tool functionalities to high-level dimensions like accuracy and completeness, bridging theory and practice.


<details>
  <summary>Details</summary>
Motivation: To address the gap between data quality standards and real-world application.

Method: Analyzing seven open-source data quality tools to map functionalities to quality dimensions.

Result: Provides a comprehensive mapping showing how tool functionalities contribute to various data quality dimensions.

Conclusion: This work offers a unified view for practitioners and researchers, enhancing data quality assessment across multiple dimensions.

Abstract: Data quality describes the degree to which data meet specific requirements
and are fit for use by humans and/or downstream tasks (e.g., artificial
intelligence). Data quality can be assessed across multiple high-level concepts
called dimensions, such as accuracy, completeness, consistency, or timeliness.
While extensive research and several attempts for standardization (e.g.,
ISO/IEC 25012) exist for data quality dimensions, their practical application
often remains unclear. In parallel to research endeavors, a large number of
tools have been developed that implement functionalities for the detection and
mitigation of specific data quality issues, such as missing values or outliers.
With this paper, we aim to bridge this gap between data quality theory and
practice by systematically connecting low-level functionalities offered by data
quality tools with high-level dimensions, revealing their many-to-many
relationships. Through an examination of seven open-source data quality tools,
we provide a comprehensive mapping between their functionalities and the data
quality dimensions, demonstrating how individual functionalities and their
variants partially contribute to the assessment of single dimensions. This
systematic survey provides both practitioners and researchers with a unified
view on the fragmented landscape of data quality checks, offering actionable
insights for quality assessment across multiple dimensions.

</details>


### [34] [SHINE: A Scalable HNSW Index in Disaggregated Memory](https://arxiv.org/abs/2507.17647)
*Manuel Widmoser,Daniel Kocher,Nikolaus Augsten*

Main category: cs.DB

TL;DR: A new scalable distributed HNSW index for ANN search in disaggregated memory systems that maintains high accuracy and improves efficiency through caching strategies.


<details>
  <summary>Details</summary>
Motivation: To enable high-accuracy ANN search in disaggregated memory systems that separate compute and memory units, addressing challenges like network bandwidth limitations and maintaining index accuracy.

Method: Proposing a graph-preserving distributed HNSW index with an efficient caching mechanism and cache-sharing among compute nodes to enhance scalability and efficiency.

Result: The proposed approach maintains the same accuracy as single-machine HNSW, significantly improves scalability, and effectively manages bandwidth constraints.

Conclusion: The integrated caching strategy and graph-preserving index enable scalable, high-accuracy ANN search in disaggregated memory architectures.

Abstract: Approximate nearest neighbor (ANN) search is a fundamental problem in
computer science for which in-memory graph-based methods, such as Hierarchical
Navigable Small World (HNSW), perform exceptionally well. To scale beyond
billions of high-dimensional vectors, the index must be distributed. The
disaggregated memory architecture physically separates compute and memory into
two distinct hardware units and has become popular in modern data centers. Both
units are connected via RDMA networks that allow compute nodes to directly
access remote memory and perform all the computations, posing unique challenges
for disaggregated indexes.
  In this work, we propose a scalable HNSW index for ANN search in
disaggregated memory. In contrast to existing distributed approaches, which
partition the graph at the cost of accuracy, our method builds a
graph-preserving index that reaches the same accuracy as a single-machine HNSW.
Continuously fetching high-dimensional vector data from remote memory leads to
severe network bandwidth limitations, which we overcome by employing an
efficient caching mechanism. Since answering a single query involves processing
numerous unique graph nodes, caching alone is not sufficient to achieve high
scalability. We logically combine the caches of the compute nodes to increase
the overall cache effectiveness and confirm the efficiency and scalability of
our method in our evaluation.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [35] [CQE under Epistemic Dependencies: Algorithms and Experiments (extended version)](https://arxiv.org/abs/2507.17487)
*Lorenzo Marconi,Flavia Ricci,Riccardo Rosati*

Main category: cs.AI

TL;DR: The paper explores Controlled Query Evaluation (CQE) over ontologies using epistemic dependencies (EDs) to regulate information disclosure, focusing on security and computational efficiency.


<details>
  <summary>Details</summary>
Motivation: To enhance security in knowledge query systems by integrating EDs with optimal censors, and to analyze computational properties for practical deployment.

Method: Combining EDs with optimal GA censors, characterizing security, focusing on certain classes of EDs and ontologies, and developing a first-order rewriting algorithm.

Result: Identified security conditions, established low computational complexity (AC^0) for certain cases, and demonstrated practical feasibility through experiments.

Conclusion: The study confirms the effectiveness and practicality of their approach for secure and efficient query answering over ontologies.

Abstract: We investigate Controlled Query Evaluation (CQE) over ontologies, where
information disclosure is regulated by epistemic dependencies (EDs), a family
of logical rules recently proposed for the CQE framework. In particular, we
combine EDs with the notion of optimal GA censors, i.e. maximal sets of ground
atoms that are entailed by the ontology and can be safely revealed. We focus on
answering Boolean unions of conjunctive queries (BUCQs) with respect to the
intersection of all optimal GA censors - an approach that has been shown in
other contexts to ensure strong security guarantees with favorable
computational behavior. First, we characterize the security of this
intersection-based approach and identify a class of EDs (namely, full EDs) for
which it remains safe. Then, for a subclass of EDs and for DL-Lite_R
ontologies, we show that answering BUCQs in the above CQE semantics is in AC^0
in data complexity by presenting a suitable, detailed first-order rewriting
algorithm. Finally, we report on experiments conducted in two different
evaluation scenarios, showing the practical feasibility of our rewriting
function.

</details>


### [36] [Towards Autonomous Sustainability Assessment via Multimodal AI Agents](https://arxiv.org/abs/2507.17012)
*Zhihan Zhang,Alexander Metzger,Yuxuan Mei,Felix Hähnlein,Zachary Englhardt,Tingyu Cheng,Gregory D. Abowd,Shwetak Patel,Adriana Schulz,Vikram Iyer*

Main category: cs.AI

TL;DR: This paper introduces multimodal AI agents to perform rapid, accurate life cycle assessments of electronic devices, significantly reducing expert effort and data gaps.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of limited data and time-consuming processes in traditional LCAs.

Method: Development of AI agents that combine data extraction from text and images, cluster-based EI estimation, and emission factor generation.

Result: AI approach reduces LCA time from months to under a minute, estimates within 19% of expert results, and improves emission factor accuracy by over 120%.

Conclusion: AI-driven methods can transform LCA workflows by providing rapid, accurate assessments, closing data gaps, and reducing reliance on proprietary data.

Abstract: Interest in sustainability information has surged in recent years. However,
the data required for a life cycle assessment (LCA) that maps the materials and
processes from product manufacturing to disposal into environmental impacts
(EI) are often unavailable. Here we reimagine conventional LCA by introducing
multimodal AI agents that emulate interactions between LCA experts and
stakeholders like product managers and engineers to calculate the
cradle-to-gate (production) carbon emissions of electronic devices. The AI
agents iteratively generate a detailed life-cycle inventory leveraging a custom
data abstraction and software tools that extract information from online text
and images from repair communities and government certifications. This approach
reduces weeks or months of expert time to under one minute and closes data
availability gaps while yielding carbon footprint estimates within 19% of
expert LCAs with zero proprietary data. Additionally, we develop a method to
directly estimate EI by comparing an input to a cluster of products with
similar descriptions and known carbon footprints. This runs in 3 ms on a laptop
with a MAPE of 12.28% on electronic products. Further, we develop a data-driven
method to generate emission factors. We use the properties of an unknown
material to represent it as a weighted sum of emission factors for similar
materials. Compared to human experts picking the closest LCA database entry,
this improves MAPE by 120.26%. We analyze the data and compute scaling of this
approach and discuss its implications for future LCA workflows.

</details>


### [37] [New Mechanisms in Flex Distribution for Bounded Suboptimal Multi-Agent Path Finding](https://arxiv.org/abs/2507.17054)
*Shao-Hung Chan,Thomy Phan,Jiaoyang Li,Sven Koenig*

Main category: cs.AI

TL;DR: This paper improves the efficiency of the EECBS algorithm for Multi-Agent Path Finding by introducing new flex distribution strategies that adaptively manage the search thresholds, leading to better performance while maintaining completeness and bounded sub-optimality.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enhance the efficiency of EECBS in solving MAPF problems without sacrificing its guarantees.

Method: The authors propose three new flex distribution methods—Conflict-Based Flex Distribution, Delay-Based Flex Distribution, and a hierarchical Mixed-Strategy Flex Distribution—and theoretically prove their properties.

Result: The new methods outperform the original flex distribution in experiments, demonstrating improved efficiency.

Conclusion: The proposed flex distribution strategies effectively speed up EECBS while preserving its completeness and solution quality.

Abstract: Multi-Agent Path Finding (MAPF) is the problem of finding a set of
collision-free paths, one for each agent in a shared environment. Its objective
is to minimize the sum of path costs (SOC), where the path cost of each agent
is defined as the travel time from its start location to its target location.
Explicit Estimation Conflict-Based Search (EECBS) is the leading algorithm for
bounded-suboptimal MAPF, with the SOC of the solution being at most a
user-specified factor $w$ away from optimal. EECBS maintains sets of paths and
a lower bound $LB$ on the optimal SOC. Then, it iteratively selects a set of
paths whose SOC is at most $w \cdot LB$ and introduces constraints to resolve
collisions. For each path in a set, EECBS maintains a lower bound on its
optimal path that satisfies constraints. By finding an individually
bounded-suboptimal path with cost at most a threshold of $w$ times its lower
bound, EECBS guarantees to find a bounded-suboptimal solution. To speed up
EECBS, previous work uses flex distribution to increase the threshold. Though
EECBS with flex distribution guarantees to find a bounded-suboptimal solution,
increasing the thresholds may push the SOC beyond $w \cdot LB$, forcing EECBS
to switch among different sets of paths instead of resolving collisions on a
particular set of paths, and thus reducing efficiency. To address this issue,
we propose Conflict-Based Flex Distribution that distributes flex in proportion
to the number of collisions. We also estimate the delays needed to satisfy
constraints and propose Delay-Based Flex Distribution. On top of that, we
propose Mixed-Strategy Flex Distribution, combining both in a hierarchical
framework. We prove that EECBS with our new flex distribution mechanisms is
complete and bounded-suboptimal. Our experiments show that our approaches
outperform the original (greedy) flex distribution.

</details>


### [38] [LoRA is All You Need for Safety Alignment of Reasoning LLMs](https://arxiv.org/abs/2507.17075)
*Yihao Xue,Baharan Mirzasoleiman*

Main category: cs.AI

TL;DR: Using LoRA for safety fine-tuning in LLMs effectively balances safety and reasoning, avoiding the 'Safety Tax' seen in full-model fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Enhance safety alignment of LLMs without degrading reasoning abilities.

Method: Applying LoRA to refusal datasets for safety fine-tuning, restricting weight updates to a low-rank space.

Result: Achieved safety levels comparable to full fine-tuning while preserving reasoning, with smaller weight update overlaps.

Conclusion: LoRA is effective for safe, reasoning-preserving LLM fine-tuning; further improvements can come from reducing weight overlap.

Abstract: Reasoning LLMs have demonstrated remarkable breakthroughs in solving complex
problems that were previously out of reach. To ensure LLMs do not assist with
harmful requests, safety alignment fine-tuning is necessary in the
post-training phase. However, safety alignment fine-tuning has recently been
shown to significantly degrade reasoning abilities, a phenomenon known as the
"Safety Tax". In this work, we show that using LoRA for SFT on refusal datasets
effectively aligns the model for safety without harming its reasoning
capabilities. This is because restricting the safety weight updates to a
low-rank space minimizes the interference with the reasoning weights. Our
extensive experiments across four benchmarks covering math, science, and coding
show that this approach produces highly safe LLMs -- with safety levels
comparable to full-model fine-tuning -- without compromising their reasoning
abilities. Additionally, we observe that LoRA induces weight updates with
smaller overlap with the initial weights compared to full-model fine-tuning. We
also explore methods that further reduce such overlap -- via regularization or
during weight merging -- and observe some improvement on certain tasks. We hope
this result motivates designing approaches that yield more consistent
improvements in the reasoning-safety trade-off.

</details>


### [39] [HySafe-AI: Hybrid Safety Architectural Analysis Framework for AI Systems: A Case Study](https://arxiv.org/abs/2507.17118)
*Mandar Pitale,Jelena Frtunikj,Abhinaw Priyadershi,Vasu Singh,Maria Spence*

Main category: cs.AI

TL;DR: The paper reviews AI architectures in safety-critical systems, evaluates safety analysis techniques, and proposes a hybrid framework, HySAFE-AI, for assessing AI safety.


<details>
  <summary>Details</summary>
Motivation: Ensure safety in AI systems, especially with complex, end-to-end models like LLMs and VLMs.

Method: Review architectures, evaluate safety analysis methods, and develop the HySAFE-AI framework.

Result: HySAFE-AI effectively adapts traditional safety analyses for modern AI architectures; future directions are discussed.

Conclusion: Hybrid safety analysis frameworks are essential for advancing safe deployment of AI in critical systems.

Abstract: AI has become integral to safety-critical areas like autonomous driving
systems (ADS) and robotics. The architecture of recent autonomous systems are
trending toward end-to-end (E2E) monolithic architectures such as large
language models (LLMs) and vision language models (VLMs). In this paper, we
review different architectural solutions and then evaluate the efficacy of
common safety analyses such as failure modes and effect analysis (FMEA) and
fault tree analysis (FTA). We show how these techniques can be improved for the
intricate nature of the foundational models, particularly in how they form and
utilize latent representations. We introduce HySAFE-AI, Hybrid Safety
Architectural Analysis Framework for AI Systems, a hybrid framework that adapts
traditional methods to evaluate the safety of AI systems. Lastly, we offer
hints of future work and suggestions to guide the evolution of future AI safety
standards.

</details>


### [40] [Improving LLMs' Generalized Reasoning Abilities by Graph Problems](https://arxiv.org/abs/2507.17168)
*Qifan Zhang,Nuo Chen,Zehua Li,Miao Peng,Jing Tang,Jia Li*

Main category: cs.AI

TL;DR: Introducing GraphPile, a large-scale dataset for Graph Problem Reasoning (GPR), to improve LLMs' reasoning across diverse tasks by domain-specific pretraining.


<details>
  <summary>Details</summary>
Motivation: Enhance the reasoning capabilities of LLMs, especially on novel and complex problems.

Method: Create GraphPile dataset with 10.9 billion tokens across 23 graph tasks; train models like Llama 3, 3.1, and Gemma 2 with GPR data.

Result: Up to 4.9% better in mathematical reasoning and 21.2% in logical/commonsense reasoning.

Conclusion: GPR-based pretraining with GraphPile bridges the gap between specialized training and universal reasoning, improving LLM robustness.

Abstract: Large Language Models (LLMs) have made remarkable strides in reasoning tasks,
yet their performance often falters on novel and complex problems.
Domain-specific continued pretraining (CPT) methods, such as those tailored for
mathematical reasoning, have shown promise but lack transferability to broader
reasoning tasks. In this work, we pioneer the use of Graph Problem Reasoning
(GPR) to enhance the general reasoning capabilities of LLMs. GPR tasks,
spanning pathfinding, network analysis, numerical computation, and topological
reasoning, require sophisticated logical and relational reasoning, making them
ideal for teaching diverse reasoning patterns. To achieve this, we introduce
GraphPile, the first large-scale corpus specifically designed for CPT using GPR
data. Spanning 10.9 billion tokens across 23 graph tasks, the dataset includes
chain-of-thought, program-of-thought, trace of execution, and real-world graph
data. Using GraphPile, we train GraphMind on popular base models Llama 3 and
3.1, as well as Gemma 2, achieving up to 4.9 percent higher accuracy in
mathematical reasoning and up to 21.2 percent improvement in non-mathematical
reasoning tasks such as logical and commonsense reasoning. By being the first
to harness GPR for enhancing reasoning patterns and introducing the first
dataset of its kind, our work bridges the gap between domain-specific
pretraining and universal reasoning capabilities, advancing the adaptability
and robustness of LLMs.

</details>


### [41] [Our Cars Can Talk: How IoT Brings AI to Vehicles](https://arxiv.org/abs/2507.17214)
*Amod Kant Agrawal*

Main category: cs.AI

TL;DR: Proposes integrating AI copilots in vehicles to enable proactive maintenance and improve user interaction.


<details>
  <summary>Details</summary>
Motivation: Transform vehicle maintenance from reactive to proactive by leveraging AI as sensing platforms.

Method: Provides a conceptual and technical perspective to guide future research in intelligent vehicle systems.

Result: Offers insights to foster interdisciplinary dialogue and development in AI-powered vehicle systems.

Conclusion: AI copilots speaking both machine and driver languages are crucial for advancing vehicle maintenance and interaction.

Abstract: Bringing AI to vehicles and enabling them as sensing platforms is key to
transforming maintenance from reactive to proactive. Now is the time to
integrate AI copilots that speak both languages: machine and driver. This
article offers a conceptual and technical perspective intended to spark
interdisciplinary dialogue and guide future research and development in
intelligent vehicle systems, predictive maintenance, and AI-powered user
interaction.

</details>


### [42] [Agent Identity Evals: Measuring Agentic Identity](https://arxiv.org/abs/2507.17257)
*Elija Perrier,Michael Timothy Bennett*

Main category: cs.AI

TL;DR: Introduction of Agent Identity Evals (AIE), a framework for measuring and maintaining the stability and reliability of language model agents over time.


<details>
  <summary>Details</summary>
Motivation: To ensure the reliability, trustworthiness, and effective agentic capabilities of language model agents by addressing inherited pathologies that undermine their identity and persistence.

Method: Development of a statistically-driven, empirical framework with novel metrics that measure the stability, identity, and recovery ability of LMAs, applicable throughout their lifecycle.

Result: A comprehensive set of metrics and formal definitions that evaluate and support the maintenance of agentic identity in LMAs, with practical application examples.

Conclusion: Agent Identity Evals provide a robust framework for enhancing the design, evaluation, and robustness of language model agents, fostering their long-term reliability and utility.

Abstract: Central to agentic capability and trustworthiness of language model agents
(LMAs) is the extent they maintain stable, reliable, identity over time.
However, LMAs inherit pathologies from large language models (LLMs)
(statelessness, stochasticity, sensitivity to prompts and
linguistically-intermediation) which can undermine their identifiability,
continuity, persistence and consistency. This attrition of identity can erode
their reliability, trustworthiness and utility by interfering with their
agentic capabilities such as reasoning, planning and action. To address these
challenges, we introduce \textit{agent identity evals} (AIE), a rigorous,
statistically-driven, empirical framework for measuring the degree to which an
LMA system exhibit and maintain their agentic identity over time, including
their capabilities, properties and ability to recover from state perturbations.
AIE comprises a set of novel metrics which can integrate with other measures of
performance, capability and agentic robustness to assist in the design of
optimal LMA infrastructure and scaffolding such as memory and tools. We set out
formal definitions and methods that can be applied at each stage of the LMA
life-cycle, and worked examples of how to apply them.

</details>


### [43] [Students' Feedback Requests and Interactions with the SCRIPT Chatbot: Do They Get What They Ask For?](https://arxiv.org/abs/2507.17258)
*Andreas Scholl,Natalie Kiesler*

Main category: cs.AI

TL;DR: This study develops SCRIPT, a ChatGPT-based chatbot for novice programming learners, and evaluates its interaction with students, revealing insights into feedback request patterns and system alignment.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enhance programming education using GenAI tools like chatbots, addressing how they can support novice learners effectively.

Method: The paper describes the development of SCRIPT and an experiment involving 136 students, analyzing interaction patterns and feedback preferences.

Result: Students' feedback requests followed a specific sequence; 75% of chatbot responses matched the requested feedback types and adhered to prompt constraints.

Conclusion: The findings guide the design of GenAI-driven learning systems, emphasizing the balance between guidance and flexibility.

Abstract: Building on prior research on Generative AI (GenAI) and related tools for
programming education, we developed SCRIPT, a chatbot based on ChatGPT-4o-mini,
to support novice learners. SCRIPT allows for open-ended interactions and
structured guidance through predefined prompts. We evaluated the tool via an
experiment with 136 students from an introductory programming course at a large
German university and analyzed how students interacted with SCRIPT while
solving programming tasks with a focus on their feedback preferences. The
results reveal that students' feedback requests seem to follow a specific
sequence. Moreover, the chatbot responses aligned well with students' requested
feedback types (in 75%), and it adhered to the system prompt constraints. These
insights inform the design of GenAI-based learning support systems and
highlight challenges in balancing guidance and flexibility in AI-assisted
tools.

</details>


### [44] [Compliance Brain Assistant: Conversational Agentic AI for Assisting Compliance Tasks in Enterprise Environments](https://arxiv.org/abs/2507.17289)
*Shitong Zhu,Chenhao Fang,Derek Larson,Neel Reddy Pochareddy,Rajeev Rao,Sophie Zeng,Yanqing Peng,Wendy Summer,Alex Goncalves,Arya Pudota,Herve Robert*

Main category: cs.AI

TL;DR: CBA, a conversational AI assistant, improves compliance task efficiency by intelligently routing queries between fast and full modes, significantly outperforming standard LLMs in accuracy metrics.


<details>
  <summary>Details</summary>
Motivation: Enhance compliance task efficiency in enterprise environments using AI.

Method: Design a query routing system with two modes for simple and complex requests, and evaluate its performance against standard LLMs.

Result: CBA outperforms vanilla LLM on key metrics, demonstrating effective trade-offs in response quality and response time.

Conclusion: Routing mechanism enhances AI assistant performance by balancing response accuracy and efficiency.

Abstract: This paper presents Compliance Brain Assistant (CBA), a conversational,
agentic AI assistant designed to boost the efficiency of daily compliance tasks
for personnel in enterprise environments. To strike a good balance between
response quality and latency, we design a user query router that can
intelligently choose between (i) FastTrack mode: to handle simple requests that
only need additional relevant context retrieved from knowledge corpora; and
(ii) FullAgentic mode: to handle complicated requests that need composite
actions and tool invocations to proactively discover context across various
compliance artifacts, and/or involving other APIs/models for accommodating
requests. A typical example would be to start with a user query, use its
description to find a specific entity and then use the entity's information to
query other APIs for curating and enriching the final AI response.
  Our experimental evaluations compared CBA against an out-of-the-box LLM on
various real-world privacy/compliance-related queries targeting various
personas. We found that CBA substantially improved upon the vanilla LLM's
performance on metrics such as average keyword match rate (83.7% vs. 41.7%) and
LLM-judge pass rate (82.0% vs. 20.0%). We also compared metrics for the full
routing-based design against the `fast-track only` and `full-agentic` modes and
found that it had a better average match-rate and pass-rate while keeping the
run-time approximately the same. This finding validated our hypothesis that the
routing mechanism leads to a good trade-off between the two worlds.

</details>


### [45] [Ctx2TrajGen: Traffic Context-Aware Microscale Vehicle Trajectories using Generative Adversarial Imitation Learning](https://arxiv.org/abs/2507.17418)
*Joobin Jin,Seokjun Hong,Gyeongseon Baek,Yeeun Kim,Byeongjoon Noh*

Main category: cs.AI

TL;DR: A framework for realistic vehicle trajectory synthesis using GAIL, conditioned on environment factors, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: To improve microscopic vehicle trajectory modeling for traffic analysis and autonomous driving.

Method: Using GAIL with PPO and WGAN-GP, explicitly conditioning on surrounding vehicles and road geometry to generate interaction-aware trajectories.

Result: Outperforms existing methods in realism, diversity, and fidelity on DRIFT dataset.

Conclusion: Provides a robust, context-aware approach that overcomes data scarcity and domain shift without simulation.

Abstract: Precise modeling of microscopic vehicle trajectories is critical for traffic
behavior analysis and autonomous driving systems. We propose Ctx2TrajGen, a
context-aware trajectory generation framework that synthesizes realistic urban
driving behaviors using GAIL. Leveraging PPO and WGAN-GP, our model addresses
nonlinear interdependencies and training instability inherent in microscopic
settings. By explicitly conditioning on surrounding vehicles and road geometry,
Ctx2TrajGen generates interaction-aware trajectories aligned with real-world
context. Experiments on the drone-captured DRIFT dataset demonstrate superior
performance over existing methods in terms of realism, behavioral diversity,
and contextual fidelity, offering a robust solution to data scarcity and domain
shift without simulation.

</details>


### [46] [An Uncertainty-Driven Adaptive Self-Alignment Framework for Large Language Models](https://arxiv.org/abs/2507.17477)
*Haoran Sun,Zekun Zhang,Shaoning Zeng*

Main category: cs.AI

TL;DR: UDASA is a framework that enhances LLM alignment with human intent by using uncertainty to guide self-adjustment without human labels.


<details>
  <summary>Details</summary>
Motivation: To improve LLM alignment and safety without relying on human annotations.

Method: Generate multiple responses per input, assess uncertainty in semantics, factuality, and values, and adapt training stages accordingly.

Result: UDASA outperforms existing methods in various alignment tasks, improving harmlessness, helpfulness, truthfulness, and sentiment control.

Conclusion: Uncertainty-driven self-alignment effectively enhances LLM alignment and safety, validating the core design and empirical benefits.

Abstract: Large Language Models (LLMs) have demonstrated remarkable progress in
instruction following and general-purpose reasoning. However, achieving
high-quality alignment with human intent and safety norms without human
annotations remains a fundamental challenge. In this work, we propose an
Uncertainty-Driven Adaptive Self-Alignment (UDASA) framework designed to
improve LLM alignment in a fully automated manner. UDASA first generates
multiple responses for each input and quantifies output uncertainty across
three dimensions: semantics, factuality, and value alignment. Based on these
uncertainty scores, the framework constructs preference pairs and categorizes
training samples into three stages, conservative, moderate, and exploratory,
according to their uncertainty difference. The model is then optimized
progressively across these stages. In addition, we conduct a series of
preliminary studies to validate the core design assumptions and provide strong
empirical motivation for the proposed framework. Experimental results show that
UDASA outperforms existing alignment methods across multiple tasks, including
harmlessness, helpfulness, truthfulness, and controlled sentiment generation,
significantly improving model performance.

</details>


### [47] [LTLZinc: a Benchmarking Framework for Continual Learning and Neuro-Symbolic Temporal Reasoning](https://arxiv.org/abs/2507.17482)
*Luca Salvatore Lorello,Nikolaos Manginas,Marco Lippi,Stefano Melacci*

Main category: cs.AI

TL;DR: Introduction of LTLZinc, a benchmarking framework for evaluating neuro-symbolic and continual learning methods on temporal reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: Addressing the gap in current neuro-symbolic AI research, which primarily focuses on static scenarios, by providing a tool for evaluating temporal reasoning and continual learning.

Method: Developing LTLZinc, a dataset generator based on linear temporal logic and MiniZinc constraints, capable of producing diverse temporal learning tasks, and conducting experiments on these datasets.

Result: Demonstrated the challenging nature of temporal learning and reasoning tasks, and revealed limitations of current methods; release of LTLZinc and datasets for research use.

Conclusion: LTLZinc fosters research in unified temporal learning and reasoning, encouraging advancements in neuro-symbolic and continual learning systems.

Abstract: Neuro-symbolic artificial intelligence aims to combine neural architectures
with symbolic approaches that can represent knowledge in a human-interpretable
formalism. Continual learning concerns with agents that expand their knowledge
over time, improving their skills while avoiding to forget previously learned
concepts. Most of the existing approaches for neuro-symbolic artificial
intelligence are applied to static scenarios only, and the challenging setting
where reasoning along the temporal dimension is necessary has been seldom
explored. In this work we introduce LTLZinc, a benchmarking framework that can
be used to generate datasets covering a variety of different problems, against
which neuro-symbolic and continual learning methods can be evaluated along the
temporal and constraint-driven dimensions. Our framework generates expressive
temporal reasoning and continual learning tasks from a linear temporal logic
specification over MiniZinc constraints, and arbitrary image classification
datasets. Fine-grained annotations allow multiple neural and neuro-symbolic
training settings on the same generated datasets. Experiments on six
neuro-symbolic sequence classification and four class-continual learning tasks
generated by LTLZinc, demonstrate the challenging nature of temporal learning
and reasoning, and highlight limitations of current state-of-the-art methods.
We release the LTLZinc generator and ten ready-to-use tasks to the
neuro-symbolic and continual learning communities, in the hope of fostering
research towards unified temporal learning and reasoning frameworks.

</details>


### [48] [Automated Hybrid Grounding Using Structural and Data-Driven Heuristics](https://arxiv.org/abs/2507.17493)
*Alexander Beiser,Markus Hecher,Stefan Woltran*

Main category: cs.AI

TL;DR: This paper introduces automated hybrid grounding for Answer Set Programming, which intelligently switches between standard and body-decoupled grounding using structural heuristics, improving performance on hard scenarios.


<details>
  <summary>Details</summary>
Motivation: To address the grounding bottleneck in Answer Set Programming, which limits its industrial adoption.

Method: Developing a splitting algorithm based on data-structural heuristics that detects the optimal grounding method according to rule structures.

Result: The automated hybrid grounding approach improves performance on challenging scenarios and approaches state-of-the-art on difficult instances.

Conclusion: Automated hybrid grounding effectively alleviates the grounding bottleneck, enhancing ASP scalability and practical applicability.

Abstract: The grounding bottleneck poses one of the key challenges that hinders the
widespread adoption of Answer Set Programming in industry. Hybrid Grounding is
a step in alleviating the bottleneck by combining the strength of standard
bottom-up grounding with recently proposed techniques where rule bodies are
decoupled during grounding. However, it has remained unclear when hybrid
grounding shall use body-decoupled grounding and when to use standard bottom-up
grounding. In this paper, we address this issue by developing automated hybrid
grounding: we introduce a splitting algorithm based on data-structural
heuristics that detects when to use body-decoupled grounding and when standard
grounding is beneficial. We base our heuristics on the structure of rules and
an estimation procedure that incorporates the data of the instance. The
experiments conducted on our prototypical implementation demonstrate promising
results, which show an improvement on hard-to-ground scenarios, whereas on
hard-to-solve instances we approach state-of-the-art performance.

</details>


### [49] [Can One Domain Help Others? A Data-Centric Study on Multi-Domain Reasoning via Reinforcement Learning](https://arxiv.org/abs/2507.17512)
*Yu Li,Zhuoshi Pan,Honglin Lin,Mengyuan Sun,Conghui He,Lijun Wu*

Main category: cs.AI

TL;DR: This paper investigates multi-domain reasoning in language models trained with reinforcement learning, focusing on mathematical reasoning, code generation, and logical puzzles, and explores domain interactions, training strategies, and model performance.


<details>
  <summary>Details</summary>
Motivation: Address the gap in understanding how multiple reasoning skills interact and develop under reinforcement learning in language models, aiming to improve integrated multi-domain reasoning.

Method: Conducts systematic experiments with the GRPO algorithm and Qwen-2.5-7B models across different domains, analyzing in-domain and cross-domain performance, training interactions, and effects of training configurations.

Result: Findings reveal how domain interactions influence reasoning capabilities, identify key factors for optimizing multi-domain reasoning, and compare the effects of different training approaches.

Conclusion: Provides insights into optimizing reinforcement learning methodologies to enhance multi-domain reasoning, guiding future research and application in LLM development.

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a
powerful paradigm for enhancing the reasoning capabilities of LLMs. Existing
research has predominantly concentrated on isolated reasoning domains such as
mathematical problem-solving, coding tasks, or logical reasoning. However, real
world reasoning scenarios inherently demand an integrated application of
multiple cognitive skills. Despite this, the interplay among these reasoning
skills under reinforcement learning remains poorly understood. To bridge this
gap, we present a systematic investigation of multi-domain reasoning within the
RLVR framework, explicitly focusing on three primary domains: mathematical
reasoning, code generation, and logical puzzle solving. We conduct a
comprehensive study comprising four key components: (1) Leveraging the GRPO
algorithm and the Qwen-2.5-7B model family, our study thoroughly evaluates the
models' in-domain improvements and cross-domain generalization capabilities
when trained on single-domain datasets. (2) Additionally, we examine the
intricate interactions including mutual enhancements and conflicts that emerge
during combined cross-domain training. (3) To further understand the influence
of SFT on RL, we also analyze and compare performance differences between base
and instruct models under identical RL configurations. (4) Furthermore, we
delve into critical RL training details, systematically exploring the impacts
of curriculum learning strategies, variations in reward design, and
language-specific factors. Through extensive experiments, our results offer
significant insights into the dynamics governing domain interactions, revealing
key factors influencing both specialized and generalizable reasoning
performance. These findings provide valuable guidance for optimizing RL
methodologies to foster comprehensive, multi-domain reasoning capabilities in
LLMs.

</details>


### [50] [TAI Scan Tool: A RAG-Based Tool With Minimalistic Input for Trustworthy AI Self-Assessment](https://arxiv.org/abs/2507.17514)
*Athanasios Davvetas,Xenia Ziouvelou,Ypatia Dami,Alexis Kaponis,Konstantina Giouvanopoulou,Michael Papademas*

Main category: cs.AI

TL;DR: The paper presents the TAI Scan Tool, a minimalistic, RAG-based self-assessment system for legal AI compliance, especially with the AI Act.


<details>
  <summary>Details</summary>
Motivation: To provide an accessible, efficient tool for AI risk assessment and compliance with legal standards.

Method: Developing a two-step assessment involving pre-screening and detailed analysis, with qualitative evaluation through use-case scenarios.

Result: The tool accurately predicts risk levels and retrieves relevant legal articles, demonstrating promising performance across different semantic groups.

Conclusion: The TAI Scan Tool effectively supports AI compliance, relying on comparison with high-risk scenarios consistent with the AI Act.

Abstract: This paper introduces the TAI Scan Tool, a RAG-based TAI self-assessment tool
with minimalistic input. The current version of the tool supports the legal TAI
assessment, with a particular emphasis on facilitating compliance with the AI
Act. It involves a two-step approach with a pre-screening and an assessment
phase. The assessment output of the system includes insight regarding the
risk-level of the AI system according to the AI Act, while at the same time
retrieving relevant articles to aid with compliance and notify on their
obligations. Our qualitative evaluation using use-case scenarios yields
promising results, correctly predicting risk levels while retrieving relevant
articles across three distinct semantic groups. Furthermore, interpretation of
results shows that the tool's reasoning relies on comparison with the setting
of high-risk systems, a behaviour attributed to their deployment requiring
careful consideration, and therefore frequently presented within the AI Act.

</details>


### [51] [Constructing Ophthalmic MLLM for Positioning-diagnosis Collaboration Through Clinical Cognitive Chain Reasoning](https://arxiv.org/abs/2507.17539)
*Xinyao Liu,Diping Song*

Main category: cs.AI

TL;DR: FundusExpert is a specialized multimodal large language model for ophthalmology that integrates localization and diagnosis reasoning capabilities, using a new dataset FundusGen. It achieves superior performance in ophthalmic question-answering and report generation, demonstrating effective data utilization and clinical alignment.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of specialized domain understanding in multimodal large language models, particularly in ophthalmology.

Method: Development of FundusExpert, fine-tuned with FundusGen dataset, utilizing an intelligent Fundus-Engine for localization, semantic expansion, and cognitive reasoning chains.

Result: FundusExpert surpasses existing models in ophthalmic question-answering and zero-shot report generation, with significant improvements in accuracy and clinical consistency, and reveals a scaling law between data quality and model capability.

Conclusion: FundusExpert offers a scalable and clinically aligned approach to bridging the visual-language gap in ophthalmology MLLMs, emphasizing the importance of region-level localization and high-quality annotations.

Abstract: Multimodal large language models (MLLMs) demonstrate significant potential in
the field of medical diagnosis. However, they face critical challenges in
specialized domains such as ophthalmology, particularly the fragmentation of
annotation granularity and inconsistencies in clinical reasoning logic, which
hinder precise cross-modal understanding. This paper introduces FundusExpert,
an ophthalmology-specific MLLM with integrated positioning-diagnosis reasoning
capabilities, along with FundusGen, a dataset constructed through the
intelligent Fundus-Engine system. Fundus-Engine automates localization and
leverages MLLM-based semantic expansion to integrate global disease
classification, local object detection, and fine-grained feature analysis
within a single fundus image. Additionally, by constructing a clinically
aligned cognitive chain, it guides the model to generate interpretable
reasoning paths. FundusExpert, fine-tuned with instruction data from FundusGen,
achieves the best performance in ophthalmic question-answering tasks,
surpassing the average accuracy of the 40B MedRegA by 26.6%. It also excels in
zero-shot report generation tasks, achieving a clinical consistency of 77.0%,
significantly outperforming GPT-4o's 47.6%. Furthermore, we reveal a scaling
law between data quality and model capability ($L \propto N^{0.068}$),
demonstrating that the cognitive alignment annotations in FundusGen enhance
data utilization efficiency. By integrating region-level localization with
diagnostic reasoning chains, our work develops a scalable, clinically-aligned
MLLM and explores a pathway toward bridging the visual-language gap in specific
MLLMs. Our project can be found at https://github.com/MeteorElf/FundusExpert.

</details>


### [52] [Simulating multiple human perspectives in socio-ecological systems using large language models](https://arxiv.org/abs/2507.17680)
*Yongchao Zeng,Calum Brown,Ioannis Kyriakou,Ronja Hotz,Mark Rounsevell*

Main category: cs.AI

TL;DR: HoPeS is a framework using LLM-powered agents to simulate diverse stakeholder perspectives in socio-ecological systems, aiding perspective-taking and decision-making.


<details>
  <summary>Details</summary>
Motivation: Understanding socio-ecological systems requires insights from multiple stakeholder perspectives, which are often difficult to access.

Method: Developed the HoPeS modeling framework with agents powered by large language models and a simulation protocol to facilitate perspective shifts.

Result: Created a prototype demonstrating perspective-taking in land use change context, revealing stakeholder conflicts and subjective experiences during simulation.

Conclusion: HoPeS shows potential to enhance interdisciplinary collaboration and exploration of stakeholder perspectives, with further refinement promising expanded application.

Abstract: Understanding socio-ecological systems requires insights from diverse
stakeholder perspectives, which are often hard to access. To enable
alternative, simulation-based exploration of different stakeholder
perspectives, we develop the HoPeS (Human-Oriented Perspective Shifting)
modelling framework. HoPeS employs agents powered by large language models
(LLMs) to represent various stakeholders; users can step into the agent roles
to experience perspectival differences. A simulation protocol serves as a
"scaffold" to streamline multiple perspective-taking simulations, supporting
users in reflecting on, transitioning between, and integrating across
perspectives. A prototype system is developed to demonstrate HoPeS in the
context of institutional dynamics and land use change, enabling both
narrative-driven and numerical experiments. In an illustrative experiment, a
user successively adopts the perspectives of a system observer and a researcher
- a role that analyses data from the embedded land use model to inform
evidence-based decision-making for other LLM agents representing various
institutions. Despite the user's effort to recommend technically sound
policies, discrepancies persist between the policy recommendation and
implementation due to stakeholders' competing advocacies, mirroring real-world
misalignment between researcher and policymaker perspectives. The user's
reflection highlights the subjective feelings of frustration and disappointment
as a researcher, especially due to the challenge of maintaining political
neutrality while attempting to gain political influence. Despite this, the user
exhibits high motivation to experiment with alternative narrative framing
strategies, suggesting the system's potential in exploring different
perspectives. Further system and protocol refinement are likely to enable new
forms of interdisciplinary collaboration in socio-ecological simulations.

</details>


### [53] [Symbiotic Agents: A Novel Paradigm for Trustworthy AGI-driven Networks](https://arxiv.org/abs/2507.17695)
*Ilias Chatzistefanidis,Navid Nikaein*

Main category: cs.AI

TL;DR: Introduction of a symbiotic paradigm combining LLMs and real-time optimization for trustworthy, AGI-driven 6G networks, significantly improving decision accuracy and resource management.


<details>
  <summary>Details</summary>
Motivation: To enhance 6G network management with autonomous, reasoning-capable agents using LLMs and AI.

Method: Designing symbiotic agents with input/output optimizers; implementing network optimizers and SLA negotiators; evaluating on a 5G testbed.

Result: Reduced decision errors fivefold; decreased GPU resources with smaller models; improved RAN utilization by 44%.

Conclusion: The symbiotic paradigm enables adaptable, efficient, and trustworthy AGI-driven networks.

Abstract: Large Language Model (LLM)-based autonomous agents are expected to play a
vital role in the evolution of 6G networks, by empowering real-time
decision-making related to management and service provisioning to end-users.
This shift facilitates the transition from a specialized intelligence approach,
where artificial intelligence (AI) algorithms handle isolated tasks, to
artificial general intelligence (AGI)-driven networks, where agents possess
broader reasoning capabilities and can manage diverse network functions. In
this paper, we introduce a novel agentic paradigm that combines LLMs with
real-time optimization algorithms towards Trustworthy AI, defined as symbiotic
agents. Optimizers at the LLM's input-level provide bounded uncertainty
steering for numerically precise tasks, whereas output-level optimizers
supervised by the LLM enable adaptive real-time control. We design and
implement two novel agent types including: (i) Radio Access Network optimizers,
and (ii) multi-agent negotiators for Service-Level Agreements (SLAs). We
further propose an end-to-end architecture for AGI networks and evaluate it on
a 5G testbed capturing channel fluctuations from moving vehicles. Results show
that symbiotic agents reduce decision errors fivefold compared to standalone
LLM-based agents, while smaller language models (SLM) achieve similar accuracy
with a 99.9% reduction in GPU resource overhead and in near-real-time loops of
82 ms. A multi-agent demonstration for collaborative RAN on the real-world
testbed highlights significant flexibility in service-level agreement and
resource allocation, reducing RAN over-utilization by approximately 44%.
Drawing on our findings and open-source implementations, we introduce the
symbiotic paradigm as the foundation for next-generation, AGI-driven
networks-systems designed to remain adaptable, efficient, and trustworthy even
as LLMs advance.

</details>


### [54] [Thinking Isn't an Illusion: Overcoming the Limitations of Reasoning Models via Tool Augmentations](https://arxiv.org/abs/2507.17699)
*Zhao Song,Song Yue,Jiahao Zhang*

Main category: cs.AI

TL;DR: Tool augmentation enhances the reasoning capabilities of Large Reasoning Models, surpassing non-reasoning models across various complexities.


<details>
  <summary>Details</summary>
Motivation: To revisit the effectiveness of reasoning processes in LRMs, especially with tool augmentation, against recent findings suggesting their limitations.

Method: Incorporating tools like Python interpreters and scratchpads into three LLMs and their LRM counterparts, then evaluating on Apple's benchmark reasoning puzzles.

Result: Tool-augmented LRMs outperform non-reasoning models across all task complexities, challenging previous claims about the limitations of reasoning in LLMs.

Conclusion: Proper tool integration can significantly improve LRM performance, highlighting the potential of reasoning strategies when augmented with tools.

Abstract: Large Reasoning Models (LRMs) have become a central focus in today's large
language model (LLM) research, where models are designed to output a
step-by-step thinking process before arriving at a final answer to handle
complex reasoning tasks. Despite their promise, recent empirical studies (e.g.,
[Shojaee et al., 2025] from Apple) suggest that this thinking process may not
actually enhance reasoning ability, where LLMs without explicit reasoning
actually outperform LRMs on tasks with low or high complexity. In this work, we
revisit these findings and investigate whether the limitations of LRMs persist
when tool augmentations are introduced. We incorporate two types of tools,
Python interpreters and scratchpads, and evaluate three representative LLMs and
their LRM counterparts on Apple's benchmark reasoning puzzles. Our results show
that, with proper tool use, LRMs consistently outperform their non-reasoning
counterparts across all levels of task complexity. These findings challenge the
recent narrative that reasoning is an illusion and highlight the potential of
tool-augmented LRMs for solving complex problems.

</details>


### [55] [Online Submission and Evaluation System Design for Competition Operations](https://arxiv.org/abs/2507.17730)
*Zhe Chen,Daniel Harabor,Ryan Hechnenberger,Nathan R. Sturtevant*

Main category: cs.AI

TL;DR: An online competition system automates submissions and evaluations to track research progress efficiently, reducing operational burdens.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of managing large volumes of submissions and compatibility issues in research competitions.

Method: Designing and implementing an online system that automates submission management and isolated environment evaluations.

Result: The system has been successfully used in multiple competitions, improving efficiency.

Conclusion: Automating competition processes facilitates better tracking of research advancements and reduces operational challenges.

Abstract: Research communities have developed benchmark datasets across domains to
compare the performance of algorithms and techniques However, tracking the
progress in these research areas is not easy, as publications appear in
different venues at the same time, and many of them claim to represent the
state-of-the-art. To address this, research communities often organise periodic
competitions to evaluate the performance of various algorithms and techniques,
thereby tracking advancements in the field. However, these competitions pose a
significant operational burden. The organisers must manage and evaluate a large
volume of submissions. Furthermore, participants typically develop their
solutions in diverse environments, leading to compatibility issues during the
evaluation of their submissions. This paper presents an online competition
system that automates the submission and evaluation process for a competition.
The competition system allows organisers to manage large numbers of submissions
efficiently, utilising isolated environments to evaluate submissions. This
system has already been used successfully for several competitions, including
the Grid-Based Pathfinding Competition and the League of Robot Runners
competition.

</details>
