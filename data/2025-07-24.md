<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 31]
- [cs.DB](#cs.DB) [Total: 3]
- [cs.AI](#cs.AI) [Total: 21]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [A Unifying Scheme for Extractive Content Selection Tasks](https://arxiv.org/abs/2507.16922)
*Shmuel Amar,Ori Shapira,Aviv Slobodkin,Ido Dagan*

Main category: cs.CL

TL;DR: The paper introduces a unified framework called instruction-guided content selection (IGCS) for NLP tasks involving selecting relevant text spans, supported by a new benchmark and synthetic dataset, which improve performance via transfer learning.


<details>
  <summary>Details</summary>
Motivation: To unify various content selection tasks in NLP that have traditionally been studied separately.

Method: Proposing IGCS as a unified approach using instructions, creating a comprehensive benchmark (igcsbench), and developing a synthetic dataset for transfer learning.

Result: Transfer learning with the synthetic dataset enhances performance across content selection tasks; the paper also addresses inference and evaluation challenges.

Conclusion: The resources and methods introduced facilitate future research and development in content selection tasks in NLP.

Abstract: A broad range of NLP tasks involve selecting relevant text spans from given
source texts. Despite this shared objective, such \textit{content selection}
tasks have traditionally been studied in isolation, each with its own modeling
approaches, datasets, and evaluation metrics. In this work, we propose
\textit{instruction-guided content selection (IGCS)} as a beneficial unified
framework for such settings, where the task definition and any
instance-specific request are encapsulated as instructions to a language model.
To promote this framework, we introduce \igcsbench{}, the first unified
benchmark covering diverse content selection tasks. Further, we create a large
generic synthetic dataset that can be leveraged for diverse content selection
tasks, and show that transfer learning with these datasets often boosts
performance, whether dedicated training for the targeted task is available or
not. Finally, we address generic inference time issues that arise in LLM-based
modeling of content selection, assess a generic evaluation metric, and overall
propose the utility of our resources and methods for future content selection
models. Models and datasets available at https://github.com/shmuelamar/igcs.

</details>


### [2] [AI-based Clinical Decision Support for Primary Care: A Real-World Study](https://arxiv.org/abs/2507.16947)
*Robert Korom,Sarah Kiptinness,Najib Adan,Kassim Said,Catherine Ithuli,Oliver Rotich,Boniface Kimani,Irene King'ori,Stellah Kamau,Elizabeth Atemba,Muna Aden,Preston Bowman,Michael Sharman,Rebecca Soskin Hicks,Rebecca Distler,Johannes Heidecke,Rahul K. Arora,Karan Singhal*

Main category: cs.CL

TL;DR: AI Consult, a large language model-based clinical decision support tool, reduces diagnostic and treatment errors in primary care settings in Kenya, improving care quality and showing potential for wider adoption.


<details>
  <summary>Details</summary>
Motivation: To evaluate the impact of LLM-based clinical decision support in live care settings, addressing patient safety and clinician decision-making accuracy.

Method: A quality improvement study comparing outcomes from 39,849 patient visits across clinics with and without AI Consult, with errors rated by independent physicians.

Result: Clinicians with AI Consult made 16% fewer diagnostic errors and 13% fewer treatment errors; estimated error reductions could prevent tens of thousands of errors annually at Penda.

Conclusion: AI Consult positively impacts error reduction and care quality, demonstrating the potential for responsible adoption of LLM-based tools in healthcare.

Abstract: We evaluate the impact of large language model-based clinical decision
support in live care. In partnership with Penda Health, a network of primary
care clinics in Nairobi, Kenya, we studied AI Consult, a tool that serves as a
safety net for clinicians by identifying potential documentation and clinical
decision-making errors. AI Consult integrates into clinician workflows,
activating only when needed and preserving clinician autonomy. We conducted a
quality improvement study, comparing outcomes for 39,849 patient visits
performed by clinicians with or without access to AI Consult across 15 clinics.
Visits were rated by independent physicians to identify clinical errors.
Clinicians with access to AI Consult made relatively fewer errors: 16% fewer
diagnostic errors and 13% fewer treatment errors. In absolute terms, the
introduction of AI Consult would avert diagnostic errors in 22,000 visits and
treatment errors in 29,000 visits annually at Penda alone. In a survey of
clinicians with AI Consult, all clinicians said that AI Consult improved the
quality of care they delivered, with 75% saying the effect was "substantial".
These results required a clinical workflow-aligned AI Consult implementation
and active deployment to encourage clinician uptake. We hope this study
demonstrates the potential for LLM-based clinical decision support tools to
reduce errors in real-world settings and provides a practical framework for
advancing responsible adoption.

</details>


### [3] [Harnessing RLHF for Robust Unanswerability Recognition and Trustworthy Response Generation in LLMs](https://arxiv.org/abs/2507.16951)
*Shuyuan Lin,Lei Duan,Philip Hughes,Yuxuan Sheng*

Main category: cs.CL

TL;DR: SALU integrates unanswerability detection within LLMs, using multi-task learning and reinforcement learning with human feedback, to improve reliability in conversational search by reducing hallucinations.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of handling unanswerable questions in CIR systems and reduce hallucinations.

Method: Multi-task training within the LLM for QA and abstention, combined with confidence-score-guided RLHF for self-awareness.

Result: SALU outperforms baselines on the C-IR_Answerability dataset, with higher accuracy and reduced hallucinations; human evaluation confirms it is more reliable.

Conclusion: Embedding unanswerability detection directly in LLMs via SALU enhances conversational AI's accuracy and trustworthiness.

Abstract: Conversational Information Retrieval (CIR) systems, while offering intuitive
access to information, face a significant challenge: reliably handling
unanswerable questions to prevent the generation of misleading or hallucinated
content. Traditional approaches often rely on external classifiers, which can
introduce inconsistencies with the core generative Large Language Models
(LLMs). This paper introduces Self-Aware LLM for Unanswerability (SALU), a
novel approach that deeply integrates unanswerability detection directly within
the LLM's generative process. SALU is trained using a multi-task learning
framework for both standard Question Answering (QA) and explicit abstention
generation for unanswerable queries. Crucially, it incorporates a
confidence-score-guided reinforcement learning with human feedback (RLHF)
phase, which explicitly penalizes hallucinated responses and rewards
appropriate abstentions, fostering intrinsic self-awareness of knowledge
boundaries. Through extensive experiments on our custom-built
C-IR_Answerability dataset, SALU consistently outperforms strong baselines,
including hybrid LLM-classifier systems, in overall accuracy for correctly
answering or abstaining from questions. Human evaluation further confirms
SALU's superior reliability, achieving high scores in factuality, appropriate
abstention, and, most importantly, a dramatic reduction in hallucination,
demonstrating its ability to robustly "know when to say 'I don't know'."

</details>


### [4] [Text-to-SPARQL Goes Beyond English: Multilingual Question Answering Over Knowledge Graphs through Human-Inspired Reasoning](https://arxiv.org/abs/2507.16971)
*Aleksandr Perevalov,Andreas Both*

Main category: cs.CL

TL;DR: mKGQAgent, a modular framework inspired by human reasoning, effectively converts multilingual natural language questions into SPARQL queries, outperforming others in benchmarks.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of querying structured knowledge graphs through natural language interfaces, especially in multilingual settings.

Method: Decompose the task into interpretable subtasks using a coordinated multi-component LLM workflow with an experience pool for in-context learning.

Result: Achieved first place in the Text2SPARQL 2025 challenge benchmarks on DBpedia and Corporate knowledge graphs.

Conclusion: This modular, human-inspired approach advances multilingual semantic parsing and reasoning systems.

Abstract: Accessing knowledge via multilingual natural-language interfaces is one of
the emerging challenges in the field of information retrieval and related ones.
Structured knowledge stored in knowledge graphs can be queried via a specific
query language (e.g., SPARQL). Therefore, one needs to transform
natural-language input into a query to fulfill an information need. Prior
approaches mostly focused on combining components (e.g., rule-based or
neural-based) that solve downstream tasks and come up with an answer at the
end. We introduce mKGQAgent, a human-inspired framework that breaks down the
task of converting natural language questions into SPARQL queries into modular,
interpretable subtasks. By leveraging a coordinated LLM agent workflow for
planning, entity linking, and query refinement - guided by an experience pool
for in-context learning - mKGQAgent efficiently handles multilingual KGQA.
Evaluated on the DBpedia- and Corporate-based KGQA benchmarks within the
Text2SPARQL challenge 2025, our approach took first place among the other
participants. This work opens new avenues for developing human-like reasoning
systems in multilingual semantic parsing.

</details>


### [5] [Leveraging Synthetic Data for Question Answering with Multilingual LLMs in the Agricultural Domain](https://arxiv.org/abs/2507.16974)
*Rishemjit Kaur,Arshdeep Singh Bhankhar,Surangika Ranathunga,Jashanpreet Singh Salh,Sudhir Rajput,Vidhi,Kashish Mahendra,Bhavika Berwal,Ritesh Kumar*

Main category: cs.CL

TL;DR: The study enhances multilingual agricultural question-answering systems by generating synthetic datasets and fine-tuning language-specific large language models, leading to improved accuracy and relevance in agriculture advice.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of general-purpose LLMs in providing accurate, localized, and multilingual agricultural advice due to insufficient domain-specific data.

Method: Generating synthetic multilingual agricultural datasets from domain documents and fine-tuning language-specific LLMs.

Result: Marked improvements in factual accuracy, relevance, and consensus, demonstrating the effectiveness of synthetic data-driven, language-specific fine-tuning.

Conclusion: Synthetic data and language-specific fine-tuning significantly enhance the performance of LLMs in delivering localized agricultural advisory services, especially in multilingual and resource-scarce settings.

Abstract: Enabling farmers to access accurate agriculture-related information in their
native languages in a timely manner is crucial for the success of the
agriculture field. Although large language models (LLMs) can be used to
implement Question Answering (QA) systems, simply using publicly available
general-purpose LLMs in agriculture typically offer generic advisories, lacking
precision in local and multilingual contexts due to insufficient
domain-specific training and scarcity of high-quality, region-specific
datasets. Our study addresses these limitations by generating multilingual
synthetic agricultural datasets (English, Hindi, Punjabi) from
agriculture-specific documents and fine-tuning language-specific LLMs. Our
evaluation on curated multilingual datasets demonstrates significant
improvements in factual accuracy, relevance, and agricultural consensus for the
fine-tuned models compared to their baseline counterparts. These results
highlight the efficacy of synthetic data-driven, language-specific fine-tuning
as an effective strategy to improve the performance of LLMs in agriculture,
especially in multilingual and low-resource settings. By enabling more accurate
and localized agricultural advisory services, this study provides a meaningful
step toward bridging the knowledge gap in AI-driven agricultural solutions for
diverse linguistic communities.

</details>


### [6] [Obscured but Not Erased: Evaluating Nationality Bias in LLMs via Name-Based Bias Benchmarks](https://arxiv.org/abs/2507.16989)
*Giulio Pelosio,Devesh Batra,Noémie Bovey,Robert Hankache,Cristovao Iglesias,Greig Cowan,Raad Khraishi*

Main category: cs.CL

TL;DR: Small LLMs exhibit higher bias and lower accuracy than larger models, especially with culturally indicative names, revealing persistent biases and error retention.


<details>
  <summary>Details</summary>
Motivation: To investigate how substituting explicit nationality labels with culturally indicative names impacts bias and accuracy in LLMs.

Method: Benchmarking across multiple industry-leading LLMs using a name-based adaptation of the BBQ dataset, assessing bias and accuracy differences.

Result: Smaller models show higher bias levels and retain more errors; larger models like Claude Sonnet outperform smaller ones such as Claude Haiku significantly in both bias mitigation and accuracy.

Conclusion: Biases in LLMs are resilient and have significant implications for their deployment in diverse global contexts.

Abstract: Large Language Models (LLMs) can exhibit latent biases towards specific
nationalities even when explicit demographic markers are not present. In this
work, we introduce a novel name-based benchmarking approach derived from the
Bias Benchmark for QA (BBQ) dataset to investigate the impact of substituting
explicit nationality labels with culturally indicative names, a scenario more
reflective of real-world LLM applications. Our novel approach examines how this
substitution affects both bias magnitude and accuracy across a spectrum of LLMs
from industry leaders such as OpenAI, Google, and Anthropic. Our experiments
show that small models are less accurate and exhibit more bias compared to
their larger counterparts. For instance, on our name-based dataset and in the
ambiguous context (where the correct choice is not revealed), Claude Haiku
exhibited the worst stereotypical bias scores of 9%, compared to only 3.5% for
its larger counterpart, Claude Sonnet, where the latter also outperformed it by
117.7% in accuracy. Additionally, we find that small models retain a larger
portion of existing errors in these ambiguous contexts. For example, after
substituting names for explicit nationality references, GPT-4o retains 68% of
the error rate versus 76% for GPT-4o-mini, with similar findings for other
model providers, in the ambiguous context. Our research highlights the stubborn
resilience of biases in LLMs, underscoring their profound implications for the
development and deployment of AI systems in diverse, global contexts.

</details>


### [7] [Multi-Label Classification with Generative AI Models in Healthcare: A Case Study of Suicidality and Risk Factors](https://arxiv.org/abs/2507.17009)
*Ming Huang,Zehan Li,Yan Hu,Wanjing Wang,Andrew Wen,Scott Lane,Salih Selek,Lokesh Shahani,Rodrigo Machado-Vieira,Jair Soares,Hua Xu,Hongfang Liu*

Main category: cs.CL

TL;DR: Study employs GPT-3.5 and GPT-4.5 to perform multi-label classification of suicidality-related factors in psychiatric EHRs, showcasing improved performance and new evaluation methods.


<details>
  <summary>Details</summary>
Motivation: To address the complexity of co-occurring suicidality-related factors and improve early identification using AI.

Method: Developed a generative multi-label classification pipeline using fine-tuned GPT models and advanced evaluation metrics.

Result: Achieved high accuracy and F1 scores, with GPT-4.5 showing robust performance across label sets, revealing error patterns and cautious over-labeling behaviors.

Conclusion: Generative AI models are feasible for complex clinical classification and can aid structuring unstructured EHR data for research and clinical decisions.

Abstract: Suicide remains a pressing global health crisis, with over 720,000 deaths
annually and millions more affected by suicide ideation (SI) and suicide
attempts (SA). Early identification of suicidality-related factors (SrFs),
including SI, SA, exposure to suicide (ES), and non-suicidal self-injury
(NSSI), is critical for timely intervention. While prior studies have applied
AI to detect SrFs in clinical notes, most treat suicidality as a binary
classification task, overlooking the complexity of cooccurring risk factors.
This study explores the use of generative large language models (LLMs),
specifically GPT-3.5 and GPT-4.5, for multi-label classification (MLC) of SrFs
from psychiatric electronic health records (EHRs). We present a novel end to
end generative MLC pipeline and introduce advanced evaluation methods,
including label set level metrics and a multilabel confusion matrix for error
analysis. Finetuned GPT-3.5 achieved top performance with 0.94 partial match
accuracy and 0.91 F1 score, while GPT-4.5 with guided prompting showed superior
performance across label sets, including rare or minority label sets,
indicating a more balanced and robust performance. Our findings reveal
systematic error patterns, such as the conflation of SI and SA, and highlight
the models tendency toward cautious over labeling. This work not only
demonstrates the feasibility of using generative AI for complex clinical
classification tasks but also provides a blueprint for structuring unstructured
EHR data to support large scale clinical research and evidence based medicine.

</details>


### [8] [Can External Validation Tools Improve Annotation Quality for LLM-as-a-Judge?](https://arxiv.org/abs/2507.17015)
*Arduin Findeis,Floris Weers,Guoli Yin,Ke Ye,Ruoming Pang,Tom Gunter*

Main category: cs.CL

TL;DR: The paper proposes using tool-using agents to improve AI annotation quality for complex response domains such as factual, math, and coding tasks, by leveraging external validation tools like web-search and code execution.


<details>
  <summary>Details</summary>
Motivation: Enhance the accuracy of human and AI annotations in complex response domains where standard metrics are insufficient.

Method: Develop a system that incorporates external tools (web-search, code execution) to ground responses and improve feedback quality.

Result: External tools generally improve annotation performance across multiple datasets, though results are sensitive to parameters and not universally effective.

Conclusion: Using external tools in annotation systems can enhance performance, but more robust benchmarks and parameter tuning are needed.

Abstract: Pairwise preferences over model responses are widely collected to evaluate
and provide feedback to large language models (LLMs). Given two alternative
model responses to the same input, a human or AI annotator selects the "better"
response. This approach can provide feedback for domains where other hard-coded
metrics are difficult to obtain (e.g., chat response quality), thereby helping
model evaluation or training. However, for some domains high-quality pairwise
comparisons can be tricky to obtain - from AI and humans. For example, for
responses with many factual statements, annotators may disproportionately weigh
writing quality rather than underlying facts. In this work, we explore
augmenting standard AI annotator systems with additional tools to improve
performance on three challenging response domains: long-form factual, math and
code tasks. We propose a tool-using agentic system to provide higher quality
feedback on these domains. Our system uses web-search and code execution to
ground itself based on external validation, independent of the LLM's internal
knowledge and biases. We provide extensive experimental results evaluating our
method across the three targeted response domains as well as general annotation
tasks, using RewardBench (incl. AlpacaEval and LLMBar), RewardMath, as well as
three new datasets for domains with saturated pre-existing datasets. Our
results indicate that external tools can indeed improve performance in many,
but not all, cases. More generally, our experiments highlight the sensitivity
of performance to simple parameters (e.g., prompt) and the need for improved
(non-saturated) annotator benchmarks. We share our code at
https://github.com/apple/ml-agent-evaluator.

</details>


### [9] [Evolutionary Feature-wise Thresholding for Binary Representation of NLP Embeddings](https://arxiv.org/abs/2507.17025)
*Soumen Sinha,Shahryar Rahnamayan,Azam Asilian Bidgoli*

Main category: cs.CL

TL;DR: The paper presents a method to optimize binary text embeddings for NLP, using coordinate search to find feature-specific thresholds, resulting in improved accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Efficient text embedding is vital for large-scale NLP applications due to storage and computational constraints.

Method: They use coordinate search to optimize individual thresholds for binarizing continuous embeddings, rather than fixed thresholds.

Result: Optimized binary embeddings outperform traditional methods in accuracy across various NLP tasks and datasets.

Conclusion: Feature-specific threshold optimization enhances binary embedding quality, with potential applications beyond NLP.

Abstract: Efficient text embedding is crucial for large-scale natural language
processing (NLP) applications, where storage and computational efficiency are
key concerns. In this paper, we explore how using binary representations
(barcodes) instead of real-valued features can be used for NLP embeddings
derived from machine learning models such as BERT. Thresholding is a common
method for converting continuous embeddings into binary representations, often
using a fixed threshold across all features. We propose a Coordinate
Search-based optimization framework that instead identifies the optimal
threshold for each feature, demonstrating that feature-specific thresholds lead
to improved performance in binary encoding. This ensures that the binary
representations are both accurate and efficient, enhancing performance across
various features. Our optimal barcode representations have shown promising
results in various NLP applications, demonstrating their potential to transform
text representation. We conducted extensive experiments and statistical tests
on different NLP tasks and datasets to evaluate our approach and compare it to
other thresholding methods. Binary embeddings generated using using optimal
thresholds found by our method outperform traditional binarization methods in
accuracy. This technique for generating binary representations is versatile and
can be applied to any features, not just limited to NLP embeddings, making it
useful for a wide range of domains in machine learning applications.

</details>


### [10] [CogDual: Enhancing Dual Cognition of LLMs via Reinforcement Learning with Implicit Rule-Based Rewards](https://arxiv.org/abs/2507.17147)
*Cheng Liu,Yifei Lu,Fanghua Ye,Jian Li,Xingyu Chen,Feiliang Ren,Zhaopeng Tu,Xiaolong Li*

Main category: cs.CL

TL;DR: CogDual is a new role-playing language model that mimics human-like cognitive processes to improve character consistency and adapt across various tasks.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing RPLAs that neglect underlying cognitive mechanisms, inspired by cognitive psychology.

Method: Introducing CogDual, which adopts a 'cognize-then-respond' paradigm, modeling external and internal awareness, combined with reinforcement learning for optimization.

Result: CogDual outperforms existing baselines on multiple benchmarks and generalizes well across different role-playing tasks.

Conclusion: Incorporating cognitive-inspired mechanisms enhances RPLA performance and consistency.

Abstract: Role-Playing Language Agents (RPLAs) have emerged as a significant
application direction for Large Language Models (LLMs). Existing approaches
typically rely on prompt engineering or supervised fine-tuning to enable models
to imitate character behaviors in specific scenarios, but often neglect the
underlying \emph{cognitive} mechanisms driving these behaviors. Inspired by
cognitive psychology, we introduce \textbf{CogDual}, a novel RPLA adopting a
\textit{cognize-then-respond } reasoning paradigm. By jointly modeling external
situational awareness and internal self-awareness, CogDual generates responses
with improved character consistency and contextual alignment. To further
optimize the performance, we employ reinforcement learning with two
general-purpose reward schemes designed for open-domain text generation.
Extensive experiments on the CoSER benchmark, as well as Cross-MR and
LifeChoice, demonstrate that CogDual consistently outperforms existing
baselines and generalizes effectively across diverse role-playing tasks.

</details>


### [11] [SKA-Bench: A Fine-Grained Benchmark for Evaluating Structured Knowledge Understanding of LLMs](https://arxiv.org/abs/2507.17178)
*Zhiqiang Liu,Enpei Niu,Yin Hua,Mengshu Sun,Lei Liang,Huajun Chen,Wen Zhang*

Main category: cs.CL

TL;DR: The paper proposes SKA-Bench, a comprehensive benchmark to evaluate LLMs' understanding of different structured knowledge forms, revealing significant gaps in their capabilities.


<details>
  <summary>Details</summary>
Motivation: To address the non-rigorous and limited evaluation of LLMs' understanding of structured knowledge, and to diagnose their specific shortcomings across various knowledge types.

Method: Development of SKA-Bench, a QA benchmark covering multiple knowledge forms (KG, Table, KG+Text, Table+Text), with a three-stage construction process and four fine-grained ability testbeds for evaluation of LLMs.

Result: Empirical testing of 8 LLMs shows that current models struggle with structured knowledge understanding, with performance impacted by noise, order of knowledge units, and hallucinations.

Conclusion: Existing LLMs have notable deficiencies in understanding structured knowledge, highlighting the need for improved models and evaluation methods.

Abstract: Although large language models (LLMs) have made significant progress in
understanding Structured Knowledge (SK) like KG and Table, existing evaluations
for SK understanding are non-rigorous (i.e., lacking evaluations of specific
capabilities) and focus on a single type of SK. Therefore, we aim to propose a
more comprehensive and rigorous structured knowledge understanding benchmark to
diagnose the shortcomings of LLMs. In this paper, we introduce SKA-Bench, a
Structured Knowledge Augmented QA Benchmark that encompasses four widely used
structured knowledge forms: KG, Table, KG+Text, and Table+Text. We utilize a
three-stage pipeline to construct SKA-Bench instances, which includes a
question, an answer, positive knowledge units, and noisy knowledge units. To
evaluate the SK understanding capabilities of LLMs in a fine-grained manner, we
expand the instances into four fundamental ability testbeds: Noise Robustness,
Order Insensitivity, Information Integration, and Negative Rejection. Empirical
evaluations on 8 representative LLMs, including the advanced DeepSeek-R1,
indicate that existing LLMs still face significant challenges in understanding
structured knowledge, and their performance is influenced by factors such as
the amount of noise, the order of knowledge units, and hallucination
phenomenon. Our dataset and code are available at
https://github.com/Lza12a/SKA-Bench.

</details>


### [12] [FinGAIA: An End-to-End Benchmark for Evaluating AI Agents in Finance](https://arxiv.org/abs/2507.17186)
*Lingfeng Zeng,Fangqi Lou,Zixuan Wang,Jiajie Xu,Jinyi Niu,Mengping Li,Yifan Dong,Qi Qi,Wei Zhang,Ziwei Yang,Jun Han,Ruilun Feng,Ruiqi Hu,Lejie Zhang,Zhengbo Feng,Yicheng Ren,Xin Guo,Zhaowei Liu,Dongpo Cheng,Weige Cai,Liwen Zhang*

Main category: cs.CL

TL;DR: Introduces FinGAIA, a benchmark for evaluating AI agents' capabilities in financial tasks, revealing current limitations and guiding future research.


<details>
  <summary>Details</summary>
Motivation: To assess and improve AI agents' abilities in the financial sector, which is underexplored.

Method: Developed a comprehensive benchmark with 407 tasks across seven financial sub-domains, evaluating 10 AI agents in a zero-shot setting.

Result: ChatGPT achieved 48.9% accuracy, surpassing non-professionals but lagging behind experts. Error patterns identified highlight key challenges.

Conclusion: FinGAIA provides a valuable benchmark, guiding future advancements in AI for finance.

Abstract: The booming development of AI agents presents unprecedented opportunities for
automating complex tasks across various domains. However, their multi-step,
multi-tool collaboration capabilities in the financial sector remain
underexplored. This paper introduces FinGAIA, an end-to-end benchmark designed
to evaluate the practical abilities of AI agents in the financial domain.
FinGAIA comprises 407 meticulously crafted tasks, spanning seven major
financial sub-domains: securities, funds, banking, insurance, futures, trusts,
and asset management. These tasks are organized into three hierarchical levels
of scenario depth: basic business analysis, asset decision support, and
strategic risk management. We evaluated 10 mainstream AI agents in a zero-shot
setting. The best-performing agent, ChatGPT, achieved an overall accuracy of
48.9\%, which, while superior to non-professionals, still lags financial
experts by over 35 percentage points. Error analysis has revealed five
recurring failure patterns: Cross-modal Alignment Deficiency, Financial
Terminological Bias, Operational Process Awareness Barrier, among others. These
patterns point to crucial directions for future research. Our work provides the
first agent benchmark closely related to the financial domain, aiming to
objectively assess and promote the development of agents in this crucial field.
Partial data is available at https://github.com/SUFE-AIFLM-Lab/FinGAIA.

</details>


### [13] [The Pluralistic Moral Gap: Understanding Judgment and Value Differences between Humans and Large Language Models](https://arxiv.org/abs/2507.17216)
*Giuseppe Russo,Debora Nozza,Paul Röttger,Dirk Hovy*

Main category: cs.CL

TL;DR: The paper evaluates how well Large Language Models (LLMs) align with human moral judgments using a new dataset, finding significant gaps in agreement and diversity, and proposes a method to improve alignment.


<details>
  <summary>Details</summary>
Motivation: To understand the moral alignment between LLMs and humans, addressing the risk of influencing human decisions.

Method: Introducing the Moral Dilemma Dataset, analyzing LLM and human judgment distributions, and developing Dynamic Moral Profiling for better alignment.

Result: LLMs largely match human judgments under consensus, but falter with disagreement; they rely on fewer moral values; DMP significantly improves alignment and value diversity.

Conclusion: Addressing the pluralistic moral gap with DMP can lead to more human-aligned and diverse moral guidance from LLMs.

Abstract: People increasingly rely on Large Language Models (LLMs) for moral advice,
which may influence humans' decisions. Yet, little is known about how closely
LLMs align with human moral judgments. To address this, we introduce the Moral
Dilemma Dataset, a benchmark of 1,618 real-world moral dilemmas paired with a
distribution of human moral judgments consisting of a binary evaluation and a
free-text rationale. We treat this problem as a pluralistic distributional
alignment task, comparing the distributions of LLM and human judgments across
dilemmas. We find that models reproduce human judgments only under high
consensus; alignment deteriorates sharply when human disagreement increases. In
parallel, using a 60-value taxonomy built from 3,783 value expressions
extracted from rationales, we show that LLMs rely on a narrower set of moral
values than humans. These findings reveal a pluralistic moral gap: a mismatch
in both the distribution and diversity of values expressed. To close this gap,
we introduce Dynamic Moral Profiling (DMP), a Dirichlet-based sampling method
that conditions model outputs on human-derived value profiles. DMP improves
alignment by 64.3% and enhances value diversity, offering a step toward more
pluralistic and human-aligned moral guidance from LLMs.

</details>


### [14] [CLARIFID: Improving Radiology Report Generation by Reinforcing Clinically Accurate Impressions and Enforcing Detailed Findings](https://arxiv.org/abs/2507.17234)
*Kyeongkyu Lee,Seonghwan Yoon,Hongki Lim*

Main category: cs.CL

TL;DR: CLARIFID improves radiology report generation by focusing on factual correctness and multi-view imaging, mirroring clinical workflows.


<details>
  <summary>Details</summary>
Motivation: Address the inadequacy of current methods in producing clinically reliable radiology reports, especially with multi-view images.

Method: Uses section-aware pretraining, reinforcement learning with clinical scores as rewards, reasoning-aware decoding, multi-view fusion, and a post-generation re-ranking strategy.

Result: Outperforms existing baselines on standard and clinical metrics on MIMIC-CXR dataset.

Conclusion: CLARIFID advances automated radiology reporting by aligning model processes with clinical reasoning and improving factual accuracy.

Abstract: Automatic generation of radiology reports has the potential to alleviate
radiologists' significant workload, yet current methods struggle to deliver
clinically reliable conclusions. In particular, most prior approaches focus on
producing fluent text without effectively ensuring the factual correctness of
the reports and often rely on single-view images, limiting diagnostic
comprehensiveness. We propose CLARIFID, a novel framework that directly
optimizes diagnostic correctness by mirroring the two-step workflow of experts.
Specifically, CLARIFID (1) learns the logical flow from Findings to Impression
through section-aware pretraining, (2) is fine-tuned with Proximal Policy
Optimization in which the CheXbert F1 score of the Impression section serves as
the reward, (3) enforces reasoning-aware decoding that completes "Findings"
before synthesizing the "Impression", and (4) fuses multiple chest X-ray views
via a vision-transformer-based multi-view encoder. During inference, we apply a
reasoning-aware next-token forcing strategy followed by report-level
re-ranking, ensuring that the model first produces a comprehensive Findings
section before synthesizing the Impression and thereby preserving coherent
clinical reasoning. Experimental results on the MIMIC-CXR dataset demonstrate
that our method achieves superior clinical efficacy and outperforms existing
baselines on both standard NLG metrics and clinically aware scores.

</details>


### [15] [Triple X: A LLM-Based Multilingual Speech Recognition System for the INTERSPEECH2025 MLC-SLM Challenge](https://arxiv.org/abs/2507.17288)
*Miaomiao Gao,Xiaoxiao Xiang,Yiwen Guo*

Main category: cs.CL

TL;DR: A multilingual speech recognition system using encoder-adapter-LLM architecture with multi-stage training achieved second place in MLC-SLM Challenge.


<details>
  <summary>Details</summary>
Motivation: Improve recognition accuracy in multilingual conversational speech scenarios.

Method: Innovative encoder-adapter-LLM architecture combined with multi-stage training on extensive multilingual datasets.

Result: Achieved competitive WER performance, ranked second in the challenge.

Conclusion: The proposed framework effectively enhances multilingual speech recognition, demonstrating the potential of integrating large language models with domain-specific adaptations.

Abstract: This paper describes our Triple X speech recognition system submitted to Task
1 of the Multi-Lingual Conversational Speech Language Modeling (MLC-SLM)
Challenge. Our work focuses on optimizing speech recognition accuracy in
multilingual conversational scenarios through an innovative encoder-adapter-LLM
architecture. This framework harnesses the powerful reasoning capabilities of
text-based large language models while incorporating domain-specific
adaptations. To further enhance multilingual recognition performance, we
adopted a meticulously designed multi-stage training strategy leveraging
extensive multilingual audio datasets. Experimental results demonstrate that
our approach achieves competitive Word Error Rate (WER) performance on both dev
and test sets, obtaining second place in the challenge ranking.

</details>


### [16] [Millions of $\text{GeAR}$-s: Extending GraphRAG to Millions of Documents](https://arxiv.org/abs/2507.17399)
*Zhili Shen,Chenxin Diao,Pascual Merita,Pavlos Vougiouklis,Jeff Z. Pan*

Main category: cs.CL

TL;DR: The paper explores adapting a graph-based Retrieval-Augmented Generation method, GeAR, for broader application beyond task-specific tasks, testing its performance on the SIGIR 2025 LiveRAG Challenge.


<details>
  <summary>Details</summary>
Motivation: To evaluate the generalizability of graph-based RAG methods like GeAR across diverse datasets, beyond their typical task-specific applications.

Method: Adapting and testing the GeAR model on the SIGIR 2025 LiveRAG Challenge dataset to assess its performance and limitations.

Result: The details are not provided in the abstract, but the study aims to evaluate GeAR's effectiveness and limitations in a broader context.

Conclusion: The paper aims to identify the potential and limitations of general-purpose graph-based RAG approaches like GeAR.

Abstract: Recent studies have explored graph-based approaches to retrieval-augmented
generation, leveraging structured or semi-structured information -- such as
entities and their relations extracted from documents -- to enhance retrieval.
However, these methods are typically designed to address specific tasks, such
as multi-hop question answering and query-focused summarisation, and therefore,
there is limited evidence of their general applicability across broader
datasets. In this paper, we aim to adapt a state-of-the-art graph-based RAG
solution: $\text{GeAR}$ and explore its performance and limitations on the
SIGIR 2025 LiveRAG Challenge.

</details>


### [17] [Investigating Subjective Factors of Argument Strength: Storytelling, Emotions, and Hedging](https://arxiv.org/abs/2507.17409)
*Carlotta Quensel,Neele Falk,Gabriella Lapesa*

Main category: cs.CL

TL;DR: This study examines how subjective features like emotions, storytelling, and hedging influence argument strength, revealing differing effects on objective and subjective argument quality through regression analysis.


<details>
  <summary>Details</summary>
Motivation: To fill the gap in large-scale analysis of subjective features' impact on argument strength in NLP.

Method: Regression analysis on datasets annotated for objective argument quality and subjective persuasion, along with evaluation of automated annotation methods for subjective features.

Result: Storytelling and hedging impact objective and subjective argument quality differently; emotion influence varies based on rhetorical usage and domain.

Conclusion: Subjective features have complex, contrasting effects on argument strength, emphasizing the importance of nuanced analysis in NLP.

Abstract: In assessing argument strength, the notions of what makes a good argument are
manifold. With the broader trend towards treating subjectivity as an asset and
not a problem in NLP, new dimensions of argument quality are studied. Although
studies on individual subjective features like personal stories exist, there is
a lack of large-scale analyses of the relation between these features and
argument strength. To address this gap, we conduct regression analysis to
quantify the impact of subjective factors $-$ emotions, storytelling, and
hedging $-$ on two standard datasets annotated for objective argument quality
and subjective persuasion. As such, our contribution is twofold: at the level
of contributed resources, as there are no datasets annotated with all studied
dimensions, this work compares and evaluates automated annotation methods for
each subjective feature. At the level of novel insights, our regression
analysis uncovers different patterns of impact of subjective features on the
two facets of argument strength encoded in the datasets. Our results show that
storytelling and hedging have contrasting effects on objective and subjective
argument quality, while the influence of emotions depends on their rhetoric
utilization rather than the domain.

</details>


### [18] [Each to Their Own: Exploring the Optimal Embedding in RAG](https://arxiv.org/abs/2507.17442)
*Shiting Chen,Zijian Zhao,Jinsong Chen*

Main category: cs.CL

TL;DR: Enhancing Retrieval-Augmented Generation (RAG) by combining multiple embedding models improves response quality, especially with the Confident RAG approach.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of heterogeneous embedding models in RAG and improve response quality.

Method: Proposing and evaluating two methods: Mixture-Embedding RAG and Confident RAG, with the latter generating multiple responses and selecting the most confident.

Result: Confident RAG achieves approximately 10% and 5% improvements over vanilla LLMs and RAG, respectively, showing consistent benefits across domains.

Conclusion: Confident RAG is an effective, plug-and-play enhancement for RAG models to improve domain-specific response quality.

Abstract: Recently, as Large Language Models (LLMs) have fundamentally impacted various
fields, the methods for incorporating up-to-date information into LLMs or
adding external knowledge to construct domain-specific models have garnered
wide attention. Retrieval-Augmented Generation (RAG), serving as an
inference-time scaling method, is notable for its low cost and minimal effort
for parameter tuning. However, due to heterogeneous training data and model
architecture, the variant embedding models used in RAG exhibit different
benefits across various areas, often leading to different similarity
calculation results and, consequently, varying response quality from LLMs. To
address this problem, we propose and examine two approaches to enhance RAG by
combining the benefits of multiple embedding models, named Mixture-Embedding
RAG and Confident RAG. Mixture-Embedding RAG simply sorts and selects
retrievals from multiple embedding models based on standardized similarity;
however, it does not outperform vanilla RAG. In contrast, Confident RAG
generates responses multiple times using different embedding models and then
selects the responses with the highest confidence level, demonstrating average
improvements of approximately 10% and 5% over vanilla LLMs and RAG,
respectively. The consistent results across different LLMs and embedding models
indicate that Confident RAG is an efficient plug-and-play approach for various
domains. We will release our code upon publication.

</details>


### [19] [MultiNRC: A Challenging and Native Multilingual Reasoning Evaluation Benchmark for LLMs](https://arxiv.org/abs/2507.17476)
*Alexander R. Fabbri,Diego Mares,Jorge Flores,Meher Mankikar,Ernesto Hernandez,Dean Lee,Bing Liu,Chen Xing*

Main category: cs.CL

TL;DR: This paper introduces MultiNRC, a benchmark for evaluating multilingual reasoning in LLMs across various languages and cultural contexts, revealing current models' limitations in native language reasoning.


<details>
  <summary>Details</summary>
Motivation: To address the gap in evaluating multilingual reasoning capabilities of LLMs in native languages and culturally grounded contexts.

Method: Developed MultiNRC with over 1,000 native, culturally grounded questions in French, Spanish, and Chinese, covering linguistic, cultural, and mathematical reasoning, with English translations for comparison; evaluated 14 leading LLMs.

Result: Current LLMs perform poorly on native multilingual reasoning, especially in culturally relevant tasks, with significant performance drops compared to English; models display distinct strengths and weaknesses.

Conclusion: LLMs still face challenges in native language and culturally grounded reasoning, highlighting the need for improved multilingual training and evaluation.

Abstract: Although recent Large Language Models (LLMs) have shown rapid improvement on
reasoning benchmarks in English, the evaluation of such LLMs' multilingual
reasoning capability across diverse languages and cultural contexts remains
limited. Existing multilingual reasoning benchmarks are typically constructed
by translating existing English reasoning benchmarks, biasing these benchmarks
towards reasoning problems with context in English language/cultures. In this
work, we introduce the Multilingual Native Reasoning Challenge (MultiNRC), a
benchmark designed to assess LLMs on more than 1,000 native, linguistic and
culturally grounded reasoning questions written by native speakers in French,
Spanish, and Chinese. MultiNRC covers four core reasoning categories:
language-specific linguistic reasoning, wordplay & riddles, cultural/tradition
reasoning, and math reasoning with cultural relevance. For cultural/tradition
reasoning and math reasoning with cultural relevance, we also provide English
equivalent translations of the multilingual questions by manual translation
from native speakers fluent in English. This set of English equivalents can
provide a direct comparison of LLM reasoning capacity in other languages vs.
English on the same reasoning questions. We systematically evaluate current 14
leading LLMs covering most LLM families on MultiNRC and its English equivalent
set. The results show that (1) current LLMs are still not good at native
multilingual reasoning, with none scoring above 50% on MultiNRC; (2) LLMs
exhibit distinct strengths and weaknesses in handling linguistic, cultural, and
logical reasoning tasks; (3) Most models perform substantially better in math
reasoning in English compared to in original languages (+10%), indicating
persistent challenges with culturally grounded knowledge.

</details>


### [20] [Seed LiveInterpret 2.0: End-to-end Simultaneous Speech-to-speech Translation with Your Voice](https://arxiv.org/abs/2507.17527)
*Shanbo Cheng,Yu Bao,Zhichao Huang,Yu Lu,Ningxin Peng,Lu Xu,Runsheng Yu,Rong Cao,Ting Han,Zeyang Li,Sitong Liu,Shengtao Ma,Shiguang Pan,Jiongchen Xiao,Nuo Xu,Meng Yang,Rong Ye,Yiming Yu,Ruofei Zhang,Wanyi Zhang,Wenhao Zhu,Liehao Zou,Lu Lu,Yuxuan Wang,Yonghui Wu*

Main category: cs.CL

TL;DR: Seed-LiveInterpret 2.0 is an advanced end-to-end simultaneous interpretation model that achieves high fidelity, low latency, and voice cloning, significantly outperforming existing solutions.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of existing automatic simultaneous interpretation systems, including poor quality, high latency, and lack of voice cloning capabilities.

Method: Developing a novel duplex speech understanding-generating framework trained with large-scale pretraining and reinforcement learning, enabling efficient speech-to-speech translation with voice cloning.

Result: The model surpasses commercial solutions in translation quality, reduces latency from 10 seconds to 3 seconds, and achieves over 70% correctness in complex scenarios, demonstrating improved practicality.

Conclusion: Seed-LiveInterpret 2.0 effectively overcomes key challenges in simultaneous interpretation, setting a new standard for real-time, high-quality speech translation.

Abstract: Simultaneous Interpretation (SI) represents one of the most daunting
frontiers in the translation industry, with product-level automatic systems
long plagued by intractable challenges: subpar transcription and translation
quality, lack of real-time speech generation, multi-speaker confusion, and
translated speech inflation, especially in long-form discourses. In this study,
we introduce Seed-LiveInterpret 2.0, an end-to-end SI model that delivers
high-fidelity, ultra-low-latency speech-to-speech generation with voice cloning
capabilities. As a fully operational product-level solution, Seed-LiveInterpret
2.0 tackles these challenges head-on through our novel duplex speech-to-speech
understanding-generating framework. Experimental results demonstrate that
through large-scale pretraining and reinforcement learning, the model achieves
a significantly better balance between translation accuracy and latency,
validated by human interpreters to exceed 70% correctness in complex scenarios.
Notably, Seed-LiveInterpret 2.0 outperforms commercial SI solutions by
significant margins in translation quality, while slashing the average latency
of cloned speech from nearly 10 seconds to a near-real-time 3 seconds, which is
around a near 70% reduction that drastically enhances practical usability.

</details>


### [21] [Synthetic Voice Data for Automatic Speech Recognition in African Languages](https://arxiv.org/abs/2507.17578)
*Brian DeRenzi,Anna Dixon,Mohamed Aymane Farhi,Christian Resch*

Main category: cs.CL

TL;DR: The paper systematically assesses large-scale synthetic voice corpora for African ASR, demonstrating cost-effective methods to improve speech recognition across multiple low-resource languages.


<details>
  <summary>Details</summary>
Motivation: Address the scarcity of speech technology for many African languages and evaluate the potential of synthetic data for ASR development.

Method: Use LLM-driven text creation, TTS voice synthesis, and ASR fine-tuning on synthetic datasets to enhance ASR models.

Result: Synthetic data significantly improved ASR performance, with models trained on synthetic and real data matching or surpassing baselines, though challenges remain in evaluation robustness.

Conclusion: Synthetic data is a promising cost-effective approach to advancing African language ASR, with open data and models supporting further research.

Abstract: Speech technology remains out of reach for most of the over 2300 languages in
Africa. We present the first systematic assessment of large-scale synthetic
voice corpora for African ASR. We apply a three-step process: LLM-driven text
creation, TTS voice synthesis, and ASR fine-tuning. Eight out of ten languages
for which we create synthetic text achieved readability scores above 5 out of
7. We evaluated ASR improvement for three (Hausa, Dholuo, Chichewa) and created
more than 2,500 hours of synthetic voice data at below 1% of the cost of real
data. Fine-tuned Wav2Vec-BERT-2.0 models trained on 250h real and 250h
synthetic Hausa matched a 500h real-data-only baseline, while 579h real and
450h to 993h synthetic data created the best performance. We also present
gender-disaggregated ASR performance evaluation. For very low-resource
languages, gains varied: Chichewa WER improved about 6.5% relative with a 1:2
real-to-synthetic ratio; a 1:1 ratio for Dholuo showed similar improvements on
some evaluation data, but not on others. Investigating intercoder reliability,
ASR errors and evaluation datasets revealed the need for more robust reviewer
protocols and more accurate evaluation data. All data and models are publicly
released to invite further work to improve synthetic data for African
languages.

</details>


### [22] [A Hybrid Early-Exit Algorithm for Large Language Models Based on Space Alignment Decoding (SPADE)](https://arxiv.org/abs/2507.17618)
*Bowen Zheng,Ming Ma,Zhongqiao Lin,Tianming Yang*

Main category: cs.CL

TL;DR: SPADE aligns intermediate and output layer representations to improve early-exit decoding in large language models, reducing inference costs without loss of accuracy.


<details>
  <summary>Details</summary>
Motivation: Address the high computational cost of large language models and improve early-exit algorithms by ensuring better representation alignment.

Method: Propose SPADE to align representations by propagating minimal sequences and train a linear approximation for confidence estimation, integrating these into a hybrid early-exit strategy.

Result: The hybrid approach achieves significant inference cost reduction while maintaining high output quality.

Conclusion: SPADE enhances early-exit decoding efficiency, providing a scalable solution for deploying large language models effectively.

Abstract: Large language models are computationally expensive due to their deep
structures. Prior research has shown that intermediate layers contain
sufficient information to generate accurate answers, leading to the development
of early-exit algorithms that reduce inference costs by terminating computation
at earlier layers. However, these methods often suffer from poor performance
due to misalignment between intermediate and output layer representations that
lead to decoding inaccuracy. To address these challenges, we propose SPADE
(SPace Alignment DEcoding), a novel decoding method that aligns intermediate
layer representations with the output layer by propagating a minimally reduced
sequence consisting of only the start token and the answer token. We further
optimize the early-exit decision-making process by training a linear
approximation of SPADE that computes entropy-based confidence metrics. Putting
them together, we create a hybrid early-exit algorithm that monitors confidence
levels and stops inference at intermediate layers while using SPADE to generate
high-quality outputs. This approach significantly reduces inference costs
without compromising accuracy, offering a scalable and efficient solution for
deploying large language models in real-world applications.

</details>


### [23] [Bridging Robustness and Generalization Against Word Substitution Attacks in NLP via the Growth Bound Matrix Approach](https://arxiv.org/abs/2507.10330)
*Mohammed Bouri,Adnane Saoud*

Main category: cs.CL

TL;DR: A novel regularization technique using Growth Bound Matrices (GBM) enhances NLP model robustness, particularly against adversarial word substitutions, across various architectures including LSTM, S4, and CNN.


<details>
  <summary>Details</summary>
Motivation: Address the vulnerability of NLP models, especially recurrent and state space models, to adversarial attacks.

Method: Introduce GBM-based regularization and compute GBM for LSTM, S4, and CNN architectures.

Result: Improved robustness, with up to 8.8% enhancement over baselines, and a systematic analysis of S4 robustness.

Conclusion: The proposed GBM regularization effectively improves adversarial robustness across multiple NLP architectures, outperforming several state-of-the-art methods.

Abstract: Despite advancements in Natural Language Processing (NLP), models remain
vulnerable to adversarial attacks, such as synonym substitutions. While prior
work has focused on improving robustness for feed-forward and convolutional
architectures, the robustness of recurrent networks and modern state space
models (SSMs), such as S4, remains understudied. These architectures pose
unique challenges due to their sequential processing and complex parameter
dynamics. In this paper, we introduce a novel regularization technique based on
Growth Bound Matrices (GBM) to improve NLP model robustness by reducing the
impact of input perturbations on model outputs. We focus on computing the GBM
for three architectures: Long Short-Term Memory (LSTM), State Space models
(S4), and Convolutional Neural Networks (CNN). Our method aims to (1) enhance
resilience against word substitution attacks, (2) improve generalization on
clean text, and (3) providing the first systematic analysis of SSM (S4)
robustness. Extensive experiments across multiple architectures and benchmark
datasets demonstrate that our method improves adversarial robustness by up to
8.8% over existing baselines. These results highlight the effectiveness of our
approach, outperforming several state-of-the-art methods in adversarial
defense. Codes are available at https://github.com/BouriMohammed/GBM

</details>


### [24] [WSM: Decay-Free Learning Rate Schedule via Checkpoint Merging for LLM Pre-training](https://arxiv.org/abs/2507.17634)
*Changxin Tian,Jiapeng Wang,Qian Zhao,Kunlong Chen,Jia Liu,Ziqi Liu,Jiaxin Mao,Wayne Xin Zhao,Zhiqiang Zhang,Jun Zhou*

Main category: cs.CL

TL;DR: WSM framework unifies learning rate decay with model merging, outperforming traditional decay methods across several benchmarks.


<details>
  <summary>Details</summary>
Motivation: To develop a decay-free learning rate scheduling approach that maintains competitive performance while simplifying the training process.

Method: Introducing WSM, a theoretical framework connecting learning rate decay and model merging, supported by extensive experiments on various benchmarks.

Result: WSM outperforms existing decay strategies and WSD, significantly improving model performance, especially with optimal merge durations.

Conclusion: WSM provides a unified, effective approach to learning rate scheduling via model merging, showing promise for long-term model enhancement.

Abstract: Recent advances in learning rate (LR) scheduling have demonstrated the
effectiveness of decay-free approaches that eliminate the traditional decay
phase while maintaining competitive performance. Model merging techniques have
emerged as particularly promising solutions in this domain. We present
Warmup-Stable and Merge (WSM), a general framework that establishes a formal
connection between learning rate decay and model merging. WSM provides a
unified theoretical foundation for emulating various decay strategies-including
cosine decay, linear decay and inverse square root decay-as principled model
averaging schemes, while remaining fully compatible with diverse optimization
methods. Through extensive experiments, we identify merge duration-the training
window for checkpoint aggregation-as the most critical factor influencing model
performance, surpassing the importance of both checkpoint interval and merge
quantity. Our framework consistently outperforms the widely-adopted
Warmup-Stable-Decay (WSD) approach across multiple benchmarks, achieving
significant improvements of +3.5% on MATH, +2.9% on HumanEval, and +5.5% on
MMLU-Pro. The performance advantages extend to supervised fine-tuning
scenarios, highlighting WSM's potential for long-term model refinement.

</details>


### [25] [Who Attacks, and Why? Using LLMs to Identify Negative Campaigning in 18M Tweets across 19 Countries](https://arxiv.org/abs/2507.17636)
*Victor Hartman,Petter Törnberg*

Main category: cs.CL

TL;DR: This study uses zero-shot Large Language Models to classify negative campaigning across multiple languages, achieving performance comparable to human coders, and conducts the largest cross-national analysis of negative messaging, revealing patterns related to party ideology.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of existing classification methods for negative campaigning and to enable large-scale, cross-lingual research.

Method: Applying zero-shot LLMs to classify negative campaigning in multilingual datasets and analyzing 18 million tweets across European countries.

Result: LLMs perform on par with native human coders and outperform traditional models; the analysis uncovers patterns in negative messaging related to party ideology and extremism across countries.

Conclusion: LLMs facilitate scalable, transparent, and culturally-aware research in political communication, enhancing understanding of strategic messaging in multi-party systems.

Abstract: Negative campaigning is a central feature of political competition, yet
empirical research has been limited by the high cost and limited scalability of
existing classification methods. This study makes two key contributions. First,
it introduces zero-shot Large Language Models (LLMs) as a novel approach for
cross-lingual classification of negative campaigning. Using benchmark datasets
in ten languages, we demonstrate that LLMs achieve performance on par with
native-speaking human coders and outperform conventional supervised machine
learning approaches. Second, we leverage this novel method to conduct the
largest cross-national study of negative campaigning to date, analyzing 18
million tweets posted by parliamentarians in 19 European countries between 2017
and 2022. The results reveal consistent cross-national patterns: governing
parties are less likely to use negative messaging, while ideologically extreme
and populist parties -- particularly those on the radical right -- engage in
significantly higher levels of negativity. These findings advance our
understanding of how party-level characteristics shape strategic communication
in multiparty systems. More broadly, the study demonstrates the potential of
LLMs to enable scalable, transparent, and replicable research in political
communication across linguistic and cultural contexts.

</details>


### [26] [Towards Greater Leverage: Scaling Laws for Efficient Mixture-of-Experts Language Models](https://arxiv.org/abs/2507.17702)
*Changxin Tian,Kunlong Chen,Jia Liu,Ziqi Liu,Zhiqiang Zhang,Jun Zhou*

Main category: cs.CL

TL;DR: The paper introduces Efficiency Leverage (EL), a metric to predict the computational advantage of Mixture-of-Experts (MoE) models. Through extensive experiments and a scaling law, it demonstrates how MoE configurations impact EL, enabling efficient model design. A pilot model validated these findings, matching large dense models with fewer resources.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of predicting model capacity in MoE configurations, which is critical for efficiently scaling large language models.

Method: Large-scale empirical study of over 300 models, deriving a scaling law relating MoE configurations to EL, and validation through a pilot model experiment.

Result: EL is primarily influenced by expert activation ratio and compute budget, following predictable power laws. The scaling law accurately predicts EL, and a pilot model validates these predictions by achieving comparable performance with fewer resources.

Conclusion: The work provides a principled framework and empirical evidence for scaling efficient MoE models, enhancing their practical deployment.

Abstract: Mixture-of-Experts (MoE) has become a dominant architecture for scaling Large
Language Models (LLMs) efficiently by decoupling total parameters from
computational cost. However, this decoupling creates a critical challenge:
predicting the model capacity of a given MoE configurations (e.g., expert
activation ratio and granularity) remains an unresolved problem. To address
this gap, we introduce Efficiency Leverage (EL), a metric quantifying the
computational advantage of an MoE model over a dense equivalent. We conduct a
large-scale empirical study, training over 300 models up to 28B parameters, to
systematically investigate the relationship between MoE architectural
configurations and EL. Our findings reveal that EL is primarily driven by the
expert activation ratio and the total compute budget, both following
predictable power laws, while expert granularity acts as a non-linear modulator
with a clear optimal range. We integrate these discoveries into a unified
scaling law that accurately predicts the EL of an MoE architecture based on its
configuration. To validate our derived scaling laws, we designed and trained
Ling-mini-beta, a pilot model for Ling-2.0 series with only 0.85B active
parameters, alongside a 6.1B dense model for comparison. When trained on an
identical 1T high-quality token dataset, Ling-mini-beta matched the performance
of the 6.1B dense model while consuming over 7x fewer computational resources,
thereby confirming the accuracy of our scaling laws. This work provides a
principled and empirically-grounded foundation for the scaling of efficient MoE
models.

</details>


### [27] [TyDi QA-WANA: A Benchmark for Information-Seeking Question Answering in Languages of West Asia and North Africa](https://arxiv.org/abs/2507.17709)
*Parker Riley,Siamak Shakeri,Waleed Ammar,Jonathan H. Clark*

Main category: cs.CL

TL;DR: TyDi QA-WANA is a dataset of 28K questions in 10 languages, aimed at testing language models' ability to extract answers from large texts.


<details>
  <summary>Details</summary>
Motivation: To create a culturally relevant, language-specific question-answering dataset for Western Asian and North African languages.

Method: Collecting genuine, information-seeking questions paired with articles in each language, without translation.

Result: Baseline model performances are provided, with code and data released for further research.

Conclusion: The dataset enables evaluation of language models on large-context QA across diverse languages, promoting improvements in multilingual NLP.

Abstract: We present TyDi QA-WANA, a question-answering dataset consisting of 28K
examples divided among 10 language varieties of western Asia and northern
Africa. The data collection process was designed to elicit information-seeking
questions, where the asker is genuinely curious to know the answer. Each
question in paired with an entire article that may or may not contain the
answer; the relatively large size of the articles results in a task suitable
for evaluating models' abilities to utilize large text contexts in answering
questions. Furthermore, the data was collected directly in each language
variety, without the use of translation, in order to avoid issues of cultural
relevance. We present performance of two baseline models, and release our code
and data to facilitate further improvement by the research community.

</details>


### [28] [From Feedback to Checklists: Grounded Evaluation of AI-Generated Clinical Notes](https://arxiv.org/abs/2507.17717)
*Karen Zhou,John Giorgi,Pranav Mani,Peng Xu,Davis Liang,Chenhao Tan*

Main category: cs.CL

TL;DR: The paper introduces a checklist-based evaluation pipeline for AI-generated clinical notes, derived from real clinician feedback, which outperforms traditional metrics in quality assessment and aligns well with clinician preferences.


<details>
  <summary>Details</summary>
Motivation: To improve the evaluation of AI-generated clinical notes, addressing the limitations of subjectivity and scalability of expert reviews.

Method: Developing a structured checklist grounded in real user feedback and evaluating it using deidentified clinical encounter data from an AI scribe system.

Result: The checklist outperforms baseline metrics in coverage, diversity, and predictability of human ratings; it is robust, interpretable, and aligns with clinician preferences.

Conclusion: The proposed checklist is an effective, practical tool for assessing clinical note quality and can aid in identifying low-quality notes in offline research.

Abstract: AI-generated clinical notes are increasingly used in healthcare, but
evaluating their quality remains a challenge due to high subjectivity and
limited scalability of expert review. Existing automated metrics often fail to
align with real-world physician preferences. To address this, we propose a
pipeline that systematically distills real user feedback into structured
checklists for note evaluation. These checklists are designed to be
interpretable, grounded in human feedback, and enforceable by LLM-based
evaluators. Using deidentified data from over 21,000 clinical encounters,
prepared in accordance with the HIPAA safe harbor standard, from a deployed AI
medical scribe system, we show that our feedback-derived checklist outperforms
baseline approaches in our offline evaluations in coverage, diversity, and
predictive power for human ratings. Extensive experiments confirm the
checklist's robustness to quality-degrading perturbations, significant
alignment with clinician preferences, and practical value as an evaluation
methodology. In offline research settings, the checklist can help identify
notes likely to fall below our chosen quality thresholds.

</details>


### [29] [AI Telephone Surveying: Automating Quantitative Data Collection with an AI Interviewer](https://arxiv.org/abs/2507.17718)
*Danny D. Leybzon,Shreyas Tirumala,Nishant Jain,Summer Gillen,Michael Jackson,Cameron McPhee,Jennifer Schmidt*

Main category: cs.CL

TL;DR: This paper explores the development and validation of an AI-powered telephone survey system using large language models, which enhances natural interaction and methodological rigor. Pilot studies show promising results in survey completion, break-off rates, and respondent satisfaction.


<details>
  <summary>Details</summary>
Motivation: To leverage AI to improve the scalability, naturalness, and effectiveness of telephone surveys while maintaining research standards.

Method: Development of an AI system utilizing LLM, ASR, and speech synthesis, followed by pilot testing with SSRS panels and a human survey for comparison.

Result: The system showed improved survey metrics with shorter surveys and more responsive AI interaction, indicating potential for broader application.

Conclusion: AI-based telephone surveying can enhance data collection by providing natural, interactive experiences that meet research quality standards.

Abstract: With the rise of voice-enabled artificial intelligence (AI) systems,
quantitative survey researchers have access to a new data-collection mode: AI
telephone surveying. By using AI to conduct phone interviews, researchers can
scale quantitative studies while balancing the dual goals of human-like
interactivity and methodological rigor. Unlike earlier efforts that used
interactive voice response (IVR) technology to automate these surveys, voice AI
enables a more natural and adaptive respondent experience as it is more robust
to interruptions, corrections, and other idiosyncrasies of human speech.
  We built and tested an AI system to conduct quantitative surveys based on
large language models (LLM), automatic speech recognition (ASR), and speech
synthesis technologies. The system was specifically designed for quantitative
research, and strictly adhered to research best practices like question order
randomization, answer order randomization, and exact wording.
  To validate the system's effectiveness, we deployed it to conduct two pilot
surveys with the SSRS Opinion Panel and followed-up with a separate
human-administered survey to assess respondent experiences. We measured three
key metrics: the survey completion rates, break-off rates, and respondent
satisfaction scores. Our results suggest that shorter instruments and more
responsive AI interviewers may contribute to improvements across all three
metrics studied.

</details>


### [30] [Megrez2 Technical Report](https://arxiv.org/abs/2507.17728)
*Boxun Li,Yadong Li,Zhiyuan Li,Congyi Liu,Weilin Liu,Guowei Niu,Zheyue Tan,Haiyang Xu,Zhuyu Yao,Tao Yuan,Dong Zhou,Yueqing Zhuang,Bo Zhao,Guohao Dai,Yu Wang*

Main category: cs.CL

TL;DR: Megrez2 is a new lightweight, high-performance language model architecture optimized for device deployment, featuring parameter sharing and memory-efficient routing.


<details>
  <summary>Details</summary>
Motivation: To develop a compact, efficient language model suitable for resource-constrained environments without sacrificing performance.

Method: Introducing Megrez2 architecture with cross-layer expert sharing and pre-gated routing, pre-training on vast data, supervised fine-tuning, and reinforcement learning.

Result: Megrez2-Preview, with 3-7.5B parameters, exhibits competitive or superior performance across various tasks, balancing accuracy and efficiency.

Conclusion: Megrez2 offers a promising approach for deploying powerful language models on resource-limited devices, demonstrating effectiveness in multiple NLP tasks.

Abstract: We present Megrez2, a novel lightweight and high-performance language model
architecture optimized for device native deployment. Megrez2 introduces a novel
cross-layer expert sharing mechanism, which significantly reduces total
parameter count by reusing expert modules across adjacent transformer layers
while maintaining most of the model's capacity. It also incorporates pre-gated
routing, enabling memory-efficient expert loading and faster inference. As the
first instantiation of the Megrez2 architecture, we introduce the
Megrez2-Preview model, which is pre-trained on a 5-trillion-token corpus and
further enhanced through supervised fine-tuning and reinforcement learning with
verifiable rewards. With only 3B activated and 7.5B stored parameters,
Megrez2-Preview demonstrates competitive or superior performance compared to
larger models on a wide range of tasks, including language understanding,
instruction following, mathematical reasoning, and code generation. These
results highlight the effectiveness of the Megrez2 architecture to achieve a
balance between accuracy, efficiency, and deployability, making it a strong
candidate for real-world, resource-constrained applications.

</details>


### [31] [Pretraining on the Test Set Is No Longer All You Need: A Debate-Driven Approach to QA Benchmarks](https://arxiv.org/abs/2507.17747)
*Linbo Cao,Jinman Zhao*

Main category: cs.CL

TL;DR: Debate-driven evaluation transforms QA datasets into adversarial debates, improving robustness and reducing bias, with promising results on MMLU-Pro questions.


<details>
  <summary>Details</summary>
Motivation: Address the limitations of current QA benchmarks, such as data contamination and shallow memorization, by providing a more rigorous evaluation method.

Method: Convert QA tasks into structured debates with models defending or opposing answers, judged by a blind model, and validate on MMLU-Pro questions.

Result: The method increases difficulty for models, reduces bias, and improves evaluation robustness, with strong empirical validation and scalability benefits.

Conclusion: Debate-based assessment offers a sustainable, effective way to measure real reasoning abilities of language models, beyond traditional benchmarks.

Abstract: As frontier language models increasingly saturate standard QA benchmarks,
concerns about data contamination, memorization, and escalating dataset
creation costs persist. We propose a debate-driven evaluation paradigm that
transforms any existing QA dataset into structured adversarial debates--where
one model is given the official answer to defend, and another constructs and
defends an alternative answer--adjudicated by a judge model blind to the
correct solution. By forcing multi-round argumentation, this approach
substantially increases difficulty while penalizing shallow memorization, yet
reuses QA items to reduce curation overhead. We make two main contributions:
(1) an evaluation pipeline to systematically convert QA tasks into debate-based
assessments, and (2) a public benchmark that demonstrates our paradigm's
effectiveness on a subset of MMLU-Pro questions, complete with standardized
protocols and reference models. Empirical results validate the robustness of
the method and its effectiveness against data contamination--a Llama 3.1 model
fine-tuned on test questions showed dramatic accuracy improvements (50% -> 82%)
but performed worse in debates. Results also show that even weaker judges can
reliably differentiate stronger debaters, highlighting how debate-based
evaluation can scale to future, more capable systems while maintaining a
fraction of the cost of creating new benchmarks. Overall, our framework
underscores that "pretraining on the test set is no longer all you need,"
offering a sustainable path for measuring the genuine reasoning ability of
advanced language models.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [32] [Triadic First-Order Logic Queries in Temporal Networks](https://arxiv.org/abs/2507.17215)
*Omkar Bhalerao,Yunjie Pan,C. Seshadhri,Nishil Talati*

Main category: cs.DB

TL;DR: Introduction of FOLTY, a novel algorithm for thresholded First Order Logic motif analysis in temporal networks, capable of efficiently handling complex triadic queries.


<details>
  <summary>Details</summary>
Motivation: Enhance motif counting by incorporating richer query semantics via logic, especially in temporal networks.

Method: Design and implement FOLTY, an algorithm that extends temporal triangle counting with thresholded FOL queries, utilizing specialized data structures.

Result: FOLTY achieves optimal theoretical runtime, practical efficiency on large graphs, and enables complex motif queries.

Conclusion: FOLTY introduces a new promising research direction in motif analysis, blending logic with temporal network mining.

Abstract: Motif counting is a fundamental problem in network analysis, and there is a
rich literature of theoretical and applied algorithms for this problem. Given a
large input network $G$, a motif $H$ is a small "pattern" graph indicative of
special local structure. Motif/pattern mining involves finding all matches of
this pattern in the input $G$. The simplest, yet challenging, case of motif
counting is when $H$ has three vertices, often called a "triadic" query. Recent
work has focused on "temporal graph mining", where the network $G$ has edges
with timestamps (and directions) and $H$ has time constraints.
  Inspired by concepts in logic and database theory, we introduce the study of
"thresholded First Order Logic (FOL) Motif Analysis" for massive temporal
networks. A typical triadic motif query asks for the existence of three
vertices that form a desired temporal pattern. An "FOL" motif query is obtained
by having both existential and thresholded universal quantifiers. This allows
for query semantics that can mine richer information from networks. A typical
triadic query would be "find all triples of vertices $u,v,w$ such that they
form a triangle within one hour". A thresholded FOL query can express "find all
pairs $u,v$ such that for half of $w$ where $(u,w)$ formed an edge, $(v,w)$
also formed an edge within an hour".
  We design the first algorithm, FOLTY, for mining thresholded FOL triadic
queries. The theoretical running time of FOLTY matches the best known running
time for temporal triangle counting in sparse graphs. We give an efficient
implementation of FOLTY using specialized temporal data structures. FOLTY has
excellent empirical behavior, and can answer triadic FOL queries on graphs with
nearly 70M edges is less than hour on commodity hardware. Our work has the
potential to start a new research direction in the classic well-studied problem
of motif analysis.

</details>


### [33] [Unfolding Data Quality Dimensions in Practice: A Survey](https://arxiv.org/abs/2507.17507)
*Vasileios Papastergios,Lisa Ehrlinger,Anastasios Gounaris*

Main category: cs.DB

TL;DR: The paper bridges the gap between data quality theory and practice by mapping functionalities of open-source tools to quality dimensions.


<details>
  <summary>Details</summary>
Motivation: Despite extensive research and standardization efforts, practical application of data quality dimensions remains unclear. There is a need to connect data quality tools with these dimensions.

Method: Examination of seven open-source data quality tools to map their functionalities to data quality dimensions.

Result: Provides a comprehensive mapping showing how individual functionalities relate to quality dimensions, revealing many-to-many relationships.

Conclusion: Offers practitioners and researchers a unified view of the fragmented landscape of data quality checks, with actionable insights for multi-dimensional quality assessment.

Abstract: Data quality describes the degree to which data meet specific requirements
and are fit for use by humans and/or downstream tasks (e.g., artificial
intelligence). Data quality can be assessed across multiple high-level concepts
called dimensions, such as accuracy, completeness, consistency, or timeliness.
While extensive research and several attempts for standardization (e.g.,
ISO/IEC 25012) exist for data quality dimensions, their practical application
often remains unclear. In parallel to research endeavors, a large number of
tools have been developed that implement functionalities for the detection and
mitigation of specific data quality issues, such as missing values or outliers.
With this paper, we aim to bridge this gap between data quality theory and
practice by systematically connecting low-level functionalities offered by data
quality tools with high-level dimensions, revealing their many-to-many
relationships. Through an examination of seven open-source data quality tools,
we provide a comprehensive mapping between their functionalities and the data
quality dimensions, demonstrating how individual functionalities and their
variants partially contribute to the assessment of single dimensions. This
systematic survey provides both practitioners and researchers with a unified
view on the fragmented landscape of data quality checks, offering actionable
insights for quality assessment across multiple dimensions.

</details>


### [34] [SHINE: A Scalable HNSW Index in Disaggregated Memory](https://arxiv.org/abs/2507.17647)
*Manuel Widmoser,Daniel Kocher,Nikolaus Augsten*

Main category: cs.DB

TL;DR: A scalable, graph-preserving HNSW index for distributed approximate nearest neighbor search in disaggregated memory, employing effective caching to overcome bandwidth limitations.


<details>
  <summary>Details</summary>
Motivation: To enable efficient high-dimensional vector search at scale in modern data centers with disaggregated memory architectures.

Method: Building a graph-preserving distributed HNSW index combined with a novel caching mechanism and cache cooperation among compute nodes.

Result: The proposed method achieves accuracy comparable to single-machine HNSW and demonstrates high scalability and efficiency in evaluations.

Conclusion: The approach effectively addresses challenges in distributed ANN search with disaggregated memory, balancing accuracy, bandwidth, and scalability.

Abstract: Approximate nearest neighbor (ANN) search is a fundamental problem in
computer science for which in-memory graph-based methods, such as Hierarchical
Navigable Small World (HNSW), perform exceptionally well. To scale beyond
billions of high-dimensional vectors, the index must be distributed. The
disaggregated memory architecture physically separates compute and memory into
two distinct hardware units and has become popular in modern data centers. Both
units are connected via RDMA networks that allow compute nodes to directly
access remote memory and perform all the computations, posing unique challenges
for disaggregated indexes.
  In this work, we propose a scalable HNSW index for ANN search in
disaggregated memory. In contrast to existing distributed approaches, which
partition the graph at the cost of accuracy, our method builds a
graph-preserving index that reaches the same accuracy as a single-machine HNSW.
Continuously fetching high-dimensional vector data from remote memory leads to
severe network bandwidth limitations, which we overcome by employing an
efficient caching mechanism. Since answering a single query involves processing
numerous unique graph nodes, caching alone is not sufficient to achieve high
scalability. We logically combine the caches of the compute nodes to increase
the overall cache effectiveness and confirm the efficiency and scalability of
our method in our evaluation.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [35] [CQE under Epistemic Dependencies: Algorithms and Experiments (extended version)](https://arxiv.org/abs/2507.17487)
*Lorenzo Marconi,Flavia Ricci,Riccardo Rosati*

Main category: cs.AI

TL;DR: The paper examines Controlled Query Evaluation (CQE) over ontologies with epistemic dependencies (EDs), focusing on security and computational efficiency. It characterizes security conditions, identifies ED classes where safety is preserved, and demonstrates an efficient rewriting algorithm, supported by experiments.


<details>
  <summary>Details</summary>
Motivation: To enhance the security and efficiency of query answering in ontology-based systems by leveraging epistemic dependencies and optimal censorship strategies.

Method: The authors combine EDs with optimal GA censors, analyze security for full EDs, develop a first-order rewriting algorithm for DL-Lite_R ontologies, and conduct experiments to assess practicality.

Result: The approach guarantees security under certain ED classes, achieves AC^0 data complexity for query answering, and is practically feasible as shown by experiments.

Conclusion: The intersection of optimal GA censors with EDs offers a secure and computationally efficient framework for CQE, with practical algorithms validated through experiments.

Abstract: We investigate Controlled Query Evaluation (CQE) over ontologies, where
information disclosure is regulated by epistemic dependencies (EDs), a family
of logical rules recently proposed for the CQE framework. In particular, we
combine EDs with the notion of optimal GA censors, i.e. maximal sets of ground
atoms that are entailed by the ontology and can be safely revealed. We focus on
answering Boolean unions of conjunctive queries (BUCQs) with respect to the
intersection of all optimal GA censors - an approach that has been shown in
other contexts to ensure strong security guarantees with favorable
computational behavior. First, we characterize the security of this
intersection-based approach and identify a class of EDs (namely, full EDs) for
which it remains safe. Then, for a subclass of EDs and for DL-Lite_R
ontologies, we show that answering BUCQs in the above CQE semantics is in AC^0
in data complexity by presenting a suitable, detailed first-order rewriting
algorithm. Finally, we report on experiments conducted in two different
evaluation scenarios, showing the practical feasibility of our rewriting
function.

</details>


### [36] [Towards Autonomous Sustainability Assessment via Multimodal AI Agents](https://arxiv.org/abs/2507.17012)
*Zhihan Zhang,Alexander Metzger,Yuxuan Mei,Felix Hähnlein,Zachary Englhardt,Tingyu Cheng,Gregory D. Abowd,Shwetak Patel,Adriana Schulz,Vikram Iyer*

Main category: cs.AI

TL;DR: The paper presents multimodal AI agents that automate and expedite life cycle assessments (LCAs) for electronic devices, accurately estimating carbon footprints with minimal expert input.


<details>
  <summary>Details</summary>
Motivation: To address data unavailability and high time consumption in traditional LCAs for environmental impact assessment.

Method: Introducing AI agents that extract data from online text and images, emulate expert interactions, and use clustering and emission factor generation techniques.

Result: AI approaches produce carbon footprint estimates within 19% of expert LCAs in under a minute, with high accuracy and significant efficiency gains.

Conclusion: AI-driven LCA methods can revolutionize sustainability assessments by making them faster, more accessible, and comparable in accuracy to traditional expert methods.

Abstract: Interest in sustainability information has surged in recent years. However,
the data required for a life cycle assessment (LCA) that maps the materials and
processes from product manufacturing to disposal into environmental impacts
(EI) are often unavailable. Here we reimagine conventional LCA by introducing
multimodal AI agents that emulate interactions between LCA experts and
stakeholders like product managers and engineers to calculate the
cradle-to-gate (production) carbon emissions of electronic devices. The AI
agents iteratively generate a detailed life-cycle inventory leveraging a custom
data abstraction and software tools that extract information from online text
and images from repair communities and government certifications. This approach
reduces weeks or months of expert time to under one minute and closes data
availability gaps while yielding carbon footprint estimates within 19% of
expert LCAs with zero proprietary data. Additionally, we develop a method to
directly estimate EI by comparing an input to a cluster of products with
similar descriptions and known carbon footprints. This runs in 3 ms on a laptop
with a MAPE of 12.28% on electronic products. Further, we develop a data-driven
method to generate emission factors. We use the properties of an unknown
material to represent it as a weighted sum of emission factors for similar
materials. Compared to human experts picking the closest LCA database entry,
this improves MAPE by 120.26%. We analyze the data and compute scaling of this
approach and discuss its implications for future LCA workflows.

</details>


### [37] [New Mechanisms in Flex Distribution for Bounded Suboptimal Multi-Agent Path Finding](https://arxiv.org/abs/2507.17054)
*Shao-Hung Chan,Thomy Phan,Jiaoyang Li,Sven Koenig*

Main category: cs.AI

TL;DR: The paper introduces new flex distribution strategies for EECBS, a MAPF algorithm, improving efficiency while maintaining bounded-suboptimal solutions.


<details>
  <summary>Details</summary>
Motivation: Enhance the performance of EECBS in solving MAPF problems by better managing collision resolutions.

Method: Proposing Conflict-Based, Delay-Based, and Mixed-Strategy Flex Distribution mechanisms within EECBS.

Result: The new methods outperform traditional flex distribution in efficiency, while ensuring completeness and bounded-suboptimality.

Conclusion: The introduced flex distribution strategies significantly improve EECBS performance in MAPF tasks.

Abstract: Multi-Agent Path Finding (MAPF) is the problem of finding a set of
collision-free paths, one for each agent in a shared environment. Its objective
is to minimize the sum of path costs (SOC), where the path cost of each agent
is defined as the travel time from its start location to its target location.
Explicit Estimation Conflict-Based Search (EECBS) is the leading algorithm for
bounded-suboptimal MAPF, with the SOC of the solution being at most a
user-specified factor $w$ away from optimal. EECBS maintains sets of paths and
a lower bound $LB$ on the optimal SOC. Then, it iteratively selects a set of
paths whose SOC is at most $w \cdot LB$ and introduces constraints to resolve
collisions. For each path in a set, EECBS maintains a lower bound on its
optimal path that satisfies constraints. By finding an individually
bounded-suboptimal path with cost at most a threshold of $w$ times its lower
bound, EECBS guarantees to find a bounded-suboptimal solution. To speed up
EECBS, previous work uses flex distribution to increase the threshold. Though
EECBS with flex distribution guarantees to find a bounded-suboptimal solution,
increasing the thresholds may push the SOC beyond $w \cdot LB$, forcing EECBS
to switch among different sets of paths instead of resolving collisions on a
particular set of paths, and thus reducing efficiency. To address this issue,
we propose Conflict-Based Flex Distribution that distributes flex in proportion
to the number of collisions. We also estimate the delays needed to satisfy
constraints and propose Delay-Based Flex Distribution. On top of that, we
propose Mixed-Strategy Flex Distribution, combining both in a hierarchical
framework. We prove that EECBS with our new flex distribution mechanisms is
complete and bounded-suboptimal. Our experiments show that our approaches
outperform the original (greedy) flex distribution.

</details>


### [38] [LoRA is All You Need for Safety Alignment of Reasoning LLMs](https://arxiv.org/abs/2507.17075)
*Yihao Xue,Baharan Mirzasoleiman*

Main category: cs.AI

TL;DR: Using LoRA for safety fine-tuning of LLMs effectively aligns models for safety without degrading reasoning abilities, avoiding the 


<details>
  <summary>Details</summary>
Motivation: To address the 

Method: Applying LoRA for safety fine-tuning on refusal datasets to minimize interference with reasoning capabilities.

Result: Achieves safety levels comparable to full-model fine-tuning while preserving reasoning, and reduces weight overlap with initial weights.

Conclusion: LoRA enables safe LLMs without compromising reasoning, suggesting potential for improving the reasoning-safety balance.

Abstract: Reasoning LLMs have demonstrated remarkable breakthroughs in solving complex
problems that were previously out of reach. To ensure LLMs do not assist with
harmful requests, safety alignment fine-tuning is necessary in the
post-training phase. However, safety alignment fine-tuning has recently been
shown to significantly degrade reasoning abilities, a phenomenon known as the
"Safety Tax". In this work, we show that using LoRA for SFT on refusal datasets
effectively aligns the model for safety without harming its reasoning
capabilities. This is because restricting the safety weight updates to a
low-rank space minimizes the interference with the reasoning weights. Our
extensive experiments across four benchmarks covering math, science, and coding
show that this approach produces highly safe LLMs -- with safety levels
comparable to full-model fine-tuning -- without compromising their reasoning
abilities. Additionally, we observe that LoRA induces weight updates with
smaller overlap with the initial weights compared to full-model fine-tuning. We
also explore methods that further reduce such overlap -- via regularization or
during weight merging -- and observe some improvement on certain tasks. We hope
this result motivates designing approaches that yield more consistent
improvements in the reasoning-safety trade-off.

</details>


### [39] [HySafe-AI: Hybrid Safety Architectural Analysis Framework for AI Systems: A Case Study](https://arxiv.org/abs/2507.17118)
*Mandar Pitale,Jelena Frtunikj,Abhinaw Priyadershi,Vasu Singh,Maria Spence*

Main category: cs.AI

TL;DR: This paper reviews architectural solutions for AI in safety-critical systems, evaluates safety analysis techniques, and introduces HySAFE-AI, a hybrid safety framework for AI systems, with future safety standards discussed.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of ensuring safety in complex, end-to-end AI architectures used in critical applications.

Method: Reviewing architectures, evaluating safety analysis methods, and developing the HySAFE-AI framework.

Result: Demonstration of improvements to safety analysis techniques and introduction of a hybrid safety framework, HySAFE-AI.

Conclusion: Hybrid approaches and tailored safety analyses are essential for advancing AI safety standards in critical systems.

Abstract: AI has become integral to safety-critical areas like autonomous driving
systems (ADS) and robotics. The architecture of recent autonomous systems are
trending toward end-to-end (E2E) monolithic architectures such as large
language models (LLMs) and vision language models (VLMs). In this paper, we
review different architectural solutions and then evaluate the efficacy of
common safety analyses such as failure modes and effect analysis (FMEA) and
fault tree analysis (FTA). We show how these techniques can be improved for the
intricate nature of the foundational models, particularly in how they form and
utilize latent representations. We introduce HySAFE-AI, Hybrid Safety
Architectural Analysis Framework for AI Systems, a hybrid framework that adapts
traditional methods to evaluate the safety of AI systems. Lastly, we offer
hints of future work and suggestions to guide the evolution of future AI safety
standards.

</details>


### [40] [Improving LLMs' Generalized Reasoning Abilities by Graph Problems](https://arxiv.org/abs/2507.17168)
*Qifan Zhang,Nuo Chen,Zehua Li,Miao Peng,Jing Tang,Jia Li*

Main category: cs.AI

TL;DR: Introducing GraphPile, a large-scale dataset for Graph Problem Reasoning (GPR) to enhance LLMs' reasoning skills across diverse tasks.


<details>
  <summary>Details</summary>
Motivation: Address the limited transferability of domain-specific pretraining methods for reasoning tasks in LLMs.

Method: Create a comprehensive GPR dataset (GraphPile) and fine-tune LLMs like Llama 3/3.1 and Gemma 2 on this data.

Result: Improvements of up to 4.9% in mathematical reasoning and 21.2% in non-mathematical reasoning tasks.

Conclusion: GPR with GraphPile enhances LLMs' reasoning capabilities, bridging the gap between domain-specific pretraining and universal reasoning.

Abstract: Large Language Models (LLMs) have made remarkable strides in reasoning tasks,
yet their performance often falters on novel and complex problems.
Domain-specific continued pretraining (CPT) methods, such as those tailored for
mathematical reasoning, have shown promise but lack transferability to broader
reasoning tasks. In this work, we pioneer the use of Graph Problem Reasoning
(GPR) to enhance the general reasoning capabilities of LLMs. GPR tasks,
spanning pathfinding, network analysis, numerical computation, and topological
reasoning, require sophisticated logical and relational reasoning, making them
ideal for teaching diverse reasoning patterns. To achieve this, we introduce
GraphPile, the first large-scale corpus specifically designed for CPT using GPR
data. Spanning 10.9 billion tokens across 23 graph tasks, the dataset includes
chain-of-thought, program-of-thought, trace of execution, and real-world graph
data. Using GraphPile, we train GraphMind on popular base models Llama 3 and
3.1, as well as Gemma 2, achieving up to 4.9 percent higher accuracy in
mathematical reasoning and up to 21.2 percent improvement in non-mathematical
reasoning tasks such as logical and commonsense reasoning. By being the first
to harness GPR for enhancing reasoning patterns and introducing the first
dataset of its kind, our work bridges the gap between domain-specific
pretraining and universal reasoning capabilities, advancing the adaptability
and robustness of LLMs.

</details>


### [41] [Our Cars Can Talk: How IoT Brings AI to Vehicles](https://arxiv.org/abs/2507.17214)
*Amod Kant Agrawal*

Main category: cs.AI

TL;DR: Integrating AI copilots into vehicles can transform vehicle maintenance from reactive to proactive, enhancing intelligent systems, predictive maintenance, and user interaction.


<details>
  <summary>Details</summary>
Motivation: The need to shift from reactive to proactive vehicle maintenance through AI integration.

Method: Providing a conceptual and technical perspective to guide future research.

Result: Facilitates interdisciplinary dialogue and outlines future directions in intelligent vehicle systems.

Conclusion: AI copilots that understand both machine and driver languages are crucial for advancing vehicle intelligence and maintenance.

Abstract: Bringing AI to vehicles and enabling them as sensing platforms is key to
transforming maintenance from reactive to proactive. Now is the time to
integrate AI copilots that speak both languages: machine and driver. This
article offers a conceptual and technical perspective intended to spark
interdisciplinary dialogue and guide future research and development in
intelligent vehicle systems, predictive maintenance, and AI-powered user
interaction.

</details>


### [42] [Agent Identity Evals: Measuring Agentic Identity](https://arxiv.org/abs/2507.17257)
*Elija Perrier,Michael Timothy Bennett*

Main category: cs.AI

TL;DR: The paper introduces Agent Identity Evals (AIE), a framework for measuring how well language model agents maintain stable identity and reliability over time, addressing challenges caused by inherited LLM pathologies.


<details>
  <summary>Details</summary>
Motivation: Ensuring the consistency and trustworthiness of language model agents by measuring their ability to preserve identity and reliability over time.

Method: Developing a formal, empirical framework with novel metrics to evaluate agent identity, capable of integration with other performance measures.

Result: AIE provides a set of metrics and methods for assessing and improving the agentic identity and robustness of language models throughout their lifecycle.

Conclusion: The framework helps in designing better LMA systems by quantifying and ensuring their identity stability and resilience.

Abstract: Central to agentic capability and trustworthiness of language model agents
(LMAs) is the extent they maintain stable, reliable, identity over time.
However, LMAs inherit pathologies from large language models (LLMs)
(statelessness, stochasticity, sensitivity to prompts and
linguistically-intermediation) which can undermine their identifiability,
continuity, persistence and consistency. This attrition of identity can erode
their reliability, trustworthiness and utility by interfering with their
agentic capabilities such as reasoning, planning and action. To address these
challenges, we introduce \textit{agent identity evals} (AIE), a rigorous,
statistically-driven, empirical framework for measuring the degree to which an
LMA system exhibit and maintain their agentic identity over time, including
their capabilities, properties and ability to recover from state perturbations.
AIE comprises a set of novel metrics which can integrate with other measures of
performance, capability and agentic robustness to assist in the design of
optimal LMA infrastructure and scaffolding such as memory and tools. We set out
formal definitions and methods that can be applied at each stage of the LMA
life-cycle, and worked examples of how to apply them.

</details>


### [43] [Students' Feedback Requests and Interactions with the SCRIPT Chatbot: Do They Get What They Ask For?](https://arxiv.org/abs/2507.17258)
*Andreas Scholl,Natalie Kiesler*

Main category: cs.AI

TL;DR: The paper introduces SCRIPT, a ChatGPT-4-based chatbot designed for programming learners, and evaluates its effectiveness with students, revealing insights into their feedback patterns and the chatbot's response alignment.


<details>
  <summary>Details</summary>
Motivation: To enhance programming education with AI tools and understand students' feedback behaviors using a specialized chatbot.

Method: Developed SCRIPT based on ChatGPT-4o-mini, then conducted an experiment with 136 students to observe interaction patterns and feedback requests.

Result: Students follow specific feedback request sequences; the chatbot responds appropriately in 75% of cases, matching requested feedback types and adhering to prompts.

Conclusion: The findings inform the design of AI-based learning tools, emphasizing balancing guidance and flexibility, and showcase the potential of AI to support novice programming learners.

Abstract: Building on prior research on Generative AI (GenAI) and related tools for
programming education, we developed SCRIPT, a chatbot based on ChatGPT-4o-mini,
to support novice learners. SCRIPT allows for open-ended interactions and
structured guidance through predefined prompts. We evaluated the tool via an
experiment with 136 students from an introductory programming course at a large
German university and analyzed how students interacted with SCRIPT while
solving programming tasks with a focus on their feedback preferences. The
results reveal that students' feedback requests seem to follow a specific
sequence. Moreover, the chatbot responses aligned well with students' requested
feedback types (in 75%), and it adhered to the system prompt constraints. These
insights inform the design of GenAI-based learning support systems and
highlight challenges in balancing guidance and flexibility in AI-assisted
tools.

</details>


### [44] [Compliance Brain Assistant: Conversational Agentic AI for Assisting Compliance Tasks in Enterprise Environments](https://arxiv.org/abs/2507.17289)
*Shitong Zhu,Chenhao Fang,Derek Larson,Neel Reddy Pochareddy,Rajeev Rao,Sophie Zeng,Yanqing Peng,Wendy Summer,Alex Goncalves,Arya Pudota,Herve Robert*

Main category: cs.AI

TL;DR: The paper introduces Compliance Brain Assistant (CBA), an AI designed to improve enterprise compliance tasks by intelligently routing user queries between fast and full agentic modes, significantly outperforming standard LLMs in relevant metrics.


<details>
  <summary>Details</summary>
Motivation: To enhance the efficiency of compliance-related tasks in enterprise environments using a conversational AI that balances response quality and latency.

Method: Designing a user query router to switch between FastTrack and FullAgentic modes, and evaluating it against baseline LLMs through real-world privacy/compliance queries.

Result: CBA significantly outperforms vanilla LLMs on key performance metrics such as keyword match rate and pass rate, with the routing mechanism effectively balancing response quality with response time.

Conclusion: The routing mechanism in CBA improves performance and efficiency, providing a practical trade-off between response quality and latency in enterprise compliance assistance.

Abstract: This paper presents Compliance Brain Assistant (CBA), a conversational,
agentic AI assistant designed to boost the efficiency of daily compliance tasks
for personnel in enterprise environments. To strike a good balance between
response quality and latency, we design a user query router that can
intelligently choose between (i) FastTrack mode: to handle simple requests that
only need additional relevant context retrieved from knowledge corpora; and
(ii) FullAgentic mode: to handle complicated requests that need composite
actions and tool invocations to proactively discover context across various
compliance artifacts, and/or involving other APIs/models for accommodating
requests. A typical example would be to start with a user query, use its
description to find a specific entity and then use the entity's information to
query other APIs for curating and enriching the final AI response.
  Our experimental evaluations compared CBA against an out-of-the-box LLM on
various real-world privacy/compliance-related queries targeting various
personas. We found that CBA substantially improved upon the vanilla LLM's
performance on metrics such as average keyword match rate (83.7% vs. 41.7%) and
LLM-judge pass rate (82.0% vs. 20.0%). We also compared metrics for the full
routing-based design against the `fast-track only` and `full-agentic` modes and
found that it had a better average match-rate and pass-rate while keeping the
run-time approximately the same. This finding validated our hypothesis that the
routing mechanism leads to a good trade-off between the two worlds.

</details>


### [45] [Ctx2TrajGen: Traffic Context-Aware Microscale Vehicle Trajectories using Generative Adversarial Imitation Learning](https://arxiv.org/abs/2507.17418)
*Joobin Jin,Seokjun Hong,Gyeongseon Baek,Yeeun Kim,Byeongjoon Noh*

Main category: cs.AI

TL;DR: Ctx2TrajGen is a framework for realistic, context-aware vehicle trajectory generation using GAIL, improving on existing methods by modeling interactions and environment details.


<details>
  <summary>Details</summary>
Motivation: Accurate microscopic vehicle trajectory modeling is essential for traffic analysis and autonomous driving.

Method: Using GAIL with PPO and WGAN-GP, conditioning on surrounding vehicles and road geometry.

Result: Outperforms existing methods in realism, diversity, and context fidelity on the DRIFT dataset.

Conclusion: Provides a robust, data-efficient, and domain-shift-resistant solution for traffic behavior synthesis.

Abstract: Precise modeling of microscopic vehicle trajectories is critical for traffic
behavior analysis and autonomous driving systems. We propose Ctx2TrajGen, a
context-aware trajectory generation framework that synthesizes realistic urban
driving behaviors using GAIL. Leveraging PPO and WGAN-GP, our model addresses
nonlinear interdependencies and training instability inherent in microscopic
settings. By explicitly conditioning on surrounding vehicles and road geometry,
Ctx2TrajGen generates interaction-aware trajectories aligned with real-world
context. Experiments on the drone-captured DRIFT dataset demonstrate superior
performance over existing methods in terms of realism, behavioral diversity,
and contextual fidelity, offering a robust solution to data scarcity and domain
shift without simulation.

</details>


### [46] [An Uncertainty-Driven Adaptive Self-Alignment Framework for Large Language Models](https://arxiv.org/abs/2507.17477)
*Haoran Sun,Zekun Zhang,Shaoning Zeng*

Main category: cs.AI

TL;DR: UDASA is an automated framework to improve LLM alignment by leveraging uncertainty across various response dimensions, enhancing safety and usefulness.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of aligning large language models with human intent and safety norms without requiring human annotations.

Method: The framework generates multiple responses, measures uncertainty in semantics, factuality, and values, and uses these to guide a staged training process.

Result: UDASA outperforms existing methods in safety, helpfulness, truthfulness, and sentiment control tasks.

Conclusion: Uncertainty-driven self-alignment offers a promising approach to improve LLM safety and alignment automatically.

Abstract: Large Language Models (LLMs) have demonstrated remarkable progress in
instruction following and general-purpose reasoning. However, achieving
high-quality alignment with human intent and safety norms without human
annotations remains a fundamental challenge. In this work, we propose an
Uncertainty-Driven Adaptive Self-Alignment (UDASA) framework designed to
improve LLM alignment in a fully automated manner. UDASA first generates
multiple responses for each input and quantifies output uncertainty across
three dimensions: semantics, factuality, and value alignment. Based on these
uncertainty scores, the framework constructs preference pairs and categorizes
training samples into three stages, conservative, moderate, and exploratory,
according to their uncertainty difference. The model is then optimized
progressively across these stages. In addition, we conduct a series of
preliminary studies to validate the core design assumptions and provide strong
empirical motivation for the proposed framework. Experimental results show that
UDASA outperforms existing alignment methods across multiple tasks, including
harmlessness, helpfulness, truthfulness, and controlled sentiment generation,
significantly improving model performance.

</details>


### [47] [LTLZinc: a Benchmarking Framework for Continual Learning and Neuro-Symbolic Temporal Reasoning](https://arxiv.org/abs/2507.17482)
*Luca Salvatore Lorello,Nikolaos Manginas,Marco Lippi,Stefano Melacci*

Main category: cs.AI

TL;DR: The paper introduces LTLZinc, a benchmarking framework for evaluating neuro-symbolic and continual learning methods in temporal reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: To address the gap in existing neuro-symbolic AI research, which mainly focuses on static scenarios, by providing tools to evaluate temporal reasoning and continual learning.

Method: Developing LTLZinc, a dataset generator based on linear temporal logic and MiniZinc constraints, capable of creating diverse temporal and continual learning tasks with detailed annotations.

Result: LTLZinc enables the creation of challenging temporal learning and reasoning datasets, revealing limitations of current methods, and is released for community use.

Conclusion: LTLZinc fosters research toward integrated temporal learning and reasoning, highlighting the need for advancing current neuro-symbolic approaches.

Abstract: Neuro-symbolic artificial intelligence aims to combine neural architectures
with symbolic approaches that can represent knowledge in a human-interpretable
formalism. Continual learning concerns with agents that expand their knowledge
over time, improving their skills while avoiding to forget previously learned
concepts. Most of the existing approaches for neuro-symbolic artificial
intelligence are applied to static scenarios only, and the challenging setting
where reasoning along the temporal dimension is necessary has been seldom
explored. In this work we introduce LTLZinc, a benchmarking framework that can
be used to generate datasets covering a variety of different problems, against
which neuro-symbolic and continual learning methods can be evaluated along the
temporal and constraint-driven dimensions. Our framework generates expressive
temporal reasoning and continual learning tasks from a linear temporal logic
specification over MiniZinc constraints, and arbitrary image classification
datasets. Fine-grained annotations allow multiple neural and neuro-symbolic
training settings on the same generated datasets. Experiments on six
neuro-symbolic sequence classification and four class-continual learning tasks
generated by LTLZinc, demonstrate the challenging nature of temporal learning
and reasoning, and highlight limitations of current state-of-the-art methods.
We release the LTLZinc generator and ten ready-to-use tasks to the
neuro-symbolic and continual learning communities, in the hope of fostering
research towards unified temporal learning and reasoning frameworks.

</details>


### [48] [Automated Hybrid Grounding Using Structural and Data-Driven Heuristics](https://arxiv.org/abs/2507.17493)
*Alexander Beiser,Markus Hecher,Stefan Woltran*

Main category: cs.AI

TL;DR: The paper introduces automated hybrid grounding for Answer Set Programming to address the grounding bottleneck by dynamically choosing between body-decoupled and standard grounding methods using heuristic-based algorithms.


<details>
  <summary>Details</summary>
Motivation: The widespread adoption of Answer Set Programming in industry is hindered by the grounding bottleneck, necessitating more efficient grounding techniques.

Method: Developing a splitting algorithm with data-structural heuristics and an estimation procedure that utilizes instance data to decide the suitable grounding method.

Result: Experiments show that the approach improves performance on difficult-to-ground scenarios and approaches state-of-the-art results on hard-to-solve instances.

Conclusion: Automated hybrid grounding effectively mitigates the grounding bottleneck, leading to better performance in Answer Set Programming.

Abstract: The grounding bottleneck poses one of the key challenges that hinders the
widespread adoption of Answer Set Programming in industry. Hybrid Grounding is
a step in alleviating the bottleneck by combining the strength of standard
bottom-up grounding with recently proposed techniques where rule bodies are
decoupled during grounding. However, it has remained unclear when hybrid
grounding shall use body-decoupled grounding and when to use standard bottom-up
grounding. In this paper, we address this issue by developing automated hybrid
grounding: we introduce a splitting algorithm based on data-structural
heuristics that detects when to use body-decoupled grounding and when standard
grounding is beneficial. We base our heuristics on the structure of rules and
an estimation procedure that incorporates the data of the instance. The
experiments conducted on our prototypical implementation demonstrate promising
results, which show an improvement on hard-to-ground scenarios, whereas on
hard-to-solve instances we approach state-of-the-art performance.

</details>


### [49] [Can One Domain Help Others? A Data-Centric Study on Multi-Domain Reasoning via Reinforcement Learning](https://arxiv.org/abs/2507.17512)
*Yu Li,Zhuoshi Pan,Honglin Lin,Mengyuan Sun,Conghui He,Lijun Wu*

Main category: cs.AI

TL;DR: The paper investigates multi-domain reasoning in large language models using reinforcement learning with verifiable rewards, focusing on mathematical, coding, and logical reasoning. It evaluates model improvements, interactions between domains, and training strategies, offering insights to optimize reasoning capabilities.


<details>
  <summary>Details</summary>
Motivation: To understand and improve how reinforcement learning with verifiable rewards (RLVR) enables large language models (LLMs) to perform integrated reasoning across multiple domains, addressing the gap in understanding domain interactions and training methods.

Method: The study employs the GRPO algorithm and Qwen-2.5-7B models to evaluate in-domain and cross-domain performance, investigates domain interactions during combined training, compares base and instruct models, and systematically explores training factors like curriculum learning and reward design.

Result: Findings reveal the dynamics of domain interactions, mutual enhancements, conflicts, and the impact of training strategies on reasoning performance, offering insights for optimizing multi-domain reasoning in LLMs.

Conclusion: The research provides valuable guidance for refining RL methodologies to develop LLMs capable of comprehensive and generalizable multi-domain reasoning.

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a
powerful paradigm for enhancing the reasoning capabilities of LLMs. Existing
research has predominantly concentrated on isolated reasoning domains such as
mathematical problem-solving, coding tasks, or logical reasoning. However, real
world reasoning scenarios inherently demand an integrated application of
multiple cognitive skills. Despite this, the interplay among these reasoning
skills under reinforcement learning remains poorly understood. To bridge this
gap, we present a systematic investigation of multi-domain reasoning within the
RLVR framework, explicitly focusing on three primary domains: mathematical
reasoning, code generation, and logical puzzle solving. We conduct a
comprehensive study comprising four key components: (1) Leveraging the GRPO
algorithm and the Qwen-2.5-7B model family, our study thoroughly evaluates the
models' in-domain improvements and cross-domain generalization capabilities
when trained on single-domain datasets. (2) Additionally, we examine the
intricate interactions including mutual enhancements and conflicts that emerge
during combined cross-domain training. (3) To further understand the influence
of SFT on RL, we also analyze and compare performance differences between base
and instruct models under identical RL configurations. (4) Furthermore, we
delve into critical RL training details, systematically exploring the impacts
of curriculum learning strategies, variations in reward design, and
language-specific factors. Through extensive experiments, our results offer
significant insights into the dynamics governing domain interactions, revealing
key factors influencing both specialized and generalizable reasoning
performance. These findings provide valuable guidance for optimizing RL
methodologies to foster comprehensive, multi-domain reasoning capabilities in
LLMs.

</details>


### [50] [TAI Scan Tool: A RAG-Based Tool With Minimalistic Input for Trustworthy AI Self-Assessment](https://arxiv.org/abs/2507.17514)
*Athanasios Davvetas,Xenia Ziouvelou,Ypatia Dami,Alexis Kaponis,Konstantina Giouvanopoulou,Michael Papademas*

Main category: cs.AI

TL;DR: The paper presents the TAI Scan Tool, a RAG-based self-assessment tool for legal AI compliance focusing on the AI Act, which accurately predicts risk levels and retrieves relevant legal articles.


<details>
  <summary>Details</summary>
Motivation: To facilitate compliance with the AI Act through a simple and effective self-assessment tool for legal risk evaluation.

Method: Development of a two-step TAI Scan Tool incorporating pre-screening and assessment, utilizing retrieval-augmented generation (RAG) techniques, and qualitative evaluation through use-case scenarios.

Result: The tool successfully predicts AI system risk levels and retrieves pertinent legal articles across different semantic groups, with reasoning based on comparison to high-risk system settings.

Conclusion: The TAI Scan Tool offers a promising approach for legal AI risk assessment, aiding compliance with the AI Act by providing insightful risk evaluations and relevant legal obligations.

Abstract: This paper introduces the TAI Scan Tool, a RAG-based TAI self-assessment tool
with minimalistic input. The current version of the tool supports the legal TAI
assessment, with a particular emphasis on facilitating compliance with the AI
Act. It involves a two-step approach with a pre-screening and an assessment
phase. The assessment output of the system includes insight regarding the
risk-level of the AI system according to the AI Act, while at the same time
retrieving relevant articles to aid with compliance and notify on their
obligations. Our qualitative evaluation using use-case scenarios yields
promising results, correctly predicting risk levels while retrieving relevant
articles across three distinct semantic groups. Furthermore, interpretation of
results shows that the tool's reasoning relies on comparison with the setting
of high-risk systems, a behaviour attributed to their deployment requiring
careful consideration, and therefore frequently presented within the AI Act.

</details>


### [51] [Constructing Ophthalmic MLLM for Positioning-diagnosis Collaboration Through Clinical Cognitive Chain Reasoning](https://arxiv.org/abs/2507.17539)
*Xinyao Liu,Diping Song*

Main category: cs.AI

TL;DR: FundusExpert, an ophthalmology-specific multimodal large language model, improves medical diagnosis by integrating localization, diagnosis reasoning, and interpretability, outperforming existing models in accuracy and clinical consistency.


<details>
  <summary>Details</summary>
Motivation: to enhance the capability of multimodal large language models (MLLMs) in specialized medical domains like ophthalmology, addressing issues such as annotation granularity and clinical reasoning inconsistencies.

Method: development of FundusExpert, fine-tuning with instruction data from the FundusGen dataset, automated localization and semantic expansion via the Fundus-Engine system, and constructing clinically aligned reasoning chains.

Result: achieved superior performance in ophthalmic question-answering and report generation tasks, with notable improvements over existing models, and demonstrated a data quality-capability scaling law.

Conclusion: integrating region-level localization and reasoning chains in a scalable, clinically-aligned MLLM advances precision in medical diagnosis and bridges the visual-language gap in specialized domains.

Abstract: Multimodal large language models (MLLMs) demonstrate significant potential in
the field of medical diagnosis. However, they face critical challenges in
specialized domains such as ophthalmology, particularly the fragmentation of
annotation granularity and inconsistencies in clinical reasoning logic, which
hinder precise cross-modal understanding. This paper introduces FundusExpert,
an ophthalmology-specific MLLM with integrated positioning-diagnosis reasoning
capabilities, along with FundusGen, a dataset constructed through the
intelligent Fundus-Engine system. Fundus-Engine automates localization and
leverages MLLM-based semantic expansion to integrate global disease
classification, local object detection, and fine-grained feature analysis
within a single fundus image. Additionally, by constructing a clinically
aligned cognitive chain, it guides the model to generate interpretable
reasoning paths. FundusExpert, fine-tuned with instruction data from FundusGen,
achieves the best performance in ophthalmic question-answering tasks,
surpassing the average accuracy of the 40B MedRegA by 26.6%. It also excels in
zero-shot report generation tasks, achieving a clinical consistency of 77.0%,
significantly outperforming GPT-4o's 47.6%. Furthermore, we reveal a scaling
law between data quality and model capability ($L \propto N^{0.068}$),
demonstrating that the cognitive alignment annotations in FundusGen enhance
data utilization efficiency. By integrating region-level localization with
diagnostic reasoning chains, our work develops a scalable, clinically-aligned
MLLM and explores a pathway toward bridging the visual-language gap in specific
MLLMs. Our project can be found at https://github.com/MeteorElf/FundusExpert.

</details>


### [52] [Simulating multiple human perspectives in socio-ecological systems using large language models](https://arxiv.org/abs/2507.17680)
*Yongchao Zeng,Calum Brown,Ioannis Kyriakou,Ronja Hotz,Mark Rounsevell*

Main category: cs.AI

TL;DR: The paper introduces HoPeS, a framework using LLM-powered agents for perspective-taking in socio-ecological simulations, enabling users to explore and reflect on diverse stakeholder viewpoints.


<details>
  <summary>Details</summary>
Motivation: Understanding socio-ecological systems requires diverse stakeholder perspectives, which are hard to access, motivating the development of a simulation-based exploration tool.

Method: HoPeS employs LLM-driven agents to represent stakeholders and a simulation protocol to facilitate perspective shifting and reflection through role adoption and experimentations.

Result: A prototype demonstrated in land use change contexts showed users could adopt multiple perspectives, revealing insights and frustrations, and highlighting system's potential for interdisciplinary collaboration.

Conclusion: Refining HoPeS could enable new approaches for exploring stakeholder perspectives and fostering collaboration in socio-ecological research.

Abstract: Understanding socio-ecological systems requires insights from diverse
stakeholder perspectives, which are often hard to access. To enable
alternative, simulation-based exploration of different stakeholder
perspectives, we develop the HoPeS (Human-Oriented Perspective Shifting)
modelling framework. HoPeS employs agents powered by large language models
(LLMs) to represent various stakeholders; users can step into the agent roles
to experience perspectival differences. A simulation protocol serves as a
"scaffold" to streamline multiple perspective-taking simulations, supporting
users in reflecting on, transitioning between, and integrating across
perspectives. A prototype system is developed to demonstrate HoPeS in the
context of institutional dynamics and land use change, enabling both
narrative-driven and numerical experiments. In an illustrative experiment, a
user successively adopts the perspectives of a system observer and a researcher
- a role that analyses data from the embedded land use model to inform
evidence-based decision-making for other LLM agents representing various
institutions. Despite the user's effort to recommend technically sound
policies, discrepancies persist between the policy recommendation and
implementation due to stakeholders' competing advocacies, mirroring real-world
misalignment between researcher and policymaker perspectives. The user's
reflection highlights the subjective feelings of frustration and disappointment
as a researcher, especially due to the challenge of maintaining political
neutrality while attempting to gain political influence. Despite this, the user
exhibits high motivation to experiment with alternative narrative framing
strategies, suggesting the system's potential in exploring different
perspectives. Further system and protocol refinement are likely to enable new
forms of interdisciplinary collaboration in socio-ecological simulations.

</details>


### [53] [Symbiotic Agents: A Novel Paradigm for Trustworthy AGI-driven Networks](https://arxiv.org/abs/2507.17695)
*Ilias Chatzistefanidis,Navid Nikaein*

Main category: cs.AI

TL;DR: The paper presents a novel symbiotic paradigm combining LLMs and real-time optimization algorithms for AGI-driven 6G networks, demonstrating improved decision accuracy, reduced resource overhead, and enhanced flexibility in real-world testbeds.


<details>
  <summary>Details</summary>
Motivation: To enable autonomous, intelligent, and trustworthy management of 6G networks through advanced AI agents.

Method: Combining LLMs with real-time optimization in a symbiotic approach, designing new agent types, and testing on a 5G testbed.

Result: Reduction of decision errors fivefold, significant resource efficiency with smaller LLMs, and improved service and resource management with reduced over-utilization.

Conclusion: The symbiotic paradigm forms the foundation for future adaptable, efficient, and trustworthy AGI-driven networks.

Abstract: Large Language Model (LLM)-based autonomous agents are expected to play a
vital role in the evolution of 6G networks, by empowering real-time
decision-making related to management and service provisioning to end-users.
This shift facilitates the transition from a specialized intelligence approach,
where artificial intelligence (AI) algorithms handle isolated tasks, to
artificial general intelligence (AGI)-driven networks, where agents possess
broader reasoning capabilities and can manage diverse network functions. In
this paper, we introduce a novel agentic paradigm that combines LLMs with
real-time optimization algorithms towards Trustworthy AI, defined as symbiotic
agents. Optimizers at the LLM's input-level provide bounded uncertainty
steering for numerically precise tasks, whereas output-level optimizers
supervised by the LLM enable adaptive real-time control. We design and
implement two novel agent types including: (i) Radio Access Network optimizers,
and (ii) multi-agent negotiators for Service-Level Agreements (SLAs). We
further propose an end-to-end architecture for AGI networks and evaluate it on
a 5G testbed capturing channel fluctuations from moving vehicles. Results show
that symbiotic agents reduce decision errors fivefold compared to standalone
LLM-based agents, while smaller language models (SLM) achieve similar accuracy
with a 99.9% reduction in GPU resource overhead and in near-real-time loops of
82 ms. A multi-agent demonstration for collaborative RAN on the real-world
testbed highlights significant flexibility in service-level agreement and
resource allocation, reducing RAN over-utilization by approximately 44%.
Drawing on our findings and open-source implementations, we introduce the
symbiotic paradigm as the foundation for next-generation, AGI-driven
networks-systems designed to remain adaptable, efficient, and trustworthy even
as LLMs advance.

</details>


### [54] [Thinking Isn't an Illusion: Overcoming the Limitations of Reasoning Models via Tool Augmentations](https://arxiv.org/abs/2507.17699)
*Zhao Song,Song Yue,Jiahao Zhang*

Main category: cs.AI

TL;DR: Tool-augmented Large Reasoning Models outperform non-reasoning models in complex tasks, challenging the idea that reasoning skills are illusory.


<details>
  <summary>Details</summary>
Motivation: To assess whether the limitations of Large Reasoning Models persist when tools are integrated, given recent findings questioning their reasoning benefits.

Method: Incorporating tools like Python interpreters and scratchpads into three LLMs and their LRMs, and evaluating them on Apple's reasoning benchmark.

Result: LRMs with tool support outperform non-reasoning models across all task complexities, indicating effective reasoning capabilities with tool augmentation.

Conclusion: Tool-augmented LRMs exhibit enhanced reasoning abilities, challenging the notion that their reasoning processes are illusory.

Abstract: Large Reasoning Models (LRMs) have become a central focus in today's large
language model (LLM) research, where models are designed to output a
step-by-step thinking process before arriving at a final answer to handle
complex reasoning tasks. Despite their promise, recent empirical studies (e.g.,
[Shojaee et al., 2025] from Apple) suggest that this thinking process may not
actually enhance reasoning ability, where LLMs without explicit reasoning
actually outperform LRMs on tasks with low or high complexity. In this work, we
revisit these findings and investigate whether the limitations of LRMs persist
when tool augmentations are introduced. We incorporate two types of tools,
Python interpreters and scratchpads, and evaluate three representative LLMs and
their LRM counterparts on Apple's benchmark reasoning puzzles. Our results show
that, with proper tool use, LRMs consistently outperform their non-reasoning
counterparts across all levels of task complexity. These findings challenge the
recent narrative that reasoning is an illusion and highlight the potential of
tool-augmented LRMs for solving complex problems.

</details>


### [55] [Online Submission and Evaluation System Design for Competition Operations](https://arxiv.org/abs/2507.17730)
*Zhe Chen,Daniel Harabor,Ryan Hechnenberger,Nathan R. Sturtevant*

Main category: cs.AI

TL;DR: An online competition system automates the submission and evaluation process, reducing operational burden and addressing compatibility issues in research competitions.


<details>
  <summary>Details</summary>
Motivation: Tracking progress in research fields is difficult due to multiple venues and the burden of managing large-scale competitions.

Method: Development of an online, automated competition platform employing isolated environments for submission evaluation.

Result: The system has been successfully utilized in multiple competitions, improving efficiency and managing large volumes of submissions.

Conclusion: Automating competition management enhances efficiency and reliability, facilitating tracking advancements in research domains.

Abstract: Research communities have developed benchmark datasets across domains to
compare the performance of algorithms and techniques However, tracking the
progress in these research areas is not easy, as publications appear in
different venues at the same time, and many of them claim to represent the
state-of-the-art. To address this, research communities often organise periodic
competitions to evaluate the performance of various algorithms and techniques,
thereby tracking advancements in the field. However, these competitions pose a
significant operational burden. The organisers must manage and evaluate a large
volume of submissions. Furthermore, participants typically develop their
solutions in diverse environments, leading to compatibility issues during the
evaluation of their submissions. This paper presents an online competition
system that automates the submission and evaluation process for a competition.
The competition system allows organisers to manage large numbers of submissions
efficiently, utilising isolated environments to evaluate submissions. This
system has already been used successfully for several competitions, including
the Grid-Based Pathfinding Competition and the League of Robot Runners
competition.

</details>
